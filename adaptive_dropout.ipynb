{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neff_adaptive_dropout.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeffDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Neff-adaptive dropout.\n",
    "    Keeps top floor(beta * Neff) activations along `dim`, zeros the rest,\n",
    "    then rescales to preserve the per-sample L1 mass (optional).\n",
    "\n",
    "    Args:\n",
    "      beta: float >= 0, multiplicative factor on Neff.\n",
    "      dim: int, feature dimension (default: last).\n",
    "      mode: 'topk' (deterministic kWTA) or 'random' (Gumbel-Top-k by p_i).\n",
    "      keep_l1: if True, rescales masked output so sum|x| along `dim` is preserved.\n",
    "      eps: numerical epsilon to avoid 0/0.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta: float = 1.0, dim: int = -1,\n",
    "                 mode: str = \"topk\", keep_l1: bool = False, eps: float = 1e-12):\n",
    "        super().__init__()\n",
    "        assert mode in (\"topk\", \"random\")\n",
    "        self.beta = beta\n",
    "        self.dim = dim\n",
    "        self.mode = mode\n",
    "        self.keep_l1 = keep_l1\n",
    "        self.eps = eps\n",
    "        self.last_mask = None  # for debugging/inspection\n",
    "\n",
    "    def _move_last(self, x):\n",
    "        dim = self.dim if self.dim >= 0 else x.dim() + self.dim\n",
    "        perm = [d for d in range(x.dim()) if d != dim] + [dim]\n",
    "        inv = [0] * x.dim()\n",
    "        for i, p in enumerate(perm):\n",
    "            inv[p] = i\n",
    "        return x.permute(*perm), perm, inv\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training:\n",
    "            return x\n",
    "        # move target dim last for simpler vectorized ops\n",
    "        x_moved, perm, inv = self._move_last(x)\n",
    "        B = x_moved.shape[:-1]\n",
    "        L = x_moved.shape[-1]\n",
    "        y = x_moved.reshape(-1, L)          # [B*, L]\n",
    "        w = y.abs()\n",
    "        wsum = w.sum(dim=1, keepdim=True)\n",
    "        p = w / (wsum + self.eps)           # probabilities on simplex\n",
    "        neff = 1.0 / (p.pow(2).sum(dim=1, keepdim=True))   # [B*,1]\n",
    "        k = torch.floor(self.beta * neff).to(torch.long)\n",
    "        k = k.clamp_(min=1, max=L)          # at least 1, at most all\n",
    "\n",
    "        # Build mask (True=keep)\n",
    "        if self.mode == \"topk\":\n",
    "            _, idx = torch.sort(p, dim=1, descending=True)  # ranks per row\n",
    "            range_row = torch.arange(L, device=y.device).view(1, -1).expand(y.size(0), -1)\n",
    "            # broadcast k across columns\n",
    "            keep = range_row < k\n",
    "            mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "            mask.scatter_(1, idx, keep)\n",
    "        else:\n",
    "            # random, weighted by p_i via Gumbel-Top-k\n",
    "            g = -torch.log(-torch.log(torch.rand_like(y) + self.eps) + self.eps)\n",
    "            logits = torch.log(p + self.eps) + g\n",
    "            _, idx = torch.sort(logits, dim=1, descending=True)\n",
    "            range_row = torch.arange(L, device=y.device).view(1, -1).expand(y.size(0), -1)\n",
    "            keep = range_row < k\n",
    "            mask = torch.zeros_like(y, dtype=torch.bool)\n",
    "            mask.scatter_(1, idx, keep)\n",
    "\n",
    "        y_masked = y * mask\n",
    "        if self.keep_l1:\n",
    "            pre = wsum                        # sum|x| before\n",
    "            post = y_masked.abs().sum(dim=1, keepdim=True)\n",
    "            scale = pre / (post + self.eps)\n",
    "            y_masked = y_masked * scale\n",
    "\n",
    "        self.last_mask = mask.reshape(*B, L).permute(*inv).contiguous()\n",
    "        out = y_masked.reshape(*B, L).permute(*inv).contiguous()\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
