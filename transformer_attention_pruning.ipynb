{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04e0ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math, os, time, random, sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset  # optional\n",
    "    HAS_DATASETS = True\n",
    "except Exception:\n",
    "    HAS_DATASETS = False\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 1337):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Make CUDA deterministic where possible (slight perf hit)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(1337)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d443872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(dataset_name='tiny_shakespeare', block_size=128, batch_size=32, num_workers=0, d_model=192, n_layers=4, n_heads=4, d_ff=768, dropout=0.1, max_steps=1000, eval_every=200, learning_rate=0.0003, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, warmup_steps=100, compile_model=False, amp=True, ckpt_path='transformer_lm.pt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data / batching\n",
    "    dataset_name: str = \"tiny_shakespeare\"   # options: 'tiny_shakespeare', 'wikitext2', 'ptb'\n",
    "    block_size: int = 128                    # context length\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 0                     # set >0 for faster dataload (if CPU not overloaded)\n",
    "\n",
    "    # Model\n",
    "    d_model: int = 192\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 4\n",
    "    d_ff: int = 4 * 192\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Optimization\n",
    "    max_steps: int = 1000                    # increase for better results\n",
    "    eval_every: int = 200\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.95\n",
    "    grad_clip: float = 1.0\n",
    "    warmup_steps: int = 100\n",
    "    compile_model: bool = False              # PyTorch 2.x compile (if available)\n",
    "\n",
    "    # Misc\n",
    "    amp: bool = True                         # mixed precision on GPU\n",
    "    ckpt_path: str = \"transformer_lm.pt\"\n",
    "\n",
    "cfg = Config()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c777106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tiny_shakespeare_text():\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    try:\n",
    "        import requests\n",
    "        txt = requests.get(url, timeout=15).text\n",
    "        if len(txt) < 1000:\n",
    "            raise RuntimeError(\"Downloaded text seems too small.\")\n",
    "        return txt\n",
    "    except Exception:\n",
    "        # Small fallback so the notebook still runs offline\n",
    "        return (\n",
    "            \"From fairest creatures we desire increase,\\n\"\n",
    "            \"That thereby beauty's rose might never die,\\n\"\n",
    "            \"But as the riper should by time decease,\\n\"\n",
    "            \"His tender heir might bear his memory:\\n\"\n",
    "        )\n",
    "\n",
    "def load_text_splits(name: str):\n",
    "    \"\"\"\n",
    "    Returns (train_text, val_text, test_text) as plain strings.\n",
    "    \"\"\"\n",
    "    if name == \"tiny_shakespeare\":\n",
    "        text = load_tiny_shakespeare_text()\n",
    "        n = len(text)\n",
    "        # 90/5/5 split\n",
    "        train_text = text[: int(0.9*n)]\n",
    "        val_text   = text[int(0.9*n): int(0.95*n)]\n",
    "        test_text  = text[int(0.95*n):]\n",
    "        return train_text, val_text, test_text\n",
    "\n",
    "    if HAS_DATASETS and name in {\"wikitext2\", \"ptb\"}:\n",
    "        ds_name = \"wikitext\" if name == \"wikitext2\" else \"ptb_text_only\"\n",
    "        ds_conf = \"wikitext-2-raw-v1\" if name == \"wikitext2\" else \"penn_treebank\"\n",
    "        ds = load_dataset(ds_name, ds_conf)\n",
    "        # Join lines into a single string per split\n",
    "        def merge(split):\n",
    "            txts = ds[split][\"text\"]\n",
    "            return \"\\n\".join(t for t in txts if t is not None)\n",
    "        return merge(\"train\"), merge(\"validation\"), merge(\"test\")\n",
    "\n",
    "    print(f\"[INFO] Falling back to Tiny Shakespeare (datasets available: {HAS_DATASETS})\")\n",
    "    return load_text_splits(\"tiny_shakespeare\")\n",
    "\n",
    "class ByteTokenizer:\n",
    "    \"\"\"Simple byte-level tokenizer (vocab size 256).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 256\n",
    "    def encode(self, s: str):\n",
    "        return torch.tensor(list(s.encode(\"utf-8\")), dtype=torch.long)\n",
    "    def decode(self, ids: torch.Tensor):\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.detach().cpu().tolist()\n",
    "        return bytes(ids).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "tokenizer = ByteTokenizer()\n",
    "\n",
    "train_text, val_text, test_text = load_text_splits(cfg.dataset_name)\n",
    "\n",
    "train_ids = tokenizer.encode(train_text)\n",
    "val_ids   = tokenizer.encode(val_text)\n",
    "test_ids  = tokenizer.encode(test_text)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8daabcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003725, 50, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LMStreamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
    "        self.ids = ids\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of possible starting positions (we sample randomly anyway)\n",
    "        return max(1, len(self.ids) - self.block_size - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Allow DataLoader to ask for a specific index; we still do random sampling\n",
    "        # so epochs are not strictly deterministic across __getitem__ orders.\n",
    "        i = random.randint(0, len(self.ids) - self.block_size - 2)\n",
    "        x = self.ids[i: i + self.block_size]\n",
    "        y = self.ids[i+1: i + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "def make_eval_loader(ids: torch.Tensor, block_size: int, batch_size: int, num_batches: int = 50):\n",
    "    \"\"\"\n",
    "    Make a fixed evaluation loader by deterministic slicing (no randomness),\n",
    "    to keep eval stable across steps.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    stride = max(1, (len(ids) - (block_size + 1)) // (num_batches * batch_size))\n",
    "    if stride == 0:\n",
    "        stride = 1\n",
    "    start = 0\n",
    "    for _ in range(num_batches * batch_size):\n",
    "        if start + block_size + 1 > len(ids):\n",
    "            start = 0\n",
    "        x = ids[start: start + block_size]\n",
    "        y = ids[start+1: start + block_size + 1]\n",
    "        chunks.append((x, y))\n",
    "        start += stride\n",
    "\n",
    "    def collate(batch):\n",
    "        xs = torch.stack([b[0] for b in batch], dim=0)\n",
    "        ys = torch.stack([b[1] for b in batch], dim=0)\n",
    "        return xs, ys\n",
    "\n",
    "    ds = chunks\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate)\n",
    "\n",
    "train_ds = LMStreamDataset(train_ids, cfg.block_size)\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, drop_last=True)\n",
    "\n",
    "val_loader  = make_eval_loader(val_ids,  cfg.block_size, cfg.batch_size, num_batches=50)\n",
    "test_loader = make_eval_loader(test_ids, cfg.block_size, cfg.batch_size, num_batches=50)\n",
    "\n",
    "len(train_ds), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bdce2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1.90M\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask cached as buffer\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer(\"mask\", mask.view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        # reshape to (B, n_heads, T, head_dim)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v  # (B, n_heads, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout, block_size):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout, block_size)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = MLP(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, d_model, n_layers, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb   = nn.Parameter(torch.zeros(1, block_size, d_model))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(d_model, n_heads, d_ff, dropout, block_size)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size, \"Sequence length > block size\"\n",
    "        tok = self.token_emb(idx)                  # (B,T,C)\n",
    "        pos = self.pos_emb[:, :T, :]               # (1,T,C)\n",
    "        x = self.drop(tok + pos)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                      # (B,T,V)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=cfg.block_size,\n",
    "    d_model=cfg.d_model,\n",
    "    n_layers=cfg.n_layers,\n",
    "    n_heads=cfg.n_heads,\n",
    "    d_ff=cfg.d_ff,\n",
    "    dropout=cfg.dropout\n",
    ").to(device)\n",
    "\n",
    "if cfg.compile_model and hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {num_params/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d110f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=cfg.learning_rate,\n",
    "    betas=(cfg.beta1, cfg.beta2),\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "def get_lr(step):\n",
    "    # simple linear warmup, then cosine decay to 10% of base LR\n",
    "    if step < cfg.warmup_steps:\n",
    "        return cfg.learning_rate * (step + 1) / cfg.warmup_steps\n",
    "    # cosine\n",
    "    progress = (step - cfg.warmup_steps) / max(1, (cfg.max_steps - cfg.warmup_steps))\n",
    "    min_lr = cfg.learning_rate * 0.1\n",
    "    return min_lr + 0.5 * (cfg.learning_rate - min_lr) * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    tot_loss, tot_tokens, tot_correct = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits, loss = model(x, targets=y)\n",
    "        # Accumulate\n",
    "        token_count = y.numel()\n",
    "        tot_loss += loss.item() * token_count\n",
    "        tot_tokens += token_count\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        tot_correct += (preds == y).sum().item()\n",
    "    mean_loss = tot_loss / tot_tokens\n",
    "    ppl = math.exp(mean_loss)\n",
    "    acc = tot_correct / tot_tokens\n",
    "    return {\"loss\": mean_loss, \"perplexity\": ppl, \"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ba2f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_6956\\3272256343.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\" and cfg.amp))\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_6956\\3272256343.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == \"cuda\" and cfg.amp)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     1 | train loss 5.5796 | train ppl 264.98 | lr 3.00e-06\n",
      "step    50 | train loss 3.9733 | train ppl 53.16 | lr 1.50e-04\n",
      "step   100 | train loss 2.7831 | train ppl 16.17 | lr 3.00e-04\n",
      "step   150 | train loss 2.5812 | train ppl 13.21 | lr 2.98e-04\n",
      "step   200 | train loss 2.5008 | train ppl 12.19 | lr 2.92e-04\n",
      "[Eval @ step 200] val loss 2.4667 | val ppl 11.78 | val acc 0.2864\n",
      "step   250 | train loss 2.3981 | train ppl 11.00 | lr 2.82e-04\n",
      "step   300 | train loss 2.3737 | train ppl 10.74 | lr 2.69e-04\n",
      "step   350 | train loss 2.2819 | train ppl 9.80 | lr 2.52e-04\n",
      "step   400 | train loss 2.2425 | train ppl 9.42 | lr 2.33e-04\n",
      "[Eval @ step 400] val loss 2.2805 | val ppl 9.78 | val acc 0.3296\n",
      "step   450 | train loss 2.2318 | train ppl 9.32 | lr 2.12e-04\n",
      "step   500 | train loss 2.1678 | train ppl 8.74 | lr 1.89e-04\n",
      "step   550 | train loss 2.1373 | train ppl 8.48 | lr 1.65e-04\n",
      "step   600 | train loss 2.1047 | train ppl 8.20 | lr 1.42e-04\n",
      "[Eval @ step 600] val loss 2.1283 | val ppl 8.40 | val acc 0.3735\n",
      "step   650 | train loss 2.0761 | train ppl 7.97 | lr 1.19e-04\n",
      "step   700 | train loss 2.0660 | train ppl 7.89 | lr 9.79e-05\n",
      "step   750 | train loss 2.0240 | train ppl 7.57 | lr 7.86e-05\n",
      "step   800 | train loss 1.9627 | train ppl 7.12 | lr 6.19e-05\n",
      "[Eval @ step 800] val loss 2.0438 | val ppl 7.72 | val acc 0.3945\n",
      "step   850 | train loss 1.9857 | train ppl 7.28 | lr 4.83e-05\n",
      "step   900 | train loss 1.9779 | train ppl 7.23 | lr 3.83e-05\n",
      "step   950 | train loss 1.9639 | train ppl 7.13 | lr 3.21e-05\n",
      "step  1000 | train loss 1.9948 | train ppl 7.35 | lr 3.00e-05\n",
      "[Eval @ step 1000] val loss 2.0029 | val ppl 7.41 | val acc 0.4064\n",
      "Training finished in 7.9s\n",
      "Saved checkpoint to transformer_lm.pt\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\" and cfg.amp))\n",
    "\n",
    "global_step = 0\n",
    "t0 = time.time()\n",
    "\n",
    "model.train()\n",
    "for step, (x, y) in enumerate(iter(train_loader)):\n",
    "    if global_step >= cfg.max_steps:\n",
    "        break\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # LR schedule\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = get_lr(global_step)\n",
    "\n",
    "    # Forward/backward\n",
    "    with torch.cuda.amp.autocast(enabled=(device == \"cuda\" and cfg.amp)):\n",
    "        logits, loss = model(x, targets=y)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    # Gradient clipping\n",
    "    if cfg.grad_clip is not None:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    global_step += 1\n",
    "\n",
    "    if global_step % 50 == 0 or global_step == 1:\n",
    "        ppl = math.exp(loss.item())\n",
    "        print(f\"step {global_step:5d} | train loss {loss.item():.4f} | train ppl {ppl:.2f} | lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if global_step % cfg.eval_every == 0 or global_step == cfg.max_steps:\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        print(f\"[Eval @ step {global_step}] val loss {val_metrics['loss']:.4f} | \"\n",
    "              f\"val ppl {val_metrics['perplexity']:.2f} | val acc {val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "print(f\"Training finished in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"cfg\": cfg.__dict__,\n",
    "    \"vocab_size\": vocab_size,\n",
    "}, cfg.ckpt_path)\n",
    "print(f\"Saved checkpoint to {cfg.ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa81461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: loss 2.0341 | ppl 7.65 | acc 0.3988\n"
     ]
    }
   ],
   "source": [
    "test_metrics = evaluate(model, test_loader)\n",
    "print(f\"TEST: loss {test_metrics['loss']:.4f} | ppl {test_metrics['perplexity']:.2f} | acc {test_metrics['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb59c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "MIget say, and dears the wi'll if lose,\n",
      "O whe conie she mart and the would some: a ill phoursm wasch count\n",
      "to hencer ladop.\n",
      "\n",
      "DUCENGLIULE:\n",
      "Golt Goodst I I youll Wherere of he, ame,\n",
      "Whonk the to vill ha\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt, max_new_tokens=200, temperature=1.0, top_k=0):\n",
    "    model.eval()\n",
    "    ids = tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = ids[:, -cfg.block_size:]\n",
    "        logits, _ = model(idx_cond)\n",
    "        logits = logits[:, -1, :] / max(1e-8, temperature)\n",
    "        if top_k > 0:\n",
    "            v, ix = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float(\"inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        ids = torch.cat([ids, next_id], dim=1)\n",
    "    return tokenizer.decode(ids[0])\n",
    "\n",
    "sample = generate(model, prompt=\"ROMEO:\\n\", max_new_tokens=200, temperature=0.8, top_k=50)\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95430008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
