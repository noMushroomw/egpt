{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2fad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "folder = \"test_results\"\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d255b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pc(module:nn.Module, beta=1.0, method='magnitude') -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    if method == 'mean':\n",
    "        x = x - x.mean(dim=0, keepdim=True)\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = 1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0)\n",
    "    r_neff = torch.floor(beta * neff)\n",
    "    r_neff = r_neff.clamp(min=1, max=output_size-1)\n",
    "\n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < r_neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "def model_pc(model, renormalize=False, beta=1.0, method='magnitude'):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask, neff = mask_pc(module, beta=beta, method=method)\n",
    "            mask = mask.to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                pre = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    post = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                    module.weight.mul_(pre / post)\n",
    "    return model, neff\n",
    "\n",
    "def mask_pr(module:nn.Module, beta=1.0, method='magnitude') -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    if method == 'mean':\n",
    "        x = x - x.mean(dim=1, keepdim=True)\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=1, keepdim=True)\n",
    "    neff = 1/torch.sum((x_norm ** 2), dim=1, keepdim=True).squeeze(0)\n",
    "    r_neff = torch.floor(beta * neff)\n",
    "    r_neff = r_neff.clamp(min=1, max=input_size-1)\n",
    "\n",
    "    _, indices = torch.sort(x_norm, dim=1, descending=True)\n",
    "    range_tensor = torch.arange(input_size, device=x.device).unsqueeze(0).expand(output_size, -1)\n",
    "    sorted_mask = range_tensor < r_neff\n",
    "\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(1, indices, sorted_mask)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "def model_pr(model, renormalize=False, beta=1.0, method='magnitude'):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask, neff = mask_pr(module, beta=beta, method=method)\n",
    "            mask = mask.to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                pre = module.weight.abs().sum(dim=1, keepdim=True)\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    post = module.weight.abs().sum(dim=1, keepdim=True)\n",
    "                    module.weight.mul_(pre / post)\n",
    "    return model, neff\n",
    "\n",
    "def mask_block(module:nn.Module, beta=1.0, method='magnitude') -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    x = x.view(-1)\n",
    "    if method == 'mean':\n",
    "        x = x - torch.mean(x)\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x))\n",
    "    neff = 1/torch.sum((x_norm ** 2))\n",
    "    r_neff = torch.floor(beta * neff)\n",
    "    r_neff = r_neff.clamp(min=1, max=len(x)-1)\n",
    "\n",
    "    _, indices = torch.sort(x_norm, descending=True)\n",
    "    range_tensor = torch.arange(len(x), device=x.device)\n",
    "    sorted_mask = range_tensor < r_neff\n",
    "\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    mask = mask.view_as(module.weight)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "def model_block(model, renormalize=False, beta=1.0, method='magnitude'):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask, neff = mask_block(module, beta=beta, method=method)\n",
    "            mask = mask.to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                pre = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    post = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                    module.weight.mul_(pre / post)\n",
    "    return model, neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3751fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "def per_layer_neff(model):\n",
    "    \"\"\"Calculate the effective parameters (Neff) per layer\"\"\"\n",
    "    neff = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            layer_neff = torch.sum(param != 0).item()\n",
    "            neff[name] = layer_neff\n",
    "    return neff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd94ea",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b7aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model class with optional dropout\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=[512, 512, 512], dropout_rate=0.0):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Testing function\n",
    "def test(model, device, test_loader, times=1):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for _ in range(times):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(test_loss)\n",
    "\n",
    "    if times == 1:\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, '\n",
    "              f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "        return test_loss, accuracy\n",
    "    \n",
    "    else:\n",
    "        return loss_list, accuracy_list, sum(accuracy_list)/times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81452f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class with optional dropout\n",
    "class geluLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=[512, 512, 512], dropout_rate=0.0):\n",
    "        super(geluLinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.gelu(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        \n",
    "        \n",
    "# Model class with optional dropout\n",
    "class SigmoidLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=[512, 512, 512], dropout_rate=0.0):\n",
    "        super(SigmoidLinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.sigmoid(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        \n",
    "        \n",
    "        \n",
    "# Model class with optional dropout\n",
    "class tanhLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=[512, 512, 512], dropout_rate=0.0):\n",
    "        super(tanhLinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.tanh(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fab23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    # Underfit & brittle\n",
    "    'Tiny_Underfit':\n",
    "        {'hidden_size': [64, 32],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 10,\n",
    "        'dropout': 0.0},\n",
    "    # Deep-narrow (depth sensitivity)\n",
    "    'Deep_Narrow':         \n",
    "        {'hidden_size': [256, 256, 256, 256],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 15,\n",
    "        'dropout': 0.2},\n",
    "    # Well-trained baseline\n",
    "    'Balanced':            \n",
    "        {'hidden_size': [512, 256, 128],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 15,\n",
    "        'dropout': 0.2},\n",
    "    # Deep but still robust\n",
    "    'Balanced_Deep':       \n",
    "        {'hidden_size': [1024, 512, 256, 128],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 20,\n",
    "        'dropout': 0.3},\n",
    "    # Overparameterized\n",
    "    'Wide':\n",
    "        {'hidden_size': [2048, 1024, 512],\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 30,\n",
    "        'dropout': 0.0},\n",
    "    # Very overparameterized (optional, keep one)\n",
    "    'Very_Wide':          \n",
    "        {'hidden_size': [4096, 2048, 1024],\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 50,\n",
    "        'dropout': 0.0},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b0f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset registry ---------------------------------------------------------\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_loaders(dataset_name, batch_size=128, test_batch_size=1000, data_root='./data'):\n",
    "    \"\"\"\n",
    "    Returns: train_loader, test_loader, input_size, num_classes, meta (dict)\n",
    "    \"\"\"\n",
    "    name = dataset_name.lower()\n",
    "    meta = {}\n",
    "\n",
    "    # Generic normalizations (safe defaults). If you want canonical stats, compute them once.\n",
    "    NORM_1C = transforms.Normalize((0.5,), (0.5,))\n",
    "    NORM_3C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    if name == 'mnist':\n",
    "        # (You already have this; included for completeness.)\n",
    "        tfm = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train = datasets.MNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.MNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'fashionmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.FashionMNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.FashionMNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'kmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.KMNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.KMNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name in ('emnist_balanced', 'emnist'):\n",
    "        # EMNIST Balanced has 47 classes. If digits look rotated, add a Rotate(90) or permute.\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.EMNIST(data_root, split='balanced', train=True, download=True, transform=tfm)\n",
    "        test  = datasets.EMNIST(data_root, split='balanced', train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 47\n",
    "        meta['note'] = 'EMNIST images can appear rotated; for visualization add a 90-degree rotate.'\n",
    "\n",
    "    elif name == 'qmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.QMNIST(data_root, what='train', download=True, transform=tfm)\n",
    "        test  = datasets.QMNIST(data_root, what='test',  download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'svhn':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.SVHN(data_root, split='train', download=True, transform=tfm)\n",
    "        test  = datasets.SVHN(data_root, split='test',  download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 10\n",
    "\n",
    "    elif name == 'cifar10':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.CIFAR10(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR10(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 10\n",
    "\n",
    "    elif name == 'cifar100':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.CIFAR100(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR100(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 100\n",
    "\n",
    "    elif name in ('stl10', 'stl10_32'):\n",
    "        # Downsample to 32x32 to keep input dim manageable for MLPs.\n",
    "        tfm = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                  transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.STL10(data_root, split='train', download=True, transform=tfm)\n",
    "        test  = datasets.STL10(data_root, split='test',  download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 10\n",
    "        meta['note'] = 'Original STL10 is 96x96; here we resize to 32x32 for MLPs.'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=test_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, test_loader, inp, ncls, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19686dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307037\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.450680\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.359494\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.223820\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.132996\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.195590\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.124673\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.098699\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.216474\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.212997\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.161746\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.111778\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.092471\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.207054\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.155771\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.167088\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.094715\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.153166\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.122710\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.061944\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.114871\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.096485\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.103897\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.078031\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.074890\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.052916\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.078225\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.102485\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.113132\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.096364\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310855\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.363399\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.330387\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.322240\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.215038\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.083277\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.118213\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.098053\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.127823\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.140106\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.072981\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.063450\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.050388\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.158434\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.117661\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.049346\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.121621\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.063114\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.066278\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.052325\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.050907\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.056940\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.020515\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.051321\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.017898\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.062904\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.010908\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.056570\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.041391\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.080648\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.047192\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.078942\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.018011\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.010525\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.080074\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.030852\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.098518\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.048958\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.049057\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.003489\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.026883\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.033634\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.019642\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.042627\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.012798\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310485\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.284465\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.132454\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.229590\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.141128\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.149638\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.113161\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.099945\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.052004\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.045662\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.047994\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.130062\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.064779\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.047128\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.107022\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.032759\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.164811\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.076138\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.085233\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.034168\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.094082\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.010256\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.030271\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.085749\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.034362\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.021296\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.038932\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.115119\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.030048\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.082937\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.015106\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.010574\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.085932\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.003036\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.028660\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.032871\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.035632\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.000740\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.001306\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.035196\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.014072\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.002727\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.010091\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.027366\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.005726\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295575\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.291329\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.203015\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.103854\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.164374\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.095477\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.050948\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.131519\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.055427\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.060307\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.130282\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.055059\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.026169\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.370372\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.095505\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.032808\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.059083\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.064661\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.086494\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.018492\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.032208\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.025234\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.051928\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.054108\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.021474\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.014465\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.013692\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.029731\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.036068\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.027931\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.003615\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.020351\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.021575\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.100876\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.021115\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.058686\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.070274\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.011634\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.045607\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.037453\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.007827\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.003121\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.017336\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.035778\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.057542\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.043752\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.016501\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.006982\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.011017\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.005930\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.015630\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.003013\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.004616\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.010385\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.087527\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.020815\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.019735\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.022528\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.002039\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.010208\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302504\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.195201\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.138580\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.096532\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.124858\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.025949\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.116286\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.082433\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.015354\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029474\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.057487\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.055561\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.040963\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.056054\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.041734\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.010828\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.040978\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.029476\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.017072\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.018385\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.052778\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.051797\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.046057\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.021116\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001538\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.061687\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.047055\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.054664\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.021699\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.003862\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.025999\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.003235\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.000521\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.000991\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.064544\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.012585\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.019331\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.005255\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.003213\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000261\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.019715\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.184291\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.008621\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.000841\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.000245\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000268\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.007382\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.003744\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.001977\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.001995\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.003623\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.019130\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.000756\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.000049\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000065\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.013745\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.021053\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.002222\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.004106\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.098512\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.002070\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.005371\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.000094\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.000738\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.000373\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.004842\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.000224\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.006392\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.007723\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.004397\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.000052\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.006090\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.004814\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.000085\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.000276\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.010867\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.005636\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.000509\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.017152\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.000339\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.001808\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.091057\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.000570\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.000106\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000301\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.004609\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.019336\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.004549\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.000077\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.000002\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299709\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.077110\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.168011\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.056960\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.108038\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.060992\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.031625\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.025028\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.120369\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.057024\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.024952\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.037260\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.035342\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.056565\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.018812\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.015942\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.105239\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.022442\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.027390\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.075884\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.015999\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.042216\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.005685\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.085674\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000282\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.018873\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002847\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001656\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.003657\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.013695\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.004646\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.035441\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.022804\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.029823\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.003018\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.009686\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.051135\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.067626\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.092704\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.001526\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.004801\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.006517\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.032888\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.032099\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.007628\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.003852\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.015957\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.032433\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.039717\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.000110\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.024239\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.005916\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.028389\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.004542\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.003073\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.029982\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.004897\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.009603\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.015663\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.000602\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.008392\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.000888\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.005623\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.000074\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.000176\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.005946\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.000344\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.041847\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.054581\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.012230\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.009212\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.004114\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.008568\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.018348\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.000126\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.002507\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.000205\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.009370\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.098510\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.012494\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.004510\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.000268\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.000086\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.000009\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.026569\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.003944\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.003829\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.000479\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.000801\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.000084\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.001262\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.000003\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.003377\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.030111\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.045341\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.004236\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.000907\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.000134\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.001235\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.008179\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.007634\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.002583\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.005406\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.000281\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.003993\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.045801\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.020509\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.003344\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.000056\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.099000\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.003883\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.000016\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.028654\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.000006\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.026409\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.000170\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.000535\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.001626\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.056447\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.000030\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.002949\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.000715\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.000537\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.000806\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.002917\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.001756\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.000092\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.001048\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.000250\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.000506\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.018111\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.000294\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.000528\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.000607\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.000001\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.012815\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.000018\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.000032\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.000079\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.030298\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.012678\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.012093\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.004526\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.029134\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.000214\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.000110\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.000156\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# Train all models\n",
    "all_results = {}\n",
    "\n",
    "datasets_name = 'mnist'\n",
    "\n",
    "train_loader, test_loader, input_size, num_classes, meta = get_loaders(datasets_name, batch_size=128)\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input({input_size}) -> {' -> '.join(map(str, config['hidden_size']))} -> Output({num_classes})\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LinearModel(\n",
    "        input_size=input_size, \n",
    "        output_size=num_classes, \n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    model.save(f'paper_{datasets_name}/{model_name}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f9f0009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293446\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.500287\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.226188\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.447308\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.241890\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.185592\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.242185\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.151940\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.124730\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.204648\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.117743\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.127246\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.060981\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.131397\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.127316\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.196561\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.061120\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.121076\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.074498\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.097487\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.092114\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.128988\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.093098\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.026684\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.135934\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.065424\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.089164\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.112284\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.045478\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.042821\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303520\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.333420\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.211237\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.157643\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.137207\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.058472\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.186367\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.142593\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.141340\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.096877\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.098139\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.081678\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.087005\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.083757\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.122765\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.060558\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.017589\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.045910\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.043796\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.026124\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.151367\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.065309\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.042486\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.035586\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.021792\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.109592\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.131835\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.027269\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.059401\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.036901\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.083806\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.011625\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.023377\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.038379\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.037726\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.070346\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.033609\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.044921\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.024400\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.014293\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.027928\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.030566\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.007551\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.037680\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.040051\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295754\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.366000\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.163773\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.261977\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.184347\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.118732\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.084455\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.144027\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.166015\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.075587\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.084830\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.029241\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.018156\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.123551\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.034660\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.032431\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.021311\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.048803\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.027829\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.029964\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.035815\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.023516\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.029538\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.008596\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.019689\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.054268\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.005134\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.018556\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.055147\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.007209\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.022595\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.005100\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.008973\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.009168\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.001388\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.045400\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.026828\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.010503\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.026018\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.005205\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.006680\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.014137\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.001701\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.012746\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.017790\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299257\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.432335\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.183527\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.127867\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.061843\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.244586\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.073143\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.182725\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.193073\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.051305\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.060570\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.049015\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.071232\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.090276\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.038439\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.057559\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.027150\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.071393\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.054015\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.060037\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.145576\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.056183\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.030304\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.014517\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.095821\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.057937\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.031735\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.106836\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.007151\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.031876\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.004658\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.004265\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.017857\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.018241\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.029234\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.007702\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.068204\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.016284\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.014522\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.002882\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.009559\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.007155\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.006055\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.013638\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.010031\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000904\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.037087\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.004642\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.039259\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.006109\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.048372\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.004482\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.002416\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.044921\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.015201\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.013188\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.041619\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.023261\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.004787\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.004732\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307120\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.135813\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.114532\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.060190\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.084209\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.082023\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.018697\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.053665\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.092511\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019608\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.075132\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.059387\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.064939\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.098311\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.035752\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.057397\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.047942\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.012708\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.063726\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.080532\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.010537\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.012623\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.011077\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.024324\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.014428\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.008475\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.045742\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.027647\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.028100\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002840\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.006808\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.022002\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.039403\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.007548\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.001330\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.011946\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.003692\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.083752\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.006933\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.040236\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.124500\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.011069\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.037418\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.031714\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.007959\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.034129\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.001234\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.008998\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.011501\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.000624\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.001809\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.013899\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.077154\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.033377\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000649\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.001981\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.002882\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.027658\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.014693\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.019256\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.006969\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.075629\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.001710\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.025063\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.006932\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.007430\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.017116\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.007658\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.002013\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.000514\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.000664\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.000338\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.008727\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.000046\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.039193\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.002633\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.042687\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.014266\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.007162\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.000704\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.004196\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.000036\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.000406\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.000575\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.001936\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.000625\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.000306\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.044577\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.000283\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.060386\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300111\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.236104\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.168150\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.058949\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.131540\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.118750\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.041893\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.088611\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.076027\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.124526\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.100755\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.032957\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.019004\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.022303\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.010744\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.007440\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.048062\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.004936\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.011443\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.034711\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.019076\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.012870\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.009177\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003870\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.003538\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.019943\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002306\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.010806\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.020733\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.005264\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.004423\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.052979\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.002023\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.048882\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.046995\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.011567\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.055089\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.024723\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.004176\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000207\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.026096\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.014000\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.009121\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.001579\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.032656\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.034797\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.004373\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.034036\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.173342\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.006053\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.002996\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.001257\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.006109\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.000088\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.028108\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.000069\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.024377\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.053367\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.000147\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.006055\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.006769\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.000342\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.000388\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.011430\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.008243\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.020732\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.013892\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.000057\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.112666\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.003561\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.097762\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.000291\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.019986\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.000059\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.003150\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.008731\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.000215\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.004476\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.000015\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.004602\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.007715\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.003061\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.006008\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.020429\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.001759\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.023822\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.006498\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.005347\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.000294\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.098502\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.000648\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.000417\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.000872\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.000144\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.051692\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.000066\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.003457\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.000829\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.040597\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.000018\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.000334\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.000050\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.000885\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.000013\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.000463\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.032763\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.036081\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.095586\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.000617\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.006635\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.001613\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.000656\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.019340\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.000215\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.000004\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.045514\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.000069\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.000484\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.000014\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.000037\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.015123\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.006397\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.000017\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.069976\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.000110\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.001157\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.148156\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.000000\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.000474\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.000079\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.004180\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.000001\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.000019\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.000566\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.068426\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.003578\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.002423\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.040082\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.002481\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.000013\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.000101\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.002718\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.000033\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.001136\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.000286\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.024570\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.016382\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.371987\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.894414\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.547931\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.421641\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.114491\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.843955\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.868989\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.702053\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.596369\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.548901\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.425141\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.370059\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.432812\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.331751\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.374751\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.349290\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.270082\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.285347\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.337320\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.276284\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.187440\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.198693\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.206075\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.196909\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.192316\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.225802\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.213923\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.120011\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.138127\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.219890\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.384041\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.523836\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.014488\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.889814\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.512856\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.581775\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.593971\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.339665\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.397399\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.461453\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.293423\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.208663\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.349402\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.310599\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.188815\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.277245\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.436111\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.278858\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.170340\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.207628\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.263059\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.156147\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.108182\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.106656\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.175949\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.133420\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.221607\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.136617\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.076100\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.279451\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.118533\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.107724\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.125467\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.078850\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.053040\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.189486\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.167046\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.118884\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.049721\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.108590\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.104783\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.109636\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.090007\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.109074\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.034325\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316169\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.211222\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.763308\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.500957\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.337068\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.274101\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.378860\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.363969\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.099691\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.110038\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.089230\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.180098\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.145002\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.137055\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.141305\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.153064\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.090773\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.109809\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.098715\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.094209\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.084411\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.072771\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.099607\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.103129\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.091996\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.102263\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.078153\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.029420\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.062130\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.059960\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.053547\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.043657\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.136011\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.042916\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.040014\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.043921\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.096989\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.054711\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.077020\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.022278\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.077740\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.095581\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.028517\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.026642\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.036496\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.334511\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.370499\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.761571\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.732275\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.559005\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.369289\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.247878\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.395654\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.274918\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.141078\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.212735\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.171573\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.254386\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.279622\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.150732\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.163308\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.173411\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.210520\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.085121\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.274546\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.109390\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.044356\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.095183\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.058131\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.060731\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.077129\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.105640\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.039149\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.069870\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.123933\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.071869\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.057076\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.103926\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.078653\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.065983\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.075387\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.069538\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.030384\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.043685\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.108964\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.122919\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.079006\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.025269\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.052260\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.058248\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.082161\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.036727\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.138427\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.035192\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.019527\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.031539\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.063674\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.198994\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.065735\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.069605\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.017916\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.050028\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.040664\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.107923\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.019646\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.318581\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.300513\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.174506\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.122097\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.087882\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.114589\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.116374\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.082266\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.126791\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.073606\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.028970\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.093371\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.079420\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.078238\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.005185\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.010947\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.019579\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.027149\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.029151\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.000778\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.011337\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.004103\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.011204\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.011872\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.028564\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.003397\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.011408\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.010675\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.006274\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.010749\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.068872\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.000947\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.013355\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.004300\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.000733\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.006134\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.002623\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.001646\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.018357\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000407\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.001663\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.003163\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.000958\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.001742\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.022268\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.001553\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.001350\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.006832\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000448\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.025494\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.002991\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.005569\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.001878\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.006894\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000121\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.000467\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.000179\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.010722\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.003375\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.000198\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.021437\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.001021\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.069215\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.009633\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.004619\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.007811\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.000479\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.000148\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.000413\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.007630\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.061801\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.001315\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.009280\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.000240\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.000035\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.006386\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.000022\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.005244\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.000137\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.001104\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.000297\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.000364\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.001800\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.001881\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.027717\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.000959\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.013350\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.000356\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.002892\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.000593\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298389\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.256663\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.148786\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.057699\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.163702\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.080669\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.019894\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.050705\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.080387\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.017093\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.014361\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.010089\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.009742\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.020965\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.113158\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004217\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.049752\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.091169\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.002570\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.006193\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.008573\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.032901\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.016304\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.033637\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001074\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.081289\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002232\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.019726\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.003718\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.043568\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.015097\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.002986\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.059850\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.057791\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.002152\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.010538\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.002742\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.004850\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.001702\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.001869\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.000812\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.007278\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.002610\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.013736\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.002583\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000993\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.000039\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.062282\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000159\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.006653\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.007124\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.001191\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.000407\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.005097\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.002395\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.010115\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.019789\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.002327\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.034631\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.024987\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.022875\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.000081\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.000488\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.000775\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.000402\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.015142\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.002820\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.001424\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.015766\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.000044\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.000111\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.007657\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.000308\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.000530\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.008231\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.000454\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.000742\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.000084\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.050256\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.050637\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.005239\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.001841\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.000055\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.000036\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.051244\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.024471\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.000118\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.000559\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.002389\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.125683\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.004214\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.006030\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.002175\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.000103\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.000730\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.004603\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.008034\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.000068\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.000769\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.000060\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.007152\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.000081\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.000449\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.000437\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.000041\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.003003\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.000089\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.002704\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.044412\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.038593\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.001315\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.010247\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.000253\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.000144\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.000063\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.000115\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.000044\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.001207\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.000270\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.000271\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.000035\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.061110\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.000247\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.000034\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.006800\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.002097\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.006647\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.000258\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.002224\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.000227\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.000170\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.027144\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.000594\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.000041\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.005247\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.002213\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.001196\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.001226\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.000399\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.000080\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.000053\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.001191\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.000002\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.001008\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.000046\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.016999\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.000071\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.000336\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.000830\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.014206\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296278\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.729803\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.443547\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.463065\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.345380\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.227705\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.238375\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.234907\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.187360\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.226146\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.071691\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.117459\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.188771\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.173621\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.183642\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.135286\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.084743\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.038791\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.050851\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.026350\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.108587\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.065533\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.090974\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.103641\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.055100\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.095178\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.112815\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.055463\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.099383\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.059363\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314323\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.324734\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.131238\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.206122\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.212909\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.238572\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.136519\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.131575\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.187314\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.179144\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.060098\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.057063\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.183528\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.109572\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.103947\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.065076\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.107001\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.073860\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.064962\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.049200\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.092452\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.044419\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.045944\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.047501\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.073992\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.130593\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.108917\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.036565\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.054767\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.077028\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.018654\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.067938\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.026205\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.010527\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.088417\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.037438\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.004122\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.058053\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.022480\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.024139\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.121617\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.027800\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.009843\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.107380\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.077829\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300102\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.223125\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.157543\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.212400\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.231032\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.194048\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.145781\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.089672\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.202407\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.045226\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.086116\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.101466\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.070332\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.024795\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.113314\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.091948\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.068874\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.051959\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.086632\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.111596\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.099676\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.071298\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.026748\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.047714\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.042585\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.051641\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.034289\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.017882\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.043840\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.042694\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.037449\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.028213\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.011352\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.040797\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.006435\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.054428\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.010347\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.026831\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.017086\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.014258\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.038922\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.073364\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.007596\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.007471\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.028596\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303147\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.330312\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.252265\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.156988\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.307015\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.143701\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.112965\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.117211\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.106900\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.072509\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.129553\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.124353\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.053040\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.042523\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.088291\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.098024\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.037912\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.089675\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.022768\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.020487\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.063564\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.078270\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.071186\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.068755\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.097113\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.076640\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.057772\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.054796\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.025865\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.050838\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.016095\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.033212\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.022971\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.009309\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.021336\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.013076\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.054122\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.119268\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.017131\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.012672\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.008716\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.020804\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.026778\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.062022\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.041452\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.008101\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.045458\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.057467\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.033606\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.022992\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.033024\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.020341\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.017861\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.035378\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.039718\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.079512\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.052453\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.014231\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.024464\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.041692\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.281768\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.246691\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.141315\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.142075\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.068130\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.091276\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.062832\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.114423\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.022564\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.076568\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.125808\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.055369\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.054892\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.036065\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.029019\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.029055\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.057089\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.078741\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.024149\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.046078\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.048431\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.066573\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.035479\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.039001\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.038638\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.044663\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.034693\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.033996\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.013854\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.018569\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.025170\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.032612\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.055137\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.016631\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.011303\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.056563\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.014217\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.052030\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.093956\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.007754\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.007604\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.010818\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.026803\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.033237\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.085850\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.005200\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.004478\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.043926\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.035156\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.053597\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.020692\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.005206\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.041592\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.039376\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.023954\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.025051\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.008055\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.039729\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.096321\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.040424\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.012010\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.011157\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.008710\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.030563\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.019244\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.034362\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.009425\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.005697\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.003240\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.014198\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.010351\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.019197\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.011124\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.007292\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.004024\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.040687\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.057449\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.040387\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.010698\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.007625\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.023288\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.003179\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.009354\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.004349\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.042891\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.036367\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.038043\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.017565\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.040311\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.037754\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315268\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.086174\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.091866\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.124980\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.166459\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.176117\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.094111\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.109293\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.037740\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.063789\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.127248\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.111277\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.028440\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.106802\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.110135\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.010381\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.051237\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.102723\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.097278\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.018657\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.103546\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.087757\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.018139\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.054113\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.063254\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.066708\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.016390\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.089688\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.022315\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.055844\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.092861\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.029925\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.111530\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.039707\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.027224\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.087934\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.022863\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.083815\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.021857\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.044511\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.065857\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.071012\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.076681\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.010189\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.068128\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.015279\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.025542\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.049421\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.009945\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.176042\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.029187\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.019638\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.036643\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.021140\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.029029\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.029359\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.128303\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.042910\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.007026\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.017926\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.014713\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.032636\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.115859\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.034763\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.079612\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.063317\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.025094\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.025605\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.020291\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.017720\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.016029\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.011101\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.021016\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.015935\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.096843\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.019738\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.033231\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.034644\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.021223\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.009719\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.009690\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.036640\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.076073\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.056430\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.068061\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.019676\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.057926\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.033179\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.008113\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.039459\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.046914\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.012821\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.063507\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.025028\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.003300\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.013697\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.076479\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.116561\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.007335\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.019331\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.042309\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.048324\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.017620\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.078525\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.073798\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.021802\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.052793\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.083489\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.030047\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.023695\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.013598\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.013060\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.014569\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.015651\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.013882\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.078253\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.031026\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.043896\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.036008\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.079343\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.035118\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.045984\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.030175\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.069068\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.104313\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.023840\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.022968\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.012435\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.049027\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.019256\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.014204\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.010435\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.025260\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.044672\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.008695\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.050823\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.057127\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.039761\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.028989\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.031480\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.034581\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.076909\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.018159\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.032513\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.041331\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.063278\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.029939\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.033069\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.068107\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.031332\n"
     ]
    }
   ],
   "source": [
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model1 = geluLinearModel(\n",
    "        input_size=input_size, \n",
    "        output_size=num_classes, \n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model1.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model1, device, train_loader, optimizer, epoch)\n",
    "    model1.save(f'paper_{datasets_name}/gelu_{model_name}.pth')\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model2 = SigmoidLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model2.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model2, device, train_loader, optimizer, epoch)\n",
    "    model2.save(f'paper_{datasets_name}/sigmoid_{model_name}.pth')\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model3 = tanhLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model3.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model3, device, train_loader, optimizer, epoch)\n",
    "    model3.save(f'paper_{datasets_name}/tanh_{model_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294a762",
   "metadata": {},
   "source": [
    "# pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf9a5526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 52,650\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%\n",
      "per column magnitude pruning test accuracy: 95.87%, sparsity: 0.4184\n",
      "per row magnitude pruning test accuracy: 95.67%, sparsity: 0.3591\n",
      "per block magnitude pruning test accuracy: 96.05%, sparsity: 0.4209\n",
      "mean column mean pruning test accuracy: 95.13%, sparsity: 0.4087\n",
      "mean row mean pruning test accuracy: 94.83%, sparsity: 0.4209\n",
      "mean block mean pruning test accuracy: 95.88%, sparsity: 0.4209\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 400,906\n",
      "model name: Deep_Narrow, test accuracy: 98.23%\n",
      "per column magnitude pruning test accuracy: 98.20%, sparsity: 0.3647\n",
      "per row magnitude pruning test accuracy: 98.26%, sparsity: 0.3334\n",
      "per block magnitude pruning test accuracy: 98.25%, sparsity: 0.3644\n",
      "mean column mean pruning test accuracy: 98.15%, sparsity: 0.3605\n",
      "mean row mean pruning test accuracy: 98.24%, sparsity: 0.3644\n",
      "mean block mean pruning test accuracy: 98.17%, sparsity: 0.3644\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 567,434\n",
      "model name: Balanced, test accuracy: 98.44%\n",
      "per column magnitude pruning test accuracy: 98.45%, sparsity: 0.3794\n",
      "per row magnitude pruning test accuracy: 98.44%, sparsity: 0.3359\n",
      "per block magnitude pruning test accuracy: 98.45%, sparsity: 0.3793\n",
      "mean column mean pruning test accuracy: 98.27%, sparsity: 0.3739\n",
      "mean row mean pruning test accuracy: 98.36%, sparsity: 0.3793\n",
      "mean block mean pruning test accuracy: 98.45%, sparsity: 0.3793\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 1,494,154\n",
      "model name: Balanced_Deep, test accuracy: 98.31%\n",
      "per column magnitude pruning test accuracy: 98.25%, sparsity: 0.3788\n",
      "per row magnitude pruning test accuracy: 98.31%, sparsity: 0.3408\n",
      "per block magnitude pruning test accuracy: 98.30%, sparsity: 0.3784\n",
      "mean column mean pruning test accuracy: 98.04%, sparsity: 0.3737\n",
      "mean row mean pruning test accuracy: 98.21%, sparsity: 0.3784\n",
      "mean block mean pruning test accuracy: 98.34%, sparsity: 0.3784\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 4,235,786\n",
      "model name: Wide, test accuracy: 98.26%\n",
      "per column magnitude pruning test accuracy: 98.10%, sparsity: 0.4934\n",
      "per row magnitude pruning test accuracy: 98.02%, sparsity: 0.4155\n",
      "per block magnitude pruning test accuracy: 98.25%, sparsity: 0.5081\n",
      "mean column mean pruning test accuracy: 97.41%, sparsity: 0.4526\n",
      "mean row mean pruning test accuracy: 97.79%, sparsity: 0.5081\n",
      "mean block mean pruning test accuracy: 97.99%, sparsity: 0.5081\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 13,714,442\n",
      "model name: Very_Wide, test accuracy: 98.37%\n",
      "per column magnitude pruning test accuracy: 98.25%, sparsity: 0.5921\n",
      "per row magnitude pruning test accuracy: 97.89%, sparsity: 0.4511\n",
      "per block magnitude pruning test accuracy: 98.19%, sparsity: 0.6015\n",
      "mean column mean pruning test accuracy: 97.12%, sparsity: 0.4926\n",
      "mean row mean pruning test accuracy: 97.49%, sparsity: 0.6015\n",
      "mean block mean pruning test accuracy: 98.10%, sparsity: 0.6015\n"
     ]
    }
   ],
   "source": [
    "# Results storage\n",
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_mnist/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    \n",
    "    # magnitude\n",
    "    pc_model, pc_neff = model_pc(model, renormalize=False, beta=1.0, method='magnitude')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pc_1_magnitude\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "    \n",
    "    pr_model, pr_neff = model_pr(model, renormalize=False, beta=1.0, method='magnitude')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pr_1_magnitude\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "    \n",
    "    pb_model, pb_neff = model_block(model, renormalize=False, beta=1.0, method='magnitude')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pb_1_magnitude\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "\n",
    "    # mean\n",
    "    mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=1.0, method='mean')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pc_1_mean\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "\n",
    "    mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=1.0, method='mean')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pr_1_mean\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "\n",
    "    mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=1.0, method='mean')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pb_1_mean\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "\n",
    "    # summary\n",
    "    print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%\")\n",
    "    print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "    print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "    print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "    print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "    print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "    print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a18c76",
   "metadata": {},
   "source": [
    "# beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "141df133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 52,650\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 50.68%, sparsity: 0.7860\n",
      "per row magnitude pruning test accuracy: 63.63%, sparsity: 0.7696\n",
      "per block magnitude pruning test accuracy: 70.32%, sparsity: 0.7870\n",
      "mean column mean pruning test accuracy: 48.38%, sparsity: 0.7832\n",
      "mean row mean pruning test accuracy: 59.36%, sparsity: 0.7870\n",
      "mean block mean pruning test accuracy: 65.60%, sparsity: 0.7870\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 64.61%, sparsity: 0.7387\n",
      "per row magnitude pruning test accuracy: 75.00%, sparsity: 0.7166\n",
      "per block magnitude pruning test accuracy: 73.39%, sparsity: 0.7398\n",
      "mean column mean pruning test accuracy: 66.65%, sparsity: 0.7349\n",
      "mean row mean pruning test accuracy: 72.85%, sparsity: 0.7398\n",
      "mean block mean pruning test accuracy: 77.88%, sparsity: 0.7398\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 79.01%, sparsity: 0.6808\n",
      "per row magnitude pruning test accuracy: 82.83%, sparsity: 0.6518\n",
      "per block magnitude pruning test accuracy: 83.21%, sparsity: 0.6822\n",
      "mean column mean pruning test accuracy: 80.42%, sparsity: 0.6760\n",
      "mean row mean pruning test accuracy: 83.37%, sparsity: 0.6822\n",
      "mean block mean pruning test accuracy: 86.88%, sparsity: 0.6822\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 82.65%, sparsity: 0.6101\n",
      "per row magnitude pruning test accuracy: 90.33%, sparsity: 0.5729\n",
      "per block magnitude pruning test accuracy: 87.34%, sparsity: 0.6118\n",
      "mean column mean pruning test accuracy: 86.45%, sparsity: 0.6040\n",
      "mean row mean pruning test accuracy: 91.51%, sparsity: 0.6118\n",
      "mean block mean pruning test accuracy: 90.80%, sparsity: 0.6118\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 93.57%, sparsity: 0.5238\n",
      "per row magnitude pruning test accuracy: 91.42%, sparsity: 0.4767\n",
      "per block magnitude pruning test accuracy: 95.41%, sparsity: 0.5259\n",
      "mean column mean pruning test accuracy: 92.64%, sparsity: 0.5161\n",
      "mean row mean pruning test accuracy: 92.26%, sparsity: 0.5259\n",
      "mean block mean pruning test accuracy: 95.41%, sparsity: 0.5259\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 95.87%, sparsity: 0.4184\n",
      "per row magnitude pruning test accuracy: 95.67%, sparsity: 0.3591\n",
      "per block magnitude pruning test accuracy: 96.05%, sparsity: 0.4209\n",
      "mean column mean pruning test accuracy: 95.13%, sparsity: 0.4087\n",
      "mean row mean pruning test accuracy: 94.83%, sparsity: 0.4209\n",
      "mean block mean pruning test accuracy: 95.88%, sparsity: 0.4209\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 96.19%, sparsity: 0.2896\n",
      "per row magnitude pruning test accuracy: 96.34%, sparsity: 0.2154\n",
      "per block magnitude pruning test accuracy: 96.44%, sparsity: 0.2926\n",
      "mean column mean pruning test accuracy: 95.98%, sparsity: 0.2777\n",
      "mean row mean pruning test accuracy: 96.03%, sparsity: 0.2926\n",
      "mean block mean pruning test accuracy: 96.45%, sparsity: 0.2926\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 96.37%, sparsity: 0.1326\n",
      "per row magnitude pruning test accuracy: 96.30%, sparsity: 0.0599\n",
      "per block magnitude pruning test accuracy: 96.43%, sparsity: 0.1364\n",
      "mean column mean pruning test accuracy: 96.25%, sparsity: 0.1196\n",
      "mean row mean pruning test accuracy: 96.29%, sparsity: 0.1364\n",
      "mean block mean pruning test accuracy: 96.48%, sparsity: 0.1364\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 96.35%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 96.42%, sparsity: 0.0194\n",
      "per block magnitude pruning test accuracy: 96.44%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 96.21%, sparsity: 0.0139\n",
      "mean row mean pruning test accuracy: 96.34%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 96.43%, sparsity: 0.0001\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 96.36%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 96.42%, sparsity: 0.0171\n",
      "per block magnitude pruning test accuracy: 96.44%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 96.21%, sparsity: 0.0036\n",
      "mean row mean pruning test accuracy: 96.35%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 96.43%, sparsity: 0.0001\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.44%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 96.36%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 96.42%, sparsity: 0.0168\n",
      "per block magnitude pruning test accuracy: 96.44%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 96.21%, sparsity: 0.0020\n",
      "mean row mean pruning test accuracy: 96.35%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 96.43%, sparsity: 0.0001\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 400,906\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 92.56%, sparsity: 0.7663\n",
      "per row magnitude pruning test accuracy: 96.43%, sparsity: 0.7562\n",
      "per block magnitude pruning test accuracy: 97.27%, sparsity: 0.7662\n",
      "mean column mean pruning test accuracy: 94.94%, sparsity: 0.7656\n",
      "mean row mean pruning test accuracy: 97.00%, sparsity: 0.7662\n",
      "mean block mean pruning test accuracy: 97.29%, sparsity: 0.7662\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 95.67%, sparsity: 0.7146\n",
      "per row magnitude pruning test accuracy: 97.43%, sparsity: 0.7017\n",
      "per block magnitude pruning test accuracy: 97.72%, sparsity: 0.7144\n",
      "mean column mean pruning test accuracy: 97.14%, sparsity: 0.7134\n",
      "mean row mean pruning test accuracy: 97.53%, sparsity: 0.7144\n",
      "mean block mean pruning test accuracy: 97.79%, sparsity: 0.7144\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.57%, sparsity: 0.6514\n",
      "per row magnitude pruning test accuracy: 97.77%, sparsity: 0.6352\n",
      "per block magnitude pruning test accuracy: 98.06%, sparsity: 0.6512\n",
      "mean column mean pruning test accuracy: 97.73%, sparsity: 0.6497\n",
      "mean row mean pruning test accuracy: 97.83%, sparsity: 0.6512\n",
      "mean block mean pruning test accuracy: 98.08%, sparsity: 0.6512\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 98.13%, sparsity: 0.5742\n",
      "per row magnitude pruning test accuracy: 98.17%, sparsity: 0.5539\n",
      "per block magnitude pruning test accuracy: 98.16%, sparsity: 0.5739\n",
      "mean column mean pruning test accuracy: 98.05%, sparsity: 0.5718\n",
      "mean row mean pruning test accuracy: 97.96%, sparsity: 0.5739\n",
      "mean block mean pruning test accuracy: 98.14%, sparsity: 0.5739\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.26%, sparsity: 0.4799\n",
      "per row magnitude pruning test accuracy: 98.30%, sparsity: 0.4547\n",
      "per block magnitude pruning test accuracy: 98.22%, sparsity: 0.4796\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.4767\n",
      "mean row mean pruning test accuracy: 98.27%, sparsity: 0.4796\n",
      "mean block mean pruning test accuracy: 98.25%, sparsity: 0.4796\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.20%, sparsity: 0.3647\n",
      "per row magnitude pruning test accuracy: 98.26%, sparsity: 0.3334\n",
      "per block magnitude pruning test accuracy: 98.25%, sparsity: 0.3644\n",
      "mean column mean pruning test accuracy: 98.15%, sparsity: 0.3605\n",
      "mean row mean pruning test accuracy: 98.24%, sparsity: 0.3644\n",
      "mean block mean pruning test accuracy: 98.17%, sparsity: 0.3644\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.23%, sparsity: 0.2241\n",
      "per row magnitude pruning test accuracy: 98.25%, sparsity: 0.1855\n",
      "per block magnitude pruning test accuracy: 98.27%, sparsity: 0.2236\n",
      "mean column mean pruning test accuracy: 98.14%, sparsity: 0.2187\n",
      "mean row mean pruning test accuracy: 98.32%, sparsity: 0.2236\n",
      "mean block mean pruning test accuracy: 98.22%, sparsity: 0.2236\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.27%, sparsity: 0.0540\n",
      "per row magnitude pruning test accuracy: 98.23%, sparsity: 0.0295\n",
      "per block magnitude pruning test accuracy: 98.22%, sparsity: 0.0537\n",
      "mean column mean pruning test accuracy: 98.19%, sparsity: 0.0534\n",
      "mean row mean pruning test accuracy: 98.24%, sparsity: 0.0537\n",
      "mean block mean pruning test accuracy: 98.24%, sparsity: 0.0537\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.22%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.22%, sparsity: 0.0048\n",
      "per block magnitude pruning test accuracy: 98.23%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.0044\n",
      "mean row mean pruning test accuracy: 98.24%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.23%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.22%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.23%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 98.23%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.0026\n",
      "mean row mean pruning test accuracy: 98.25%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.23%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.23%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.22%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.23%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 98.23%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.0026\n",
      "mean row mean pruning test accuracy: 98.25%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.23%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 567,434\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 89.88%, sparsity: 0.7717\n",
      "per row magnitude pruning test accuracy: 97.59%, sparsity: 0.7566\n",
      "per block magnitude pruning test accuracy: 96.60%, sparsity: 0.7716\n",
      "mean column mean pruning test accuracy: 85.22%, sparsity: 0.7702\n",
      "mean row mean pruning test accuracy: 97.72%, sparsity: 0.7716\n",
      "mean block mean pruning test accuracy: 97.68%, sparsity: 0.7716\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 95.15%, sparsity: 0.7211\n",
      "per row magnitude pruning test accuracy: 97.95%, sparsity: 0.7024\n",
      "per block magnitude pruning test accuracy: 98.17%, sparsity: 0.7211\n",
      "mean column mean pruning test accuracy: 95.78%, sparsity: 0.7191\n",
      "mean row mean pruning test accuracy: 97.98%, sparsity: 0.7211\n",
      "mean block mean pruning test accuracy: 98.04%, sparsity: 0.7211\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.86%, sparsity: 0.6594\n",
      "per row magnitude pruning test accuracy: 98.16%, sparsity: 0.6362\n",
      "per block magnitude pruning test accuracy: 98.30%, sparsity: 0.6593\n",
      "mean column mean pruning test accuracy: 97.48%, sparsity: 0.6567\n",
      "mean row mean pruning test accuracy: 98.13%, sparsity: 0.6593\n",
      "mean block mean pruning test accuracy: 98.20%, sparsity: 0.6593\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 98.19%, sparsity: 0.5840\n",
      "per row magnitude pruning test accuracy: 98.34%, sparsity: 0.5553\n",
      "per block magnitude pruning test accuracy: 98.45%, sparsity: 0.5839\n",
      "mean column mean pruning test accuracy: 97.95%, sparsity: 0.5805\n",
      "mean row mean pruning test accuracy: 98.28%, sparsity: 0.5839\n",
      "mean block mean pruning test accuracy: 98.43%, sparsity: 0.5839\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.40%, sparsity: 0.4919\n",
      "per row magnitude pruning test accuracy: 98.41%, sparsity: 0.4565\n",
      "per block magnitude pruning test accuracy: 98.44%, sparsity: 0.4918\n",
      "mean column mean pruning test accuracy: 98.07%, sparsity: 0.4876\n",
      "mean row mean pruning test accuracy: 98.32%, sparsity: 0.4918\n",
      "mean block mean pruning test accuracy: 98.40%, sparsity: 0.4918\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.45%, sparsity: 0.3794\n",
      "per row magnitude pruning test accuracy: 98.44%, sparsity: 0.3359\n",
      "per block magnitude pruning test accuracy: 98.45%, sparsity: 0.3793\n",
      "mean column mean pruning test accuracy: 98.27%, sparsity: 0.3739\n",
      "mean row mean pruning test accuracy: 98.36%, sparsity: 0.3793\n",
      "mean block mean pruning test accuracy: 98.45%, sparsity: 0.3793\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.44%, sparsity: 0.2420\n",
      "per row magnitude pruning test accuracy: 98.44%, sparsity: 0.1887\n",
      "per block magnitude pruning test accuracy: 98.47%, sparsity: 0.2418\n",
      "mean column mean pruning test accuracy: 98.39%, sparsity: 0.2351\n",
      "mean row mean pruning test accuracy: 98.41%, sparsity: 0.2418\n",
      "mean block mean pruning test accuracy: 98.44%, sparsity: 0.2418\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.43%, sparsity: 0.0745\n",
      "per row magnitude pruning test accuracy: 98.46%, sparsity: 0.0333\n",
      "per block magnitude pruning test accuracy: 98.45%, sparsity: 0.0743\n",
      "mean column mean pruning test accuracy: 98.43%, sparsity: 0.0668\n",
      "mean row mean pruning test accuracy: 98.43%, sparsity: 0.0743\n",
      "mean block mean pruning test accuracy: 98.45%, sparsity: 0.0743\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.46%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.44%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.47%, sparsity: 0.0032\n",
      "mean row mean pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.46%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.44%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.47%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.44%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.46%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.44%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.47%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.44%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 1,494,154\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 94.75%, sparsity: 0.7715\n",
      "per row magnitude pruning test accuracy: 97.60%, sparsity: 0.7580\n",
      "per block magnitude pruning test accuracy: 97.76%, sparsity: 0.7713\n",
      "mean column mean pruning test accuracy: 93.99%, sparsity: 0.7700\n",
      "mean row mean pruning test accuracy: 97.80%, sparsity: 0.7713\n",
      "mean block mean pruning test accuracy: 97.60%, sparsity: 0.7713\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 96.60%, sparsity: 0.7209\n",
      "per row magnitude pruning test accuracy: 97.93%, sparsity: 0.7043\n",
      "per block magnitude pruning test accuracy: 97.85%, sparsity: 0.7207\n",
      "mean column mean pruning test accuracy: 96.71%, sparsity: 0.7189\n",
      "mean row mean pruning test accuracy: 98.18%, sparsity: 0.7207\n",
      "mean block mean pruning test accuracy: 97.92%, sparsity: 0.7207\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.44%, sparsity: 0.6591\n",
      "per row magnitude pruning test accuracy: 98.12%, sparsity: 0.6386\n",
      "per block magnitude pruning test accuracy: 98.02%, sparsity: 0.6589\n",
      "mean column mean pruning test accuracy: 97.21%, sparsity: 0.6566\n",
      "mean row mean pruning test accuracy: 98.25%, sparsity: 0.6589\n",
      "mean block mean pruning test accuracy: 98.28%, sparsity: 0.6589\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.84%, sparsity: 0.5836\n",
      "per row magnitude pruning test accuracy: 98.19%, sparsity: 0.5584\n",
      "per block magnitude pruning test accuracy: 98.25%, sparsity: 0.5833\n",
      "mean column mean pruning test accuracy: 97.49%, sparsity: 0.5804\n",
      "mean row mean pruning test accuracy: 98.34%, sparsity: 0.5833\n",
      "mean block mean pruning test accuracy: 98.26%, sparsity: 0.5833\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.23%, sparsity: 0.4914\n",
      "per row magnitude pruning test accuracy: 98.26%, sparsity: 0.4604\n",
      "per block magnitude pruning test accuracy: 98.29%, sparsity: 0.4911\n",
      "mean column mean pruning test accuracy: 97.89%, sparsity: 0.4874\n",
      "mean row mean pruning test accuracy: 98.28%, sparsity: 0.4911\n",
      "mean block mean pruning test accuracy: 98.34%, sparsity: 0.4911\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.25%, sparsity: 0.3788\n",
      "per row magnitude pruning test accuracy: 98.31%, sparsity: 0.3408\n",
      "per block magnitude pruning test accuracy: 98.30%, sparsity: 0.3784\n",
      "mean column mean pruning test accuracy: 98.04%, sparsity: 0.3737\n",
      "mean row mean pruning test accuracy: 98.21%, sparsity: 0.3784\n",
      "mean block mean pruning test accuracy: 98.34%, sparsity: 0.3784\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.30%, sparsity: 0.2413\n",
      "per row magnitude pruning test accuracy: 98.29%, sparsity: 0.1946\n",
      "per block magnitude pruning test accuracy: 98.27%, sparsity: 0.2408\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.2349\n",
      "mean row mean pruning test accuracy: 98.36%, sparsity: 0.2408\n",
      "mean block mean pruning test accuracy: 98.30%, sparsity: 0.2408\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.29%, sparsity: 0.0734\n",
      "per row magnitude pruning test accuracy: 98.28%, sparsity: 0.0337\n",
      "per block magnitude pruning test accuracy: 98.30%, sparsity: 0.0728\n",
      "mean column mean pruning test accuracy: 98.29%, sparsity: 0.0670\n",
      "mean row mean pruning test accuracy: 98.31%, sparsity: 0.0728\n",
      "mean block mean pruning test accuracy: 98.31%, sparsity: 0.0728\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.30%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.31%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.33%, sparsity: 0.0023\n",
      "mean row mean pruning test accuracy: 98.30%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.30%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.31%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.33%, sparsity: 0.0013\n",
      "mean row mean pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.31%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.30%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.31%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.33%, sparsity: 0.0013\n",
      "mean row mean pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.31%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 4,235,786\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 96.46%, sparsity: 0.8136\n",
      "per row magnitude pruning test accuracy: 96.76%, sparsity: 0.7853\n",
      "per block magnitude pruning test accuracy: 97.21%, sparsity: 0.8190\n",
      "mean column mean pruning test accuracy: 92.55%, sparsity: 0.7989\n",
      "mean row mean pruning test accuracy: 97.27%, sparsity: 0.8190\n",
      "mean block mean pruning test accuracy: 97.00%, sparsity: 0.8190\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 97.38%, sparsity: 0.7724\n",
      "per row magnitude pruning test accuracy: 90.36%, sparsity: 0.7376\n",
      "per block magnitude pruning test accuracy: 97.04%, sparsity: 0.7790\n",
      "mean column mean pruning test accuracy: 94.40%, sparsity: 0.7543\n",
      "mean row mean pruning test accuracy: 97.20%, sparsity: 0.7790\n",
      "mean block mean pruning test accuracy: 97.50%, sparsity: 0.7790\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.69%, sparsity: 0.7220\n",
      "per row magnitude pruning test accuracy: 96.55%, sparsity: 0.6794\n",
      "per block magnitude pruning test accuracy: 96.22%, sparsity: 0.7300\n",
      "mean column mean pruning test accuracy: 94.87%, sparsity: 0.6998\n",
      "mean row mean pruning test accuracy: 97.42%, sparsity: 0.7300\n",
      "mean block mean pruning test accuracy: 97.61%, sparsity: 0.7300\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.61%, sparsity: 0.6604\n",
      "per row magnitude pruning test accuracy: 97.12%, sparsity: 0.6083\n",
      "per block magnitude pruning test accuracy: 97.33%, sparsity: 0.6702\n",
      "mean column mean pruning test accuracy: 95.91%, sparsity: 0.6332\n",
      "mean row mean pruning test accuracy: 97.36%, sparsity: 0.6702\n",
      "mean block mean pruning test accuracy: 97.76%, sparsity: 0.6702\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 97.91%, sparsity: 0.5853\n",
      "per row magnitude pruning test accuracy: 97.72%, sparsity: 0.5215\n",
      "per block magnitude pruning test accuracy: 97.99%, sparsity: 0.5972\n",
      "mean column mean pruning test accuracy: 96.80%, sparsity: 0.5519\n",
      "mean row mean pruning test accuracy: 97.47%, sparsity: 0.5972\n",
      "mean block mean pruning test accuracy: 97.87%, sparsity: 0.5972\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.10%, sparsity: 0.4934\n",
      "per row magnitude pruning test accuracy: 98.02%, sparsity: 0.4155\n",
      "per block magnitude pruning test accuracy: 98.25%, sparsity: 0.5081\n",
      "mean column mean pruning test accuracy: 97.41%, sparsity: 0.4526\n",
      "mean row mean pruning test accuracy: 97.79%, sparsity: 0.5081\n",
      "mean block mean pruning test accuracy: 97.99%, sparsity: 0.5081\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.21%, sparsity: 0.3813\n",
      "per row magnitude pruning test accuracy: 98.17%, sparsity: 0.2859\n",
      "per block magnitude pruning test accuracy: 98.29%, sparsity: 0.3991\n",
      "mean column mean pruning test accuracy: 97.80%, sparsity: 0.3313\n",
      "mean row mean pruning test accuracy: 98.10%, sparsity: 0.3991\n",
      "mean block mean pruning test accuracy: 98.12%, sparsity: 0.3991\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.23%, sparsity: 0.2443\n",
      "per row magnitude pruning test accuracy: 98.22%, sparsity: 0.1292\n",
      "per block magnitude pruning test accuracy: 98.27%, sparsity: 0.2661\n",
      "mean column mean pruning test accuracy: 98.17%, sparsity: 0.1849\n",
      "mean row mean pruning test accuracy: 98.21%, sparsity: 0.2661\n",
      "mean block mean pruning test accuracy: 98.24%, sparsity: 0.2661\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.22%, sparsity: 0.0789\n",
      "per row magnitude pruning test accuracy: 98.28%, sparsity: 0.0137\n",
      "per block magnitude pruning test accuracy: 98.25%, sparsity: 0.1037\n",
      "mean column mean pruning test accuracy: 98.20%, sparsity: 0.0502\n",
      "mean row mean pruning test accuracy: 98.26%, sparsity: 0.1037\n",
      "mean block mean pruning test accuracy: 98.25%, sparsity: 0.1037\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.26%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 98.26%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.0025\n",
      "mean row mean pruning test accuracy: 98.25%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.26%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.26%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.26%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 98.26%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.0009\n",
      "mean row mean pruning test accuracy: 98.26%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.26%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 13,714,442\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 89.87%, sparsity: 0.8500\n",
      "per row magnitude pruning test accuracy: 95.33%, sparsity: 0.7983\n",
      "per block magnitude pruning test accuracy: 96.21%, sparsity: 0.8534\n",
      "mean column mean pruning test accuracy: 93.70%, sparsity: 0.8135\n",
      "mean row mean pruning test accuracy: 96.40%, sparsity: 0.8534\n",
      "mean block mean pruning test accuracy: 96.12%, sparsity: 0.8534\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 93.69%, sparsity: 0.8167\n",
      "per row magnitude pruning test accuracy: 95.77%, sparsity: 0.7535\n",
      "per block magnitude pruning test accuracy: 96.90%, sparsity: 0.8209\n",
      "mean column mean pruning test accuracy: 94.49%, sparsity: 0.7722\n",
      "mean row mean pruning test accuracy: 96.64%, sparsity: 0.8209\n",
      "mean block mean pruning test accuracy: 97.04%, sparsity: 0.8209\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 96.54%, sparsity: 0.7762\n",
      "per row magnitude pruning test accuracy: 95.74%, sparsity: 0.6989\n",
      "per block magnitude pruning test accuracy: 97.26%, sparsity: 0.7813\n",
      "mean column mean pruning test accuracy: 94.64%, sparsity: 0.7217\n",
      "mean row mean pruning test accuracy: 96.47%, sparsity: 0.7813\n",
      "mean block mean pruning test accuracy: 97.41%, sparsity: 0.7813\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.71%, sparsity: 0.7266\n",
      "per row magnitude pruning test accuracy: 96.49%, sparsity: 0.6322\n",
      "per block magnitude pruning test accuracy: 97.75%, sparsity: 0.7328\n",
      "mean column mean pruning test accuracy: 95.83%, sparsity: 0.6600\n",
      "mean row mean pruning test accuracy: 96.63%, sparsity: 0.7328\n",
      "mean block mean pruning test accuracy: 97.67%, sparsity: 0.7328\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.07%, sparsity: 0.6661\n",
      "per row magnitude pruning test accuracy: 97.13%, sparsity: 0.5507\n",
      "per block magnitude pruning test accuracy: 98.07%, sparsity: 0.6737\n",
      "mean column mean pruning test accuracy: 96.45%, sparsity: 0.5846\n",
      "mean row mean pruning test accuracy: 97.04%, sparsity: 0.6737\n",
      "mean block mean pruning test accuracy: 98.01%, sparsity: 0.6737\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.25%, sparsity: 0.5921\n",
      "per row magnitude pruning test accuracy: 97.89%, sparsity: 0.4511\n",
      "per block magnitude pruning test accuracy: 98.19%, sparsity: 0.6015\n",
      "mean column mean pruning test accuracy: 97.12%, sparsity: 0.4926\n",
      "mean row mean pruning test accuracy: 97.49%, sparsity: 0.6015\n",
      "mean block mean pruning test accuracy: 98.10%, sparsity: 0.6015\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.33%, sparsity: 0.5018\n",
      "per row magnitude pruning test accuracy: 98.23%, sparsity: 0.3295\n",
      "per block magnitude pruning test accuracy: 98.36%, sparsity: 0.5132\n",
      "mean column mean pruning test accuracy: 97.86%, sparsity: 0.3802\n",
      "mean row mean pruning test accuracy: 98.00%, sparsity: 0.5132\n",
      "mean block mean pruning test accuracy: 98.12%, sparsity: 0.5132\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.38%, sparsity: 0.3915\n",
      "per row magnitude pruning test accuracy: 98.32%, sparsity: 0.1838\n",
      "per block magnitude pruning test accuracy: 98.36%, sparsity: 0.4054\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.2476\n",
      "mean row mean pruning test accuracy: 98.29%, sparsity: 0.4054\n",
      "mean block mean pruning test accuracy: 98.22%, sparsity: 0.4054\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.39%, sparsity: 0.2568\n",
      "per row magnitude pruning test accuracy: 98.41%, sparsity: 0.0499\n",
      "per block magnitude pruning test accuracy: 98.35%, sparsity: 0.2738\n",
      "mean column mean pruning test accuracy: 98.35%, sparsity: 0.1282\n",
      "mean row mean pruning test accuracy: 98.39%, sparsity: 0.2738\n",
      "mean block mean pruning test accuracy: 98.35%, sparsity: 0.2738\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.39%, sparsity: 0.1145\n",
      "per row magnitude pruning test accuracy: 98.37%, sparsity: 0.0028\n",
      "per block magnitude pruning test accuracy: 98.37%, sparsity: 0.1165\n",
      "mean column mean pruning test accuracy: 98.37%, sparsity: 0.0362\n",
      "mean row mean pruning test accuracy: 98.37%, sparsity: 0.1165\n",
      "mean block mean pruning test accuracy: 98.36%, sparsity: 0.1165\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.37%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.39%, sparsity: 0.0043\n",
      "per row magnitude pruning test accuracy: 98.37%, sparsity: 0.0006\n",
      "per block magnitude pruning test accuracy: 98.37%, sparsity: 0.0058\n",
      "mean column mean pruning test accuracy: 98.36%, sparsity: 0.0007\n",
      "mean row mean pruning test accuracy: 98.37%, sparsity: 0.0058\n",
      "mean block mean pruning test accuracy: 98.37%, sparsity: 0.0058\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_mnist/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beae1264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 52,650\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 36.08%, sparsity: 0.7866\n",
      "per row magnitude pruning test accuracy: 48.63%, sparsity: 0.7711\n",
      "per block magnitude pruning test accuracy: 42.92%, sparsity: 0.7874\n",
      "mean column mean pruning test accuracy: 30.98%, sparsity: 0.7837\n",
      "mean row mean pruning test accuracy: 49.05%, sparsity: 0.7874\n",
      "mean block mean pruning test accuracy: 47.50%, sparsity: 0.7874\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 52.58%, sparsity: 0.7393\n",
      "per row magnitude pruning test accuracy: 44.75%, sparsity: 0.7191\n",
      "per block magnitude pruning test accuracy: 50.79%, sparsity: 0.7403\n",
      "mean column mean pruning test accuracy: 56.39%, sparsity: 0.7357\n",
      "mean row mean pruning test accuracy: 58.18%, sparsity: 0.7403\n",
      "mean block mean pruning test accuracy: 55.75%, sparsity: 0.7403\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 71.53%, sparsity: 0.6816\n",
      "per row magnitude pruning test accuracy: 64.82%, sparsity: 0.6549\n",
      "per block magnitude pruning test accuracy: 71.56%, sparsity: 0.6828\n",
      "mean column mean pruning test accuracy: 73.26%, sparsity: 0.6770\n",
      "mean row mean pruning test accuracy: 61.96%, sparsity: 0.6828\n",
      "mean block mean pruning test accuracy: 75.28%, sparsity: 0.6828\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 82.98%, sparsity: 0.6111\n",
      "per row magnitude pruning test accuracy: 82.07%, sparsity: 0.5764\n",
      "per block magnitude pruning test accuracy: 89.82%, sparsity: 0.6126\n",
      "mean column mean pruning test accuracy: 83.50%, sparsity: 0.6052\n",
      "mean row mean pruning test accuracy: 78.46%, sparsity: 0.6126\n",
      "mean block mean pruning test accuracy: 90.13%, sparsity: 0.6126\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 93.87%, sparsity: 0.5250\n",
      "per row magnitude pruning test accuracy: 93.01%, sparsity: 0.4809\n",
      "per block magnitude pruning test accuracy: 93.83%, sparsity: 0.5268\n",
      "mean column mean pruning test accuracy: 91.32%, sparsity: 0.5176\n",
      "mean row mean pruning test accuracy: 89.81%, sparsity: 0.5268\n",
      "mean block mean pruning test accuracy: 92.95%, sparsity: 0.5268\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 96.24%, sparsity: 0.4198\n",
      "per row magnitude pruning test accuracy: 95.84%, sparsity: 0.3643\n",
      "per block magnitude pruning test accuracy: 96.34%, sparsity: 0.4220\n",
      "mean column mean pruning test accuracy: 95.19%, sparsity: 0.4106\n",
      "mean row mean pruning test accuracy: 95.23%, sparsity: 0.4220\n",
      "mean block mean pruning test accuracy: 96.37%, sparsity: 0.4220\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 96.75%, sparsity: 0.2913\n",
      "per row magnitude pruning test accuracy: 96.66%, sparsity: 0.2217\n",
      "per block magnitude pruning test accuracy: 96.92%, sparsity: 0.2940\n",
      "mean column mean pruning test accuracy: 96.85%, sparsity: 0.2799\n",
      "mean row mean pruning test accuracy: 96.83%, sparsity: 0.2940\n",
      "mean block mean pruning test accuracy: 96.90%, sparsity: 0.2940\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 96.88%, sparsity: 0.1350\n",
      "per row magnitude pruning test accuracy: 97.05%, sparsity: 0.0671\n",
      "per block magnitude pruning test accuracy: 96.90%, sparsity: 0.1383\n",
      "mean column mean pruning test accuracy: 96.95%, sparsity: 0.1221\n",
      "mean row mean pruning test accuracy: 97.08%, sparsity: 0.1383\n",
      "mean block mean pruning test accuracy: 96.97%, sparsity: 0.1383\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 96.84%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 96.97%, sparsity: 0.0191\n",
      "per block magnitude pruning test accuracy: 96.96%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 96.89%, sparsity: 0.0158\n",
      "mean row mean pruning test accuracy: 96.97%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 96.97%, sparsity: 0.0001\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 96.84%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 97.02%, sparsity: 0.0168\n",
      "per block magnitude pruning test accuracy: 96.96%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 96.90%, sparsity: 0.0023\n",
      "mean row mean pruning test accuracy: 96.99%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 96.97%, sparsity: 0.0001\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 96.96%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 96.84%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 97.02%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 96.96%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 96.90%, sparsity: 0.0020\n",
      "mean row mean pruning test accuracy: 96.99%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 96.97%, sparsity: 0.0001\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 400,906\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 86.10%, sparsity: 0.7660\n",
      "per row magnitude pruning test accuracy: 94.62%, sparsity: 0.7560\n",
      "per block magnitude pruning test accuracy: 95.07%, sparsity: 0.7656\n",
      "mean column mean pruning test accuracy: 92.95%, sparsity: 0.7651\n",
      "mean row mean pruning test accuracy: 95.24%, sparsity: 0.7656\n",
      "mean block mean pruning test accuracy: 96.82%, sparsity: 0.7656\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 91.36%, sparsity: 0.7142\n",
      "per row magnitude pruning test accuracy: 96.86%, sparsity: 0.7015\n",
      "per block magnitude pruning test accuracy: 96.82%, sparsity: 0.7137\n",
      "mean column mean pruning test accuracy: 95.32%, sparsity: 0.7128\n",
      "mean row mean pruning test accuracy: 96.00%, sparsity: 0.7137\n",
      "mean block mean pruning test accuracy: 97.32%, sparsity: 0.7137\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 96.37%, sparsity: 0.6509\n",
      "per row magnitude pruning test accuracy: 97.43%, sparsity: 0.6349\n",
      "per block magnitude pruning test accuracy: 97.51%, sparsity: 0.6503\n",
      "mean column mean pruning test accuracy: 96.95%, sparsity: 0.6489\n",
      "mean row mean pruning test accuracy: 96.47%, sparsity: 0.6503\n",
      "mean block mean pruning test accuracy: 97.65%, sparsity: 0.6503\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.16%, sparsity: 0.5736\n",
      "per row magnitude pruning test accuracy: 97.92%, sparsity: 0.5536\n",
      "per block magnitude pruning test accuracy: 97.78%, sparsity: 0.5729\n",
      "mean column mean pruning test accuracy: 97.58%, sparsity: 0.5709\n",
      "mean row mean pruning test accuracy: 96.83%, sparsity: 0.5729\n",
      "mean block mean pruning test accuracy: 97.96%, sparsity: 0.5729\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 97.87%, sparsity: 0.4792\n",
      "per row magnitude pruning test accuracy: 98.07%, sparsity: 0.4542\n",
      "per block magnitude pruning test accuracy: 98.09%, sparsity: 0.4783\n",
      "mean column mean pruning test accuracy: 97.88%, sparsity: 0.4756\n",
      "mean row mean pruning test accuracy: 97.39%, sparsity: 0.4783\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.4783\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.10%, sparsity: 0.3639\n",
      "per row magnitude pruning test accuracy: 98.11%, sparsity: 0.3329\n",
      "per block magnitude pruning test accuracy: 98.14%, sparsity: 0.3628\n",
      "mean column mean pruning test accuracy: 98.13%, sparsity: 0.3592\n",
      "mean row mean pruning test accuracy: 97.79%, sparsity: 0.3628\n",
      "mean block mean pruning test accuracy: 98.12%, sparsity: 0.3628\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.17%, sparsity: 0.2230\n",
      "per row magnitude pruning test accuracy: 98.15%, sparsity: 0.1849\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.2217\n",
      "mean column mean pruning test accuracy: 98.19%, sparsity: 0.2170\n",
      "mean row mean pruning test accuracy: 98.04%, sparsity: 0.2217\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.2217\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.21%, sparsity: 0.0521\n",
      "per row magnitude pruning test accuracy: 98.16%, sparsity: 0.0282\n",
      "per block magnitude pruning test accuracy: 98.19%, sparsity: 0.0506\n",
      "mean column mean pruning test accuracy: 98.26%, sparsity: 0.0504\n",
      "mean row mean pruning test accuracy: 98.16%, sparsity: 0.0506\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0506\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.21%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.19%, sparsity: 0.0047\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.22%, sparsity: 0.0041\n",
      "mean row mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.18%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.19%, sparsity: 0.0026\n",
      "mean row mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 98.18%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.18%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.19%, sparsity: 0.0026\n",
      "mean row mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 567,434\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 91.84%, sparsity: 0.7722\n",
      "per row magnitude pruning test accuracy: 96.93%, sparsity: 0.7568\n",
      "per block magnitude pruning test accuracy: 96.77%, sparsity: 0.7722\n",
      "mean column mean pruning test accuracy: 90.13%, sparsity: 0.7704\n",
      "mean row mean pruning test accuracy: 97.03%, sparsity: 0.7722\n",
      "mean block mean pruning test accuracy: 97.35%, sparsity: 0.7722\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 96.06%, sparsity: 0.7218\n",
      "per row magnitude pruning test accuracy: 97.81%, sparsity: 0.7026\n",
      "per block magnitude pruning test accuracy: 97.81%, sparsity: 0.7217\n",
      "mean column mean pruning test accuracy: 95.31%, sparsity: 0.7195\n",
      "mean row mean pruning test accuracy: 97.39%, sparsity: 0.7217\n",
      "mean block mean pruning test accuracy: 97.47%, sparsity: 0.7217\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.26%, sparsity: 0.6602\n",
      "per row magnitude pruning test accuracy: 98.13%, sparsity: 0.6365\n",
      "per block magnitude pruning test accuracy: 97.92%, sparsity: 0.6601\n",
      "mean column mean pruning test accuracy: 96.98%, sparsity: 0.6572\n",
      "mean row mean pruning test accuracy: 97.43%, sparsity: 0.6601\n",
      "mean block mean pruning test accuracy: 97.90%, sparsity: 0.6601\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.80%, sparsity: 0.5850\n",
      "per row magnitude pruning test accuracy: 98.12%, sparsity: 0.5557\n",
      "per block magnitude pruning test accuracy: 97.94%, sparsity: 0.5848\n",
      "mean column mean pruning test accuracy: 97.58%, sparsity: 0.5811\n",
      "mean row mean pruning test accuracy: 97.74%, sparsity: 0.5848\n",
      "mean block mean pruning test accuracy: 98.09%, sparsity: 0.5848\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.21%, sparsity: 0.4931\n",
      "per row magnitude pruning test accuracy: 98.30%, sparsity: 0.4569\n",
      "per block magnitude pruning test accuracy: 98.19%, sparsity: 0.4929\n",
      "mean column mean pruning test accuracy: 97.91%, sparsity: 0.4882\n",
      "mean row mean pruning test accuracy: 98.14%, sparsity: 0.4929\n",
      "mean block mean pruning test accuracy: 98.27%, sparsity: 0.4929\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.36%, sparsity: 0.3808\n",
      "per row magnitude pruning test accuracy: 98.33%, sparsity: 0.3364\n",
      "per block magnitude pruning test accuracy: 98.36%, sparsity: 0.3807\n",
      "mean column mean pruning test accuracy: 98.16%, sparsity: 0.3747\n",
      "mean row mean pruning test accuracy: 98.21%, sparsity: 0.3807\n",
      "mean block mean pruning test accuracy: 98.38%, sparsity: 0.3807\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.45%, sparsity: 0.2438\n",
      "per row magnitude pruning test accuracy: 98.41%, sparsity: 0.1892\n",
      "per block magnitude pruning test accuracy: 98.38%, sparsity: 0.2435\n",
      "mean column mean pruning test accuracy: 98.31%, sparsity: 0.2361\n",
      "mean row mean pruning test accuracy: 98.35%, sparsity: 0.2435\n",
      "mean block mean pruning test accuracy: 98.39%, sparsity: 0.2435\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.38%, sparsity: 0.0767\n",
      "per row magnitude pruning test accuracy: 98.38%, sparsity: 0.0343\n",
      "per block magnitude pruning test accuracy: 98.37%, sparsity: 0.0764\n",
      "mean column mean pruning test accuracy: 98.40%, sparsity: 0.0681\n",
      "mean row mean pruning test accuracy: 98.41%, sparsity: 0.0764\n",
      "mean block mean pruning test accuracy: 98.37%, sparsity: 0.0764\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.40%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.37%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.37%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.38%, sparsity: 0.0035\n",
      "mean row mean pruning test accuracy: 98.37%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.38%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.40%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.37%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.37%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.38%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 98.37%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.38%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.37%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.40%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.37%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.37%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.38%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 98.37%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.38%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 1,494,154\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 91.83%, sparsity: 0.7717\n",
      "per row magnitude pruning test accuracy: 97.99%, sparsity: 0.7580\n",
      "per block magnitude pruning test accuracy: 97.50%, sparsity: 0.7711\n",
      "mean column mean pruning test accuracy: 95.33%, sparsity: 0.7700\n",
      "mean row mean pruning test accuracy: 97.11%, sparsity: 0.7711\n",
      "mean block mean pruning test accuracy: 97.73%, sparsity: 0.7711\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 96.94%, sparsity: 0.7212\n",
      "per row magnitude pruning test accuracy: 98.35%, sparsity: 0.7043\n",
      "per block magnitude pruning test accuracy: 98.21%, sparsity: 0.7205\n",
      "mean column mean pruning test accuracy: 97.06%, sparsity: 0.7190\n",
      "mean row mean pruning test accuracy: 97.87%, sparsity: 0.7205\n",
      "mean block mean pruning test accuracy: 98.28%, sparsity: 0.7205\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.93%, sparsity: 0.6594\n",
      "per row magnitude pruning test accuracy: 98.41%, sparsity: 0.6386\n",
      "per block magnitude pruning test accuracy: 98.42%, sparsity: 0.6586\n",
      "mean column mean pruning test accuracy: 97.79%, sparsity: 0.6566\n",
      "mean row mean pruning test accuracy: 97.98%, sparsity: 0.6586\n",
      "mean block mean pruning test accuracy: 98.33%, sparsity: 0.6586\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 98.38%, sparsity: 0.5840\n",
      "per row magnitude pruning test accuracy: 98.54%, sparsity: 0.5583\n",
      "per block magnitude pruning test accuracy: 98.38%, sparsity: 0.5830\n",
      "mean column mean pruning test accuracy: 97.87%, sparsity: 0.5805\n",
      "mean row mean pruning test accuracy: 97.98%, sparsity: 0.5830\n",
      "mean block mean pruning test accuracy: 98.38%, sparsity: 0.5830\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.47%, sparsity: 0.4919\n",
      "per row magnitude pruning test accuracy: 98.53%, sparsity: 0.4604\n",
      "per block magnitude pruning test accuracy: 98.52%, sparsity: 0.4906\n",
      "mean column mean pruning test accuracy: 97.98%, sparsity: 0.4874\n",
      "mean row mean pruning test accuracy: 98.25%, sparsity: 0.4906\n",
      "mean block mean pruning test accuracy: 98.52%, sparsity: 0.4906\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.58%, sparsity: 0.3794\n",
      "per row magnitude pruning test accuracy: 98.65%, sparsity: 0.3407\n",
      "per block magnitude pruning test accuracy: 98.65%, sparsity: 0.3779\n",
      "mean column mean pruning test accuracy: 98.35%, sparsity: 0.3738\n",
      "mean row mean pruning test accuracy: 98.42%, sparsity: 0.3779\n",
      "mean block mean pruning test accuracy: 98.61%, sparsity: 0.3779\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.67%, sparsity: 0.2420\n",
      "per row magnitude pruning test accuracy: 98.65%, sparsity: 0.1946\n",
      "per block magnitude pruning test accuracy: 98.69%, sparsity: 0.2401\n",
      "mean column mean pruning test accuracy: 98.52%, sparsity: 0.2350\n",
      "mean row mean pruning test accuracy: 98.58%, sparsity: 0.2401\n",
      "mean block mean pruning test accuracy: 98.66%, sparsity: 0.2401\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.67%, sparsity: 0.0744\n",
      "per row magnitude pruning test accuracy: 98.68%, sparsity: 0.0339\n",
      "per block magnitude pruning test accuracy: 98.67%, sparsity: 0.0721\n",
      "mean column mean pruning test accuracy: 98.67%, sparsity: 0.0664\n",
      "mean row mean pruning test accuracy: 98.70%, sparsity: 0.0721\n",
      "mean block mean pruning test accuracy: 98.67%, sparsity: 0.0721\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.65%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.68%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.66%, sparsity: 0.0023\n",
      "mean row mean pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.65%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.68%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.66%, sparsity: 0.0013\n",
      "mean row mean pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.68%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.65%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.68%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.66%, sparsity: 0.0013\n",
      "mean row mean pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.68%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 4,235,786\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 86.87%, sparsity: 0.8039\n",
      "per row magnitude pruning test accuracy: 97.03%, sparsity: 0.7792\n",
      "per block magnitude pruning test accuracy: 97.54%, sparsity: 0.8090\n",
      "mean column mean pruning test accuracy: 87.49%, sparsity: 0.7972\n",
      "mean row mean pruning test accuracy: 96.94%, sparsity: 0.8090\n",
      "mean block mean pruning test accuracy: 96.95%, sparsity: 0.8090\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 94.67%, sparsity: 0.7605\n",
      "per row magnitude pruning test accuracy: 97.22%, sparsity: 0.7302\n",
      "per block magnitude pruning test accuracy: 97.64%, sparsity: 0.7667\n",
      "mean column mean pruning test accuracy: 94.07%, sparsity: 0.7522\n",
      "mean row mean pruning test accuracy: 97.60%, sparsity: 0.7667\n",
      "mean block mean pruning test accuracy: 97.47%, sparsity: 0.7667\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.31%, sparsity: 0.7075\n",
      "per row magnitude pruning test accuracy: 97.49%, sparsity: 0.6704\n",
      "per block magnitude pruning test accuracy: 97.74%, sparsity: 0.7150\n",
      "mean column mean pruning test accuracy: 95.34%, sparsity: 0.6972\n",
      "mean row mean pruning test accuracy: 97.74%, sparsity: 0.7150\n",
      "mean block mean pruning test accuracy: 97.67%, sparsity: 0.7150\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 98.04%, sparsity: 0.6427\n",
      "per row magnitude pruning test accuracy: 97.63%, sparsity: 0.5973\n",
      "per block magnitude pruning test accuracy: 97.92%, sparsity: 0.6519\n",
      "mean column mean pruning test accuracy: 96.46%, sparsity: 0.6301\n",
      "mean row mean pruning test accuracy: 97.92%, sparsity: 0.6519\n",
      "mean block mean pruning test accuracy: 97.78%, sparsity: 0.6519\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.03%, sparsity: 0.5636\n",
      "per row magnitude pruning test accuracy: 97.79%, sparsity: 0.5080\n",
      "per block magnitude pruning test accuracy: 97.98%, sparsity: 0.5748\n",
      "mean column mean pruning test accuracy: 97.08%, sparsity: 0.5481\n",
      "mean row mean pruning test accuracy: 97.93%, sparsity: 0.5748\n",
      "mean block mean pruning test accuracy: 97.86%, sparsity: 0.5748\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.06%, sparsity: 0.4670\n",
      "per row magnitude pruning test accuracy: 97.82%, sparsity: 0.3990\n",
      "per block magnitude pruning test accuracy: 98.00%, sparsity: 0.4807\n",
      "mean column mean pruning test accuracy: 97.53%, sparsity: 0.4480\n",
      "mean row mean pruning test accuracy: 98.04%, sparsity: 0.4807\n",
      "mean block mean pruning test accuracy: 97.89%, sparsity: 0.4807\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.04%, sparsity: 0.3490\n",
      "per row magnitude pruning test accuracy: 98.04%, sparsity: 0.2658\n",
      "per block magnitude pruning test accuracy: 98.06%, sparsity: 0.3657\n",
      "mean column mean pruning test accuracy: 97.82%, sparsity: 0.3257\n",
      "mean row mean pruning test accuracy: 98.08%, sparsity: 0.3657\n",
      "mean block mean pruning test accuracy: 97.98%, sparsity: 0.3657\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.05%, sparsity: 0.2049\n",
      "per row magnitude pruning test accuracy: 98.04%, sparsity: 0.1033\n",
      "per block magnitude pruning test accuracy: 98.02%, sparsity: 0.2253\n",
      "mean column mean pruning test accuracy: 98.05%, sparsity: 0.1765\n",
      "mean row mean pruning test accuracy: 98.07%, sparsity: 0.2253\n",
      "mean block mean pruning test accuracy: 98.02%, sparsity: 0.2253\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.01%, sparsity: 0.0425\n",
      "per row magnitude pruning test accuracy: 98.02%, sparsity: 0.0126\n",
      "per block magnitude pruning test accuracy: 98.02%, sparsity: 0.0660\n",
      "mean column mean pruning test accuracy: 98.09%, sparsity: 0.0391\n",
      "mean row mean pruning test accuracy: 98.01%, sparsity: 0.0660\n",
      "mean block mean pruning test accuracy: 98.02%, sparsity: 0.0660\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.02%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.01%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 98.02%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.04%, sparsity: 0.0025\n",
      "mean row mean pruning test accuracy: 98.01%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.02%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 98.02%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.02%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.02%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 98.02%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.04%, sparsity: 0.0009\n",
      "mean row mean pruning test accuracy: 98.02%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.02%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 13,714,442\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 88.38%, sparsity: 0.8139\n",
      "per row magnitude pruning test accuracy: 94.59%, sparsity: 0.7869\n",
      "per block magnitude pruning test accuracy: 94.56%, sparsity: 0.8191\n",
      "mean column mean pruning test accuracy: 78.35%, sparsity: 0.8040\n",
      "mean row mean pruning test accuracy: 96.90%, sparsity: 0.8191\n",
      "mean block mean pruning test accuracy: 96.35%, sparsity: 0.8191\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 92.59%, sparsity: 0.7727\n",
      "per row magnitude pruning test accuracy: 95.53%, sparsity: 0.7397\n",
      "per block magnitude pruning test accuracy: 96.48%, sparsity: 0.7790\n",
      "mean column mean pruning test accuracy: 81.40%, sparsity: 0.7606\n",
      "mean row mean pruning test accuracy: 96.70%, sparsity: 0.7790\n",
      "mean block mean pruning test accuracy: 96.60%, sparsity: 0.7790\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 92.73%, sparsity: 0.7224\n",
      "per row magnitude pruning test accuracy: 96.42%, sparsity: 0.6820\n",
      "per block magnitude pruning test accuracy: 97.26%, sparsity: 0.7301\n",
      "mean column mean pruning test accuracy: 87.45%, sparsity: 0.7075\n",
      "mean row mean pruning test accuracy: 96.74%, sparsity: 0.7301\n",
      "mean block mean pruning test accuracy: 96.97%, sparsity: 0.7301\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 96.77%, sparsity: 0.6609\n",
      "per row magnitude pruning test accuracy: 96.81%, sparsity: 0.6115\n",
      "per block magnitude pruning test accuracy: 97.51%, sparsity: 0.6704\n",
      "mean column mean pruning test accuracy: 91.99%, sparsity: 0.6427\n",
      "mean row mean pruning test accuracy: 96.97%, sparsity: 0.6704\n",
      "mean block mean pruning test accuracy: 97.48%, sparsity: 0.6704\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 93.07%, sparsity: 0.5859\n",
      "per row magnitude pruning test accuracy: 97.33%, sparsity: 0.5254\n",
      "per block magnitude pruning test accuracy: 97.70%, sparsity: 0.5974\n",
      "mean column mean pruning test accuracy: 94.86%, sparsity: 0.5635\n",
      "mean row mean pruning test accuracy: 97.36%, sparsity: 0.5974\n",
      "mean block mean pruning test accuracy: 97.60%, sparsity: 0.5974\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 93.71%, sparsity: 0.4942\n",
      "per row magnitude pruning test accuracy: 97.76%, sparsity: 0.4203\n",
      "per block magnitude pruning test accuracy: 98.00%, sparsity: 0.5083\n",
      "mean column mean pruning test accuracy: 96.70%, sparsity: 0.4668\n",
      "mean row mean pruning test accuracy: 97.70%, sparsity: 0.5083\n",
      "mean block mean pruning test accuracy: 97.80%, sparsity: 0.5083\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 97.89%, sparsity: 0.3822\n",
      "per row magnitude pruning test accuracy: 98.03%, sparsity: 0.2919\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.3994\n",
      "mean column mean pruning test accuracy: 97.77%, sparsity: 0.3487\n",
      "mean row mean pruning test accuracy: 97.92%, sparsity: 0.3994\n",
      "mean block mean pruning test accuracy: 97.94%, sparsity: 0.3994\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.17%, sparsity: 0.2454\n",
      "per row magnitude pruning test accuracy: 98.22%, sparsity: 0.1351\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.2664\n",
      "mean column mean pruning test accuracy: 98.16%, sparsity: 0.2055\n",
      "mean row mean pruning test accuracy: 98.12%, sparsity: 0.2664\n",
      "mean block mean pruning test accuracy: 98.16%, sparsity: 0.2664\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.18%, sparsity: 0.0941\n",
      "per row magnitude pruning test accuracy: 98.22%, sparsity: 0.0160\n",
      "per block magnitude pruning test accuracy: 98.21%, sparsity: 0.1085\n",
      "mean column mean pruning test accuracy: 98.23%, sparsity: 0.0685\n",
      "mean row mean pruning test accuracy: 98.30%, sparsity: 0.1085\n",
      "mean block mean pruning test accuracy: 98.20%, sparsity: 0.1085\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.19%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.18%, sparsity: 0.0006\n",
      "per block magnitude pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.25%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 98.22%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.20%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.19%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.20%, sparsity: 0.0006\n",
      "per block magnitude pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.25%, sparsity: 0.0006\n",
      "mean row mean pruning test accuracy: 98.20%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.20%, sparsity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = geluLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_mnist/gelu_{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "324894dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 52,650\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 52.43%, sparsity: 0.7980\n",
      "per row magnitude pruning test accuracy: 41.83%, sparsity: 0.7677\n",
      "per block magnitude pruning test accuracy: 43.28%, sparsity: 0.7979\n",
      "mean column mean pruning test accuracy: 69.56%, sparsity: 0.7997\n",
      "mean row mean pruning test accuracy: 40.14%, sparsity: 0.7979\n",
      "mean block mean pruning test accuracy: 57.68%, sparsity: 0.7979\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 73.93%, sparsity: 0.7533\n",
      "per row magnitude pruning test accuracy: 59.08%, sparsity: 0.7144\n",
      "per block magnitude pruning test accuracy: 74.90%, sparsity: 0.7531\n",
      "mean column mean pruning test accuracy: 76.84%, sparsity: 0.7552\n",
      "mean row mean pruning test accuracy: 56.00%, sparsity: 0.7531\n",
      "mean block mean pruning test accuracy: 71.77%, sparsity: 0.7531\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 82.32%, sparsity: 0.6987\n",
      "per row magnitude pruning test accuracy: 76.89%, sparsity: 0.6494\n",
      "per block magnitude pruning test accuracy: 81.90%, sparsity: 0.6985\n",
      "mean column mean pruning test accuracy: 87.96%, sparsity: 0.7007\n",
      "mean row mean pruning test accuracy: 72.62%, sparsity: 0.6985\n",
      "mean block mean pruning test accuracy: 77.76%, sparsity: 0.6985\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 92.86%, sparsity: 0.6319\n",
      "per row magnitude pruning test accuracy: 84.42%, sparsity: 0.5697\n",
      "per block magnitude pruning test accuracy: 84.68%, sparsity: 0.6317\n",
      "mean column mean pruning test accuracy: 89.69%, sparsity: 0.6342\n",
      "mean row mean pruning test accuracy: 85.87%, sparsity: 0.6317\n",
      "mean block mean pruning test accuracy: 87.02%, sparsity: 0.6317\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 93.38%, sparsity: 0.5505\n",
      "per row magnitude pruning test accuracy: 91.74%, sparsity: 0.4728\n",
      "per block magnitude pruning test accuracy: 91.59%, sparsity: 0.5502\n",
      "mean column mean pruning test accuracy: 92.84%, sparsity: 0.5529\n",
      "mean row mean pruning test accuracy: 91.75%, sparsity: 0.5502\n",
      "mean block mean pruning test accuracy: 91.96%, sparsity: 0.5502\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 94.14%, sparsity: 0.4509\n",
      "per row magnitude pruning test accuracy: 93.61%, sparsity: 0.3539\n",
      "per block magnitude pruning test accuracy: 93.67%, sparsity: 0.4506\n",
      "mean column mean pruning test accuracy: 94.23%, sparsity: 0.4538\n",
      "mean row mean pruning test accuracy: 93.87%, sparsity: 0.4506\n",
      "mean block mean pruning test accuracy: 93.54%, sparsity: 0.4506\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 94.18%, sparsity: 0.3296\n",
      "per row magnitude pruning test accuracy: 94.18%, sparsity: 0.2097\n",
      "per block magnitude pruning test accuracy: 94.30%, sparsity: 0.3293\n",
      "mean column mean pruning test accuracy: 94.29%, sparsity: 0.3331\n",
      "mean row mean pruning test accuracy: 94.20%, sparsity: 0.3293\n",
      "mean block mean pruning test accuracy: 94.30%, sparsity: 0.3293\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 94.24%, sparsity: 0.1850\n",
      "per row magnitude pruning test accuracy: 94.38%, sparsity: 0.0565\n",
      "per block magnitude pruning test accuracy: 94.35%, sparsity: 0.1847\n",
      "mean column mean pruning test accuracy: 94.37%, sparsity: 0.1904\n",
      "mean row mean pruning test accuracy: 94.37%, sparsity: 0.1847\n",
      "mean block mean pruning test accuracy: 94.36%, sparsity: 0.1847\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 94.27%, sparsity: 0.0146\n",
      "per row magnitude pruning test accuracy: 94.40%, sparsity: 0.0183\n",
      "per block magnitude pruning test accuracy: 94.39%, sparsity: 0.0141\n",
      "mean column mean pruning test accuracy: 94.38%, sparsity: 0.0476\n",
      "mean row mean pruning test accuracy: 94.38%, sparsity: 0.0141\n",
      "mean block mean pruning test accuracy: 94.43%, sparsity: 0.0141\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 94.27%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 94.40%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 94.39%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 94.38%, sparsity: 0.0043\n",
      "mean row mean pruning test accuracy: 94.38%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 94.43%, sparsity: 0.0001\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 94.39%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 94.27%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 94.40%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 94.39%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 94.38%, sparsity: 0.0020\n",
      "mean row mean pruning test accuracy: 94.37%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 94.43%, sparsity: 0.0001\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 400,906\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 81.94%, sparsity: 0.7871\n",
      "per row magnitude pruning test accuracy: 58.29%, sparsity: 0.7698\n",
      "per block magnitude pruning test accuracy: 60.49%, sparsity: 0.7886\n",
      "mean column mean pruning test accuracy: 83.81%, sparsity: 0.7880\n",
      "mean row mean pruning test accuracy: 74.74%, sparsity: 0.7886\n",
      "mean block mean pruning test accuracy: 60.15%, sparsity: 0.7886\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 89.91%, sparsity: 0.7400\n",
      "per row magnitude pruning test accuracy: 73.20%, sparsity: 0.7182\n",
      "per block magnitude pruning test accuracy: 77.17%, sparsity: 0.7418\n",
      "mean column mean pruning test accuracy: 90.11%, sparsity: 0.7408\n",
      "mean row mean pruning test accuracy: 84.95%, sparsity: 0.7418\n",
      "mean block mean pruning test accuracy: 74.79%, sparsity: 0.7418\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 93.86%, sparsity: 0.6825\n",
      "per row magnitude pruning test accuracy: 83.92%, sparsity: 0.6554\n",
      "per block magnitude pruning test accuracy: 87.53%, sparsity: 0.6846\n",
      "mean column mean pruning test accuracy: 94.08%, sparsity: 0.6831\n",
      "mean row mean pruning test accuracy: 91.95%, sparsity: 0.6846\n",
      "mean block mean pruning test accuracy: 85.12%, sparsity: 0.6846\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 95.56%, sparsity: 0.6121\n",
      "per row magnitude pruning test accuracy: 91.24%, sparsity: 0.5786\n",
      "per block magnitude pruning test accuracy: 93.10%, sparsity: 0.6148\n",
      "mean column mean pruning test accuracy: 95.35%, sparsity: 0.6128\n",
      "mean row mean pruning test accuracy: 94.77%, sparsity: 0.6148\n",
      "mean block mean pruning test accuracy: 92.47%, sparsity: 0.6148\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 96.65%, sparsity: 0.5263\n",
      "per row magnitude pruning test accuracy: 95.19%, sparsity: 0.4848\n",
      "per block magnitude pruning test accuracy: 95.87%, sparsity: 0.5295\n",
      "mean column mean pruning test accuracy: 95.91%, sparsity: 0.5267\n",
      "mean row mean pruning test accuracy: 95.88%, sparsity: 0.5295\n",
      "mean block mean pruning test accuracy: 95.70%, sparsity: 0.5295\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 97.10%, sparsity: 0.4214\n",
      "per row magnitude pruning test accuracy: 96.65%, sparsity: 0.3702\n",
      "per block magnitude pruning test accuracy: 96.89%, sparsity: 0.4254\n",
      "mean column mean pruning test accuracy: 96.59%, sparsity: 0.4216\n",
      "mean row mean pruning test accuracy: 95.97%, sparsity: 0.4254\n",
      "mean block mean pruning test accuracy: 96.86%, sparsity: 0.4254\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 97.13%, sparsity: 0.2933\n",
      "per row magnitude pruning test accuracy: 97.04%, sparsity: 0.2303\n",
      "per block magnitude pruning test accuracy: 97.12%, sparsity: 0.2981\n",
      "mean column mean pruning test accuracy: 96.92%, sparsity: 0.2934\n",
      "mean row mean pruning test accuracy: 96.20%, sparsity: 0.2981\n",
      "mean block mean pruning test accuracy: 97.06%, sparsity: 0.2981\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 97.10%, sparsity: 0.1368\n",
      "per row magnitude pruning test accuracy: 97.07%, sparsity: 0.0680\n",
      "per block magnitude pruning test accuracy: 97.09%, sparsity: 0.1428\n",
      "mean column mean pruning test accuracy: 97.13%, sparsity: 0.1378\n",
      "mean row mean pruning test accuracy: 96.65%, sparsity: 0.1428\n",
      "mean block mean pruning test accuracy: 97.05%, sparsity: 0.1428\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 97.05%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.02%, sparsity: 0.0063\n",
      "per block magnitude pruning test accuracy: 97.03%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.11%, sparsity: 0.0168\n",
      "mean row mean pruning test accuracy: 97.02%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.03%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 97.04%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.03%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 97.03%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.08%, sparsity: 0.0040\n",
      "mean row mean pruning test accuracy: 97.03%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.03%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.03%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 97.04%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.03%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 97.03%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.08%, sparsity: 0.0026\n",
      "mean row mean pruning test accuracy: 97.04%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.03%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 567,434\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 85.58%, sparsity: 0.7914\n",
      "per row magnitude pruning test accuracy: 49.55%, sparsity: 0.7651\n",
      "per block magnitude pruning test accuracy: 45.80%, sparsity: 0.7923\n",
      "mean column mean pruning test accuracy: 81.69%, sparsity: 0.7916\n",
      "mean row mean pruning test accuracy: 85.29%, sparsity: 0.7923\n",
      "mean block mean pruning test accuracy: 39.44%, sparsity: 0.7923\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 91.63%, sparsity: 0.7452\n",
      "per row magnitude pruning test accuracy: 62.97%, sparsity: 0.7127\n",
      "per block magnitude pruning test accuracy: 65.79%, sparsity: 0.7463\n",
      "mean column mean pruning test accuracy: 89.46%, sparsity: 0.7453\n",
      "mean row mean pruning test accuracy: 91.61%, sparsity: 0.7463\n",
      "mean block mean pruning test accuracy: 63.06%, sparsity: 0.7463\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 94.38%, sparsity: 0.6888\n",
      "per row magnitude pruning test accuracy: 77.95%, sparsity: 0.6489\n",
      "per block magnitude pruning test accuracy: 81.82%, sparsity: 0.6902\n",
      "mean column mean pruning test accuracy: 94.79%, sparsity: 0.6887\n",
      "mean row mean pruning test accuracy: 94.79%, sparsity: 0.6902\n",
      "mean block mean pruning test accuracy: 82.53%, sparsity: 0.6902\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 96.99%, sparsity: 0.6199\n",
      "per row magnitude pruning test accuracy: 92.38%, sparsity: 0.5707\n",
      "per block magnitude pruning test accuracy: 93.94%, sparsity: 0.6216\n",
      "mean column mean pruning test accuracy: 96.64%, sparsity: 0.6196\n",
      "mean row mean pruning test accuracy: 96.33%, sparsity: 0.6216\n",
      "mean block mean pruning test accuracy: 91.52%, sparsity: 0.6216\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 97.56%, sparsity: 0.5357\n",
      "per row magnitude pruning test accuracy: 96.43%, sparsity: 0.4754\n",
      "per block magnitude pruning test accuracy: 96.66%, sparsity: 0.5378\n",
      "mean column mean pruning test accuracy: 97.43%, sparsity: 0.5352\n",
      "mean row mean pruning test accuracy: 96.46%, sparsity: 0.5378\n",
      "mean block mean pruning test accuracy: 96.41%, sparsity: 0.5378\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 97.78%, sparsity: 0.4329\n",
      "per row magnitude pruning test accuracy: 97.33%, sparsity: 0.3589\n",
      "per block magnitude pruning test accuracy: 97.43%, sparsity: 0.4355\n",
      "mean column mean pruning test accuracy: 97.58%, sparsity: 0.4321\n",
      "mean row mean pruning test accuracy: 96.31%, sparsity: 0.4355\n",
      "mean block mean pruning test accuracy: 97.54%, sparsity: 0.4355\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 97.76%, sparsity: 0.3074\n",
      "per row magnitude pruning test accuracy: 97.75%, sparsity: 0.2168\n",
      "per block magnitude pruning test accuracy: 97.76%, sparsity: 0.3105\n",
      "mean column mean pruning test accuracy: 97.79%, sparsity: 0.3063\n",
      "mean row mean pruning test accuracy: 96.63%, sparsity: 0.3105\n",
      "mean block mean pruning test accuracy: 97.70%, sparsity: 0.3105\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 97.83%, sparsity: 0.1544\n",
      "per row magnitude pruning test accuracy: 97.85%, sparsity: 0.0496\n",
      "per block magnitude pruning test accuracy: 97.84%, sparsity: 0.1583\n",
      "mean column mean pruning test accuracy: 97.84%, sparsity: 0.1535\n",
      "mean row mean pruning test accuracy: 97.64%, sparsity: 0.1583\n",
      "mean block mean pruning test accuracy: 97.81%, sparsity: 0.1583\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 97.81%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.82%, sparsity: 0.0041\n",
      "per block magnitude pruning test accuracy: 97.82%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.84%, sparsity: 0.0197\n",
      "mean row mean pruning test accuracy: 97.86%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.82%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 97.81%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.82%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 97.82%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.84%, sparsity: 0.0043\n",
      "mean row mean pruning test accuracy: 97.81%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.82%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 97.82%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 97.81%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.82%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 97.82%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.84%, sparsity: 0.0017\n",
      "mean row mean pruning test accuracy: 97.81%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.82%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 1,494,154\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 91.04%, sparsity: 0.7920\n",
      "per row magnitude pruning test accuracy: 52.08%, sparsity: 0.7652\n",
      "per block magnitude pruning test accuracy: 55.50%, sparsity: 0.7933\n",
      "mean column mean pruning test accuracy: 86.22%, sparsity: 0.7921\n",
      "mean row mean pruning test accuracy: 76.61%, sparsity: 0.7933\n",
      "mean block mean pruning test accuracy: 54.38%, sparsity: 0.7933\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 95.94%, sparsity: 0.7459\n",
      "per row magnitude pruning test accuracy: 63.62%, sparsity: 0.7130\n",
      "per block magnitude pruning test accuracy: 71.12%, sparsity: 0.7476\n",
      "mean column mean pruning test accuracy: 93.55%, sparsity: 0.7459\n",
      "mean row mean pruning test accuracy: 85.26%, sparsity: 0.7476\n",
      "mean block mean pruning test accuracy: 62.85%, sparsity: 0.7476\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 96.96%, sparsity: 0.6896\n",
      "per row magnitude pruning test accuracy: 75.17%, sparsity: 0.6493\n",
      "per block magnitude pruning test accuracy: 82.50%, sparsity: 0.6917\n",
      "mean column mean pruning test accuracy: 95.36%, sparsity: 0.6895\n",
      "mean row mean pruning test accuracy: 92.96%, sparsity: 0.6917\n",
      "mean block mean pruning test accuracy: 76.65%, sparsity: 0.6917\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.52%, sparsity: 0.6209\n",
      "per row magnitude pruning test accuracy: 88.14%, sparsity: 0.5714\n",
      "per block magnitude pruning test accuracy: 91.09%, sparsity: 0.6235\n",
      "mean column mean pruning test accuracy: 96.72%, sparsity: 0.6206\n",
      "mean row mean pruning test accuracy: 96.30%, sparsity: 0.6235\n",
      "mean block mean pruning test accuracy: 88.11%, sparsity: 0.6235\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 97.81%, sparsity: 0.5370\n",
      "per row magnitude pruning test accuracy: 95.12%, sparsity: 0.4763\n",
      "per block magnitude pruning test accuracy: 95.70%, sparsity: 0.5401\n",
      "mean column mean pruning test accuracy: 97.23%, sparsity: 0.5365\n",
      "mean row mean pruning test accuracy: 97.16%, sparsity: 0.5401\n",
      "mean block mean pruning test accuracy: 94.94%, sparsity: 0.5401\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.06%, sparsity: 0.4345\n",
      "per row magnitude pruning test accuracy: 97.28%, sparsity: 0.3602\n",
      "per block magnitude pruning test accuracy: 97.59%, sparsity: 0.4383\n",
      "mean column mean pruning test accuracy: 97.39%, sparsity: 0.4337\n",
      "mean row mean pruning test accuracy: 96.86%, sparsity: 0.4383\n",
      "mean block mean pruning test accuracy: 97.19%, sparsity: 0.4383\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.14%, sparsity: 0.3092\n",
      "per row magnitude pruning test accuracy: 97.95%, sparsity: 0.2183\n",
      "per block magnitude pruning test accuracy: 98.00%, sparsity: 0.3139\n",
      "mean column mean pruning test accuracy: 97.74%, sparsity: 0.3082\n",
      "mean row mean pruning test accuracy: 97.05%, sparsity: 0.3139\n",
      "mean block mean pruning test accuracy: 97.82%, sparsity: 0.3139\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.08%, sparsity: 0.1564\n",
      "per row magnitude pruning test accuracy: 98.05%, sparsity: 0.0513\n",
      "per block magnitude pruning test accuracy: 98.10%, sparsity: 0.1621\n",
      "mean column mean pruning test accuracy: 98.03%, sparsity: 0.1552\n",
      "mean row mean pruning test accuracy: 97.69%, sparsity: 0.1621\n",
      "mean block mean pruning test accuracy: 97.99%, sparsity: 0.1621\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.14%, sparsity: 0.0022\n",
      "per row magnitude pruning test accuracy: 98.11%, sparsity: 0.0023\n",
      "per block magnitude pruning test accuracy: 98.13%, sparsity: 0.0023\n",
      "mean column mean pruning test accuracy: 98.09%, sparsity: 0.0154\n",
      "mean row mean pruning test accuracy: 98.09%, sparsity: 0.0023\n",
      "mean block mean pruning test accuracy: 98.12%, sparsity: 0.0023\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.13%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.13%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.13%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.11%, sparsity: 0.0022\n",
      "mean row mean pruning test accuracy: 98.12%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.13%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.13%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.13%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.13%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.13%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.11%, sparsity: 0.0013\n",
      "mean row mean pruning test accuracy: 98.14%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.13%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 4,235,786\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 93.85%, sparsity: 0.7907\n",
      "per row magnitude pruning test accuracy: 87.41%, sparsity: 0.7708\n",
      "per block magnitude pruning test accuracy: 88.44%, sparsity: 0.7953\n",
      "mean column mean pruning test accuracy: 82.73%, sparsity: 0.7945\n",
      "mean row mean pruning test accuracy: 92.41%, sparsity: 0.7953\n",
      "mean block mean pruning test accuracy: 84.71%, sparsity: 0.7953\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 96.40%, sparsity: 0.7443\n",
      "per row magnitude pruning test accuracy: 90.44%, sparsity: 0.7200\n",
      "per block magnitude pruning test accuracy: 91.17%, sparsity: 0.7500\n",
      "mean column mean pruning test accuracy: 88.48%, sparsity: 0.7489\n",
      "mean row mean pruning test accuracy: 93.15%, sparsity: 0.7500\n",
      "mean block mean pruning test accuracy: 89.07%, sparsity: 0.7500\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.16%, sparsity: 0.6877\n",
      "per row magnitude pruning test accuracy: 93.14%, sparsity: 0.6578\n",
      "per block magnitude pruning test accuracy: 93.47%, sparsity: 0.6947\n",
      "mean column mean pruning test accuracy: 92.36%, sparsity: 0.6932\n",
      "mean row mean pruning test accuracy: 94.20%, sparsity: 0.6947\n",
      "mean block mean pruning test accuracy: 92.28%, sparsity: 0.6947\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.49%, sparsity: 0.6186\n",
      "per row magnitude pruning test accuracy: 95.25%, sparsity: 0.5820\n",
      "per block magnitude pruning test accuracy: 95.75%, sparsity: 0.6271\n",
      "mean column mean pruning test accuracy: 94.01%, sparsity: 0.6252\n",
      "mean row mean pruning test accuracy: 95.63%, sparsity: 0.6271\n",
      "mean block mean pruning test accuracy: 94.61%, sparsity: 0.6271\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 97.79%, sparsity: 0.5341\n",
      "per row magnitude pruning test accuracy: 96.71%, sparsity: 0.4893\n",
      "per block magnitude pruning test accuracy: 97.16%, sparsity: 0.5445\n",
      "mean column mean pruning test accuracy: 95.06%, sparsity: 0.5421\n",
      "mean row mean pruning test accuracy: 96.80%, sparsity: 0.5445\n",
      "mean block mean pruning test accuracy: 96.44%, sparsity: 0.5445\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 97.92%, sparsity: 0.4310\n",
      "per row magnitude pruning test accuracy: 97.66%, sparsity: 0.3761\n",
      "per block magnitude pruning test accuracy: 97.85%, sparsity: 0.4437\n",
      "mean column mean pruning test accuracy: 96.60%, sparsity: 0.4407\n",
      "mean row mean pruning test accuracy: 97.63%, sparsity: 0.4437\n",
      "mean block mean pruning test accuracy: 97.54%, sparsity: 0.4437\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 97.96%, sparsity: 0.3050\n",
      "per row magnitude pruning test accuracy: 98.03%, sparsity: 0.2379\n",
      "per block magnitude pruning test accuracy: 98.08%, sparsity: 0.3205\n",
      "mean column mean pruning test accuracy: 97.59%, sparsity: 0.3167\n",
      "mean row mean pruning test accuracy: 97.88%, sparsity: 0.3205\n",
      "mean block mean pruning test accuracy: 97.93%, sparsity: 0.3205\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 97.96%, sparsity: 0.1512\n",
      "per row magnitude pruning test accuracy: 98.02%, sparsity: 0.0711\n",
      "per block magnitude pruning test accuracy: 97.99%, sparsity: 0.1702\n",
      "mean column mean pruning test accuracy: 97.98%, sparsity: 0.1656\n",
      "mean row mean pruning test accuracy: 97.97%, sparsity: 0.1702\n",
      "mean block mean pruning test accuracy: 97.99%, sparsity: 0.1702\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 97.94%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 97.98%, sparsity: 0.0012\n",
      "per block magnitude pruning test accuracy: 97.95%, sparsity: 0.0240\n",
      "mean column mean pruning test accuracy: 97.95%, sparsity: 0.0352\n",
      "mean row mean pruning test accuracy: 98.04%, sparsity: 0.0240\n",
      "mean block mean pruning test accuracy: 97.96%, sparsity: 0.0240\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 97.94%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.95%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 97.95%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.95%, sparsity: 0.0037\n",
      "mean row mean pruning test accuracy: 97.96%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.95%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.95%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 97.94%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.95%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 97.95%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.95%, sparsity: 0.0009\n",
      "mean row mean pruning test accuracy: 97.94%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.95%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 13,714,442\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 94.03%, sparsity: 0.8239\n",
      "per row magnitude pruning test accuracy: 73.09%, sparsity: 0.8187\n",
      "per block magnitude pruning test accuracy: 79.79%, sparsity: 0.8343\n",
      "mean column mean pruning test accuracy: 46.95%, sparsity: 0.8052\n",
      "mean row mean pruning test accuracy: 89.95%, sparsity: 0.8343\n",
      "mean block mean pruning test accuracy: 88.86%, sparsity: 0.8343\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 96.00%, sparsity: 0.7850\n",
      "per row magnitude pruning test accuracy: 79.12%, sparsity: 0.7785\n",
      "per block magnitude pruning test accuracy: 85.49%, sparsity: 0.7976\n",
      "mean column mean pruning test accuracy: 76.22%, sparsity: 0.7621\n",
      "mean row mean pruning test accuracy: 90.56%, sparsity: 0.7976\n",
      "mean block mean pruning test accuracy: 90.25%, sparsity: 0.7976\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.18%, sparsity: 0.7374\n",
      "per row magnitude pruning test accuracy: 84.45%, sparsity: 0.7293\n",
      "per block magnitude pruning test accuracy: 89.25%, sparsity: 0.7527\n",
      "mean column mean pruning test accuracy: 82.00%, sparsity: 0.7093\n",
      "mean row mean pruning test accuracy: 91.90%, sparsity: 0.7527\n",
      "mean block mean pruning test accuracy: 91.77%, sparsity: 0.7527\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.90%, sparsity: 0.6792\n",
      "per row magnitude pruning test accuracy: 89.92%, sparsity: 0.6693\n",
      "per block magnitude pruning test accuracy: 92.39%, sparsity: 0.6980\n",
      "mean column mean pruning test accuracy: 82.63%, sparsity: 0.6449\n",
      "mean row mean pruning test accuracy: 93.20%, sparsity: 0.6980\n",
      "mean block mean pruning test accuracy: 93.36%, sparsity: 0.6980\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.26%, sparsity: 0.6082\n",
      "per row magnitude pruning test accuracy: 94.26%, sparsity: 0.5961\n",
      "per block magnitude pruning test accuracy: 95.28%, sparsity: 0.6311\n",
      "mean column mean pruning test accuracy: 84.40%, sparsity: 0.5662\n",
      "mean row mean pruning test accuracy: 95.38%, sparsity: 0.6311\n",
      "mean block mean pruning test accuracy: 95.12%, sparsity: 0.6311\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.43%, sparsity: 0.5214\n",
      "per row magnitude pruning test accuracy: 96.93%, sparsity: 0.5066\n",
      "per block magnitude pruning test accuracy: 97.38%, sparsity: 0.5495\n",
      "mean column mean pruning test accuracy: 88.62%, sparsity: 0.4701\n",
      "mean row mean pruning test accuracy: 97.04%, sparsity: 0.5495\n",
      "mean block mean pruning test accuracy: 96.72%, sparsity: 0.5495\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.48%, sparsity: 0.4155\n",
      "per row magnitude pruning test accuracy: 97.98%, sparsity: 0.3973\n",
      "per block magnitude pruning test accuracy: 98.24%, sparsity: 0.4497\n",
      "mean column mean pruning test accuracy: 94.84%, sparsity: 0.3528\n",
      "mean row mean pruning test accuracy: 97.98%, sparsity: 0.4497\n",
      "mean block mean pruning test accuracy: 97.81%, sparsity: 0.4497\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.49%, sparsity: 0.2860\n",
      "per row magnitude pruning test accuracy: 98.36%, sparsity: 0.2641\n",
      "per block magnitude pruning test accuracy: 98.43%, sparsity: 0.3279\n",
      "mean column mean pruning test accuracy: 97.92%, sparsity: 0.2096\n",
      "mean row mean pruning test accuracy: 98.26%, sparsity: 0.3279\n",
      "mean block mean pruning test accuracy: 98.31%, sparsity: 0.3279\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.47%, sparsity: 0.1329\n",
      "per row magnitude pruning test accuracy: 98.43%, sparsity: 0.1355\n",
      "per block magnitude pruning test accuracy: 98.52%, sparsity: 0.1793\n",
      "mean column mean pruning test accuracy: 98.45%, sparsity: 0.0633\n",
      "mean row mean pruning test accuracy: 98.43%, sparsity: 0.1793\n",
      "mean block mean pruning test accuracy: 98.42%, sparsity: 0.1793\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.50%, sparsity: 0.0060\n",
      "per row magnitude pruning test accuracy: 98.51%, sparsity: 0.0520\n",
      "per block magnitude pruning test accuracy: 98.51%, sparsity: 0.0355\n",
      "mean column mean pruning test accuracy: 98.49%, sparsity: 0.0116\n",
      "mean row mean pruning test accuracy: 98.48%, sparsity: 0.0355\n",
      "mean block mean pruning test accuracy: 98.48%, sparsity: 0.0355\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 98.51%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.50%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.51%, sparsity: 0.0103\n",
      "per block magnitude pruning test accuracy: 98.51%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.50%, sparsity: 0.0006\n",
      "mean row mean pruning test accuracy: 98.50%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.51%, sparsity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = SigmoidLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_mnist/sigmoid_{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e59b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> 32 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 52,650\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 70.07%, sparsity: 0.7842\n",
      "per row magnitude pruning test accuracy: 79.24%, sparsity: 0.7694\n",
      "per block magnitude pruning test accuracy: 80.31%, sparsity: 0.7842\n",
      "mean column mean pruning test accuracy: 71.46%, sparsity: 0.7847\n",
      "mean row mean pruning test accuracy: 73.38%, sparsity: 0.7842\n",
      "mean block mean pruning test accuracy: 82.36%, sparsity: 0.7842\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 81.51%, sparsity: 0.7364\n",
      "per row magnitude pruning test accuracy: 87.48%, sparsity: 0.7167\n",
      "per block magnitude pruning test accuracy: 89.35%, sparsity: 0.7364\n",
      "mean column mean pruning test accuracy: 77.80%, sparsity: 0.7369\n",
      "mean row mean pruning test accuracy: 86.42%, sparsity: 0.7364\n",
      "mean block mean pruning test accuracy: 91.24%, sparsity: 0.7364\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 92.11%, sparsity: 0.6780\n",
      "per row magnitude pruning test accuracy: 93.04%, sparsity: 0.6522\n",
      "per block magnitude pruning test accuracy: 93.99%, sparsity: 0.6780\n",
      "mean column mean pruning test accuracy: 90.73%, sparsity: 0.6783\n",
      "mean row mean pruning test accuracy: 92.65%, sparsity: 0.6780\n",
      "mean block mean pruning test accuracy: 94.16%, sparsity: 0.6780\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 94.64%, sparsity: 0.6067\n",
      "per row magnitude pruning test accuracy: 95.40%, sparsity: 0.5732\n",
      "per block magnitude pruning test accuracy: 96.02%, sparsity: 0.6067\n",
      "mean column mean pruning test accuracy: 94.30%, sparsity: 0.6070\n",
      "mean row mean pruning test accuracy: 95.17%, sparsity: 0.6067\n",
      "mean block mean pruning test accuracy: 96.11%, sparsity: 0.6067\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 96.24%, sparsity: 0.5196\n",
      "per row magnitude pruning test accuracy: 96.20%, sparsity: 0.4771\n",
      "per block magnitude pruning test accuracy: 96.63%, sparsity: 0.5197\n",
      "mean column mean pruning test accuracy: 95.79%, sparsity: 0.5196\n",
      "mean row mean pruning test accuracy: 96.21%, sparsity: 0.5197\n",
      "mean block mean pruning test accuracy: 96.61%, sparsity: 0.5197\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 96.90%, sparsity: 0.4132\n",
      "per row magnitude pruning test accuracy: 96.91%, sparsity: 0.3591\n",
      "per block magnitude pruning test accuracy: 96.94%, sparsity: 0.4133\n",
      "mean column mean pruning test accuracy: 96.84%, sparsity: 0.4130\n",
      "mean row mean pruning test accuracy: 96.76%, sparsity: 0.4133\n",
      "mean block mean pruning test accuracy: 96.99%, sparsity: 0.4133\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 97.10%, sparsity: 0.2838\n",
      "per row magnitude pruning test accuracy: 96.99%, sparsity: 0.2161\n",
      "per block magnitude pruning test accuracy: 97.04%, sparsity: 0.2839\n",
      "mean column mean pruning test accuracy: 96.97%, sparsity: 0.2833\n",
      "mean row mean pruning test accuracy: 97.01%, sparsity: 0.2839\n",
      "mean block mean pruning test accuracy: 97.02%, sparsity: 0.2839\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 97.10%, sparsity: 0.1277\n",
      "per row magnitude pruning test accuracy: 97.02%, sparsity: 0.0636\n",
      "per block magnitude pruning test accuracy: 97.09%, sparsity: 0.1278\n",
      "mean column mean pruning test accuracy: 97.13%, sparsity: 0.1274\n",
      "mean row mean pruning test accuracy: 97.06%, sparsity: 0.1278\n",
      "mean block mean pruning test accuracy: 97.04%, sparsity: 0.1278\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 97.14%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 97.02%, sparsity: 0.0201\n",
      "per block magnitude pruning test accuracy: 97.09%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 97.06%, sparsity: 0.0117\n",
      "mean row mean pruning test accuracy: 97.04%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 97.09%, sparsity: 0.0001\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 97.16%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 97.09%, sparsity: 0.0169\n",
      "per block magnitude pruning test accuracy: 97.09%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 97.04%, sparsity: 0.0037\n",
      "mean row mean pruning test accuracy: 97.06%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 97.09%, sparsity: 0.0001\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 97.09%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 97.16%, sparsity: 0.0001\n",
      "per row magnitude pruning test accuracy: 97.09%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 97.09%, sparsity: 0.0001\n",
      "mean column mean pruning test accuracy: 97.04%, sparsity: 0.0020\n",
      "mean row mean pruning test accuracy: 97.06%, sparsity: 0.0001\n",
      "mean block mean pruning test accuracy: 97.09%, sparsity: 0.0001\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 256 -> 256 -> 256 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 400,906\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 95.44%, sparsity: 0.7664\n",
      "per row magnitude pruning test accuracy: 96.96%, sparsity: 0.7579\n",
      "per block magnitude pruning test accuracy: 96.94%, sparsity: 0.7664\n",
      "mean column mean pruning test accuracy: 96.21%, sparsity: 0.7669\n",
      "mean row mean pruning test accuracy: 96.91%, sparsity: 0.7664\n",
      "mean block mean pruning test accuracy: 96.88%, sparsity: 0.7664\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 97.08%, sparsity: 0.7147\n",
      "per row magnitude pruning test accuracy: 97.46%, sparsity: 0.7038\n",
      "per block magnitude pruning test accuracy: 97.50%, sparsity: 0.7147\n",
      "mean column mean pruning test accuracy: 97.14%, sparsity: 0.7150\n",
      "mean row mean pruning test accuracy: 97.53%, sparsity: 0.7147\n",
      "mean block mean pruning test accuracy: 97.49%, sparsity: 0.7147\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.47%, sparsity: 0.6515\n",
      "per row magnitude pruning test accuracy: 97.87%, sparsity: 0.6377\n",
      "per block magnitude pruning test accuracy: 97.71%, sparsity: 0.6515\n",
      "mean column mean pruning test accuracy: 97.65%, sparsity: 0.6517\n",
      "mean row mean pruning test accuracy: 97.82%, sparsity: 0.6515\n",
      "mean block mean pruning test accuracy: 97.75%, sparsity: 0.6515\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.92%, sparsity: 0.5744\n",
      "per row magnitude pruning test accuracy: 97.94%, sparsity: 0.5571\n",
      "per block magnitude pruning test accuracy: 97.92%, sparsity: 0.5744\n",
      "mean column mean pruning test accuracy: 97.79%, sparsity: 0.5743\n",
      "mean row mean pruning test accuracy: 97.96%, sparsity: 0.5744\n",
      "mean block mean pruning test accuracy: 97.94%, sparsity: 0.5744\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 97.92%, sparsity: 0.4801\n",
      "per row magnitude pruning test accuracy: 97.98%, sparsity: 0.4584\n",
      "per block magnitude pruning test accuracy: 97.92%, sparsity: 0.4801\n",
      "mean column mean pruning test accuracy: 97.93%, sparsity: 0.4797\n",
      "mean row mean pruning test accuracy: 97.91%, sparsity: 0.4801\n",
      "mean block mean pruning test accuracy: 97.92%, sparsity: 0.4801\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 97.99%, sparsity: 0.3650\n",
      "per row magnitude pruning test accuracy: 97.95%, sparsity: 0.3381\n",
      "per block magnitude pruning test accuracy: 97.96%, sparsity: 0.3650\n",
      "mean column mean pruning test accuracy: 97.95%, sparsity: 0.3642\n",
      "mean row mean pruning test accuracy: 97.86%, sparsity: 0.3650\n",
      "mean block mean pruning test accuracy: 97.94%, sparsity: 0.3650\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 97.94%, sparsity: 0.2244\n",
      "per row magnitude pruning test accuracy: 97.94%, sparsity: 0.1912\n",
      "per block magnitude pruning test accuracy: 97.93%, sparsity: 0.2244\n",
      "mean column mean pruning test accuracy: 97.94%, sparsity: 0.2232\n",
      "mean row mean pruning test accuracy: 98.00%, sparsity: 0.2244\n",
      "mean block mean pruning test accuracy: 97.92%, sparsity: 0.2244\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 97.96%, sparsity: 0.0548\n",
      "per row magnitude pruning test accuracy: 97.98%, sparsity: 0.0313\n",
      "per block magnitude pruning test accuracy: 97.96%, sparsity: 0.0548\n",
      "mean column mean pruning test accuracy: 97.94%, sparsity: 0.0594\n",
      "mean row mean pruning test accuracy: 97.97%, sparsity: 0.0548\n",
      "mean block mean pruning test accuracy: 97.96%, sparsity: 0.0548\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 97.96%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.95%, sparsity: 0.0048\n",
      "per block magnitude pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.91%, sparsity: 0.0054\n",
      "mean row mean pruning test accuracy: 97.95%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.97%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.91%, sparsity: 0.0028\n",
      "mean row mean pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 97.97%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.97%, sparsity: 0.0045\n",
      "per block magnitude pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.91%, sparsity: 0.0026\n",
      "mean row mean pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.97%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 567,434\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 96.13%, sparsity: 0.7725\n",
      "per row magnitude pruning test accuracy: 96.93%, sparsity: 0.7590\n",
      "per block magnitude pruning test accuracy: 97.26%, sparsity: 0.7725\n",
      "mean column mean pruning test accuracy: 95.97%, sparsity: 0.7722\n",
      "mean row mean pruning test accuracy: 96.77%, sparsity: 0.7725\n",
      "mean block mean pruning test accuracy: 97.21%, sparsity: 0.7725\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 97.09%, sparsity: 0.7221\n",
      "per row magnitude pruning test accuracy: 97.54%, sparsity: 0.7053\n",
      "per block magnitude pruning test accuracy: 97.80%, sparsity: 0.7221\n",
      "mean column mean pruning test accuracy: 96.74%, sparsity: 0.7216\n",
      "mean row mean pruning test accuracy: 97.54%, sparsity: 0.7221\n",
      "mean block mean pruning test accuracy: 97.82%, sparsity: 0.7221\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.75%, sparsity: 0.6605\n",
      "per row magnitude pruning test accuracy: 97.82%, sparsity: 0.6398\n",
      "per block magnitude pruning test accuracy: 98.08%, sparsity: 0.6606\n",
      "mean column mean pruning test accuracy: 97.70%, sparsity: 0.6598\n",
      "mean row mean pruning test accuracy: 97.89%, sparsity: 0.6606\n",
      "mean block mean pruning test accuracy: 98.08%, sparsity: 0.6606\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.97%, sparsity: 0.5854\n",
      "per row magnitude pruning test accuracy: 98.09%, sparsity: 0.5598\n",
      "per block magnitude pruning test accuracy: 98.15%, sparsity: 0.5854\n",
      "mean column mean pruning test accuracy: 97.93%, sparsity: 0.5843\n",
      "mean row mean pruning test accuracy: 98.03%, sparsity: 0.5854\n",
      "mean block mean pruning test accuracy: 98.14%, sparsity: 0.5854\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.12%, sparsity: 0.4936\n",
      "per row magnitude pruning test accuracy: 98.13%, sparsity: 0.4620\n",
      "per block magnitude pruning test accuracy: 98.15%, sparsity: 0.4936\n",
      "mean column mean pruning test accuracy: 98.14%, sparsity: 0.4920\n",
      "mean row mean pruning test accuracy: 98.10%, sparsity: 0.4936\n",
      "mean block mean pruning test accuracy: 98.14%, sparsity: 0.4936\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.12%, sparsity: 0.3815\n",
      "per row magnitude pruning test accuracy: 98.18%, sparsity: 0.3425\n",
      "per block magnitude pruning test accuracy: 98.14%, sparsity: 0.3815\n",
      "mean column mean pruning test accuracy: 98.17%, sparsity: 0.3794\n",
      "mean row mean pruning test accuracy: 98.22%, sparsity: 0.3815\n",
      "mean block mean pruning test accuracy: 98.16%, sparsity: 0.3815\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.15%, sparsity: 0.2445\n",
      "per row magnitude pruning test accuracy: 98.12%, sparsity: 0.1968\n",
      "per block magnitude pruning test accuracy: 98.15%, sparsity: 0.2445\n",
      "mean column mean pruning test accuracy: 98.17%, sparsity: 0.2419\n",
      "mean row mean pruning test accuracy: 98.19%, sparsity: 0.2445\n",
      "mean block mean pruning test accuracy: 98.15%, sparsity: 0.2445\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.17%, sparsity: 0.0778\n",
      "per row magnitude pruning test accuracy: 98.19%, sparsity: 0.0369\n",
      "per block magnitude pruning test accuracy: 98.19%, sparsity: 0.0778\n",
      "mean column mean pruning test accuracy: 98.18%, sparsity: 0.0753\n",
      "mean row mean pruning test accuracy: 98.19%, sparsity: 0.0778\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0778\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.17%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.19%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.17%, sparsity: 0.0057\n",
      "mean row mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.17%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.18%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.17%, sparsity: 0.0017\n",
      "mean row mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 98.18%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.17%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.18%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.17%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.18%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 1024 -> 512 -> 256 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 1,494,154\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 96.43%, sparsity: 0.7719\n",
      "per row magnitude pruning test accuracy: 97.14%, sparsity: 0.7605\n",
      "per block magnitude pruning test accuracy: 97.37%, sparsity: 0.7719\n",
      "mean column mean pruning test accuracy: 96.16%, sparsity: 0.7718\n",
      "mean row mean pruning test accuracy: 97.15%, sparsity: 0.7719\n",
      "mean block mean pruning test accuracy: 97.36%, sparsity: 0.7719\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 97.30%, sparsity: 0.7214\n",
      "per row magnitude pruning test accuracy: 97.56%, sparsity: 0.7073\n",
      "per block magnitude pruning test accuracy: 97.67%, sparsity: 0.7214\n",
      "mean column mean pruning test accuracy: 97.18%, sparsity: 0.7211\n",
      "mean row mean pruning test accuracy: 97.62%, sparsity: 0.7214\n",
      "mean block mean pruning test accuracy: 97.79%, sparsity: 0.7214\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 97.65%, sparsity: 0.6597\n",
      "per row magnitude pruning test accuracy: 97.97%, sparsity: 0.6423\n",
      "per block magnitude pruning test accuracy: 97.96%, sparsity: 0.6597\n",
      "mean column mean pruning test accuracy: 97.68%, sparsity: 0.6592\n",
      "mean row mean pruning test accuracy: 97.87%, sparsity: 0.6597\n",
      "mean block mean pruning test accuracy: 97.89%, sparsity: 0.6597\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 97.95%, sparsity: 0.5844\n",
      "per row magnitude pruning test accuracy: 98.03%, sparsity: 0.5629\n",
      "per block magnitude pruning test accuracy: 97.98%, sparsity: 0.5844\n",
      "mean column mean pruning test accuracy: 98.01%, sparsity: 0.5836\n",
      "mean row mean pruning test accuracy: 97.98%, sparsity: 0.5844\n",
      "mean block mean pruning test accuracy: 97.97%, sparsity: 0.5844\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 98.07%, sparsity: 0.4924\n",
      "per row magnitude pruning test accuracy: 98.15%, sparsity: 0.4659\n",
      "per block magnitude pruning test accuracy: 98.13%, sparsity: 0.4924\n",
      "mean column mean pruning test accuracy: 98.09%, sparsity: 0.4913\n",
      "mean row mean pruning test accuracy: 98.10%, sparsity: 0.4924\n",
      "mean block mean pruning test accuracy: 98.11%, sparsity: 0.4924\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 98.06%, sparsity: 0.3800\n",
      "per row magnitude pruning test accuracy: 98.05%, sparsity: 0.3475\n",
      "per block magnitude pruning test accuracy: 98.10%, sparsity: 0.3800\n",
      "mean column mean pruning test accuracy: 98.12%, sparsity: 0.3785\n",
      "mean row mean pruning test accuracy: 98.09%, sparsity: 0.3800\n",
      "mean block mean pruning test accuracy: 98.12%, sparsity: 0.3800\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 98.07%, sparsity: 0.2427\n",
      "per row magnitude pruning test accuracy: 98.09%, sparsity: 0.2029\n",
      "per block magnitude pruning test accuracy: 98.11%, sparsity: 0.2427\n",
      "mean column mean pruning test accuracy: 98.11%, sparsity: 0.2408\n",
      "mean row mean pruning test accuracy: 98.14%, sparsity: 0.2427\n",
      "mean block mean pruning test accuracy: 98.10%, sparsity: 0.2427\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 98.05%, sparsity: 0.0752\n",
      "per row magnitude pruning test accuracy: 98.09%, sparsity: 0.0390\n",
      "per block magnitude pruning test accuracy: 98.06%, sparsity: 0.0752\n",
      "mean column mean pruning test accuracy: 98.08%, sparsity: 0.0732\n",
      "mean row mean pruning test accuracy: 98.10%, sparsity: 0.0752\n",
      "mean block mean pruning test accuracy: 98.06%, sparsity: 0.0752\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 98.06%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.07%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.10%, sparsity: 0.0033\n",
      "mean row mean pruning test accuracy: 98.09%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 98.06%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.07%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.10%, sparsity: 0.0013\n",
      "mean row mean pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 98.08%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 98.06%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 98.07%, sparsity: 0.0018\n",
      "per block magnitude pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 98.10%, sparsity: 0.0013\n",
      "mean row mean pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 98.08%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 4,235,786\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 94.81%, sparsity: 0.7828\n",
      "per row magnitude pruning test accuracy: 94.27%, sparsity: 0.7677\n",
      "per block magnitude pruning test accuracy: 94.49%, sparsity: 0.7828\n",
      "mean column mean pruning test accuracy: 94.06%, sparsity: 0.7863\n",
      "mean row mean pruning test accuracy: 94.28%, sparsity: 0.7828\n",
      "mean block mean pruning test accuracy: 94.52%, sparsity: 0.7828\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 95.65%, sparsity: 0.7347\n",
      "per row magnitude pruning test accuracy: 94.92%, sparsity: 0.7162\n",
      "per block magnitude pruning test accuracy: 94.89%, sparsity: 0.7347\n",
      "mean column mean pruning test accuracy: 95.62%, sparsity: 0.7388\n",
      "mean row mean pruning test accuracy: 95.22%, sparsity: 0.7347\n",
      "mean block mean pruning test accuracy: 94.80%, sparsity: 0.7347\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 96.41%, sparsity: 0.6759\n",
      "per row magnitude pruning test accuracy: 95.42%, sparsity: 0.6532\n",
      "per block magnitude pruning test accuracy: 95.39%, sparsity: 0.6759\n",
      "mean column mean pruning test accuracy: 96.34%, sparsity: 0.6809\n",
      "mean row mean pruning test accuracy: 95.34%, sparsity: 0.6759\n",
      "mean block mean pruning test accuracy: 95.45%, sparsity: 0.6759\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 96.89%, sparsity: 0.6042\n",
      "per row magnitude pruning test accuracy: 95.77%, sparsity: 0.5763\n",
      "per block magnitude pruning test accuracy: 95.80%, sparsity: 0.6042\n",
      "mean column mean pruning test accuracy: 96.82%, sparsity: 0.6102\n",
      "mean row mean pruning test accuracy: 95.57%, sparsity: 0.6042\n",
      "mean block mean pruning test accuracy: 95.87%, sparsity: 0.6042\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 97.17%, sparsity: 0.5165\n",
      "per row magnitude pruning test accuracy: 96.54%, sparsity: 0.4824\n",
      "per block magnitude pruning test accuracy: 96.52%, sparsity: 0.5165\n",
      "mean column mean pruning test accuracy: 97.26%, sparsity: 0.5238\n",
      "mean row mean pruning test accuracy: 95.85%, sparsity: 0.5165\n",
      "mean block mean pruning test accuracy: 96.53%, sparsity: 0.5165\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 97.48%, sparsity: 0.4095\n",
      "per row magnitude pruning test accuracy: 96.99%, sparsity: 0.3677\n",
      "per block magnitude pruning test accuracy: 97.15%, sparsity: 0.4095\n",
      "mean column mean pruning test accuracy: 97.33%, sparsity: 0.4183\n",
      "mean row mean pruning test accuracy: 96.62%, sparsity: 0.4095\n",
      "mean block mean pruning test accuracy: 97.10%, sparsity: 0.4095\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 97.43%, sparsity: 0.2788\n",
      "per row magnitude pruning test accuracy: 97.37%, sparsity: 0.2276\n",
      "per block magnitude pruning test accuracy: 97.38%, sparsity: 0.2788\n",
      "mean column mean pruning test accuracy: 97.52%, sparsity: 0.2894\n",
      "mean row mean pruning test accuracy: 97.21%, sparsity: 0.2788\n",
      "mean block mean pruning test accuracy: 97.39%, sparsity: 0.2788\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 97.49%, sparsity: 0.1191\n",
      "per row magnitude pruning test accuracy: 97.40%, sparsity: 0.0567\n",
      "per block magnitude pruning test accuracy: 97.50%, sparsity: 0.1191\n",
      "mean column mean pruning test accuracy: 97.51%, sparsity: 0.1323\n",
      "mean row mean pruning test accuracy: 97.32%, sparsity: 0.1191\n",
      "mean block mean pruning test accuracy: 97.50%, sparsity: 0.1191\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 97.51%, sparsity: 0.0012\n",
      "per row magnitude pruning test accuracy: 97.50%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 97.49%, sparsity: 0.0012\n",
      "mean column mean pruning test accuracy: 97.55%, sparsity: 0.0295\n",
      "mean row mean pruning test accuracy: 97.45%, sparsity: 0.0012\n",
      "mean block mean pruning test accuracy: 97.49%, sparsity: 0.0012\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 97.50%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.49%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 97.49%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.55%, sparsity: 0.0024\n",
      "mean row mean pruning test accuracy: 97.48%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.49%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 97.49%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 97.50%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.49%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 97.49%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.55%, sparsity: 0.0009\n",
      "mean row mean pruning test accuracy: 97.49%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.49%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 13,714,442\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 88.37%, sparsity: 0.7786\n",
      "per row magnitude pruning test accuracy: 90.61%, sparsity: 0.7676\n",
      "per block magnitude pruning test accuracy: 87.54%, sparsity: 0.7786\n",
      "mean column mean pruning test accuracy: 90.46%, sparsity: 0.7793\n",
      "mean row mean pruning test accuracy: 91.09%, sparsity: 0.7786\n",
      "mean block mean pruning test accuracy: 87.66%, sparsity: 0.7786\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 89.70%, sparsity: 0.7296\n",
      "per row magnitude pruning test accuracy: 89.35%, sparsity: 0.7160\n",
      "per block magnitude pruning test accuracy: 87.06%, sparsity: 0.7296\n",
      "mean column mean pruning test accuracy: 87.69%, sparsity: 0.7303\n",
      "mean row mean pruning test accuracy: 90.28%, sparsity: 0.7296\n",
      "mean block mean pruning test accuracy: 87.15%, sparsity: 0.7296\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 92.77%, sparsity: 0.6697\n",
      "per row magnitude pruning test accuracy: 89.76%, sparsity: 0.6531\n",
      "per block magnitude pruning test accuracy: 88.42%, sparsity: 0.6697\n",
      "mean column mean pruning test accuracy: 88.64%, sparsity: 0.6706\n",
      "mean row mean pruning test accuracy: 89.19%, sparsity: 0.6697\n",
      "mean block mean pruning test accuracy: 88.39%, sparsity: 0.6697\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 94.07%, sparsity: 0.5966\n",
      "per row magnitude pruning test accuracy: 89.91%, sparsity: 0.5762\n",
      "per block magnitude pruning test accuracy: 89.99%, sparsity: 0.5966\n",
      "mean column mean pruning test accuracy: 93.38%, sparsity: 0.5976\n",
      "mean row mean pruning test accuracy: 89.54%, sparsity: 0.5966\n",
      "mean block mean pruning test accuracy: 90.12%, sparsity: 0.5966\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 95.69%, sparsity: 0.5073\n",
      "per row magnitude pruning test accuracy: 92.69%, sparsity: 0.4823\n",
      "per block magnitude pruning test accuracy: 92.36%, sparsity: 0.5073\n",
      "mean column mean pruning test accuracy: 95.78%, sparsity: 0.5084\n",
      "mean row mean pruning test accuracy: 90.93%, sparsity: 0.5073\n",
      "mean block mean pruning test accuracy: 92.74%, sparsity: 0.5073\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 96.82%, sparsity: 0.3982\n",
      "per row magnitude pruning test accuracy: 95.58%, sparsity: 0.3676\n",
      "per block magnitude pruning test accuracy: 95.70%, sparsity: 0.3982\n",
      "mean column mean pruning test accuracy: 96.57%, sparsity: 0.3995\n",
      "mean row mean pruning test accuracy: 94.42%, sparsity: 0.3982\n",
      "mean block mean pruning test accuracy: 95.62%, sparsity: 0.3982\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 96.91%, sparsity: 0.2650\n",
      "per row magnitude pruning test accuracy: 97.02%, sparsity: 0.2276\n",
      "per block magnitude pruning test accuracy: 97.25%, sparsity: 0.2650\n",
      "mean column mean pruning test accuracy: 96.99%, sparsity: 0.2665\n",
      "mean row mean pruning test accuracy: 96.46%, sparsity: 0.2650\n",
      "mean block mean pruning test accuracy: 97.25%, sparsity: 0.2650\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 96.99%, sparsity: 0.1022\n",
      "per row magnitude pruning test accuracy: 97.17%, sparsity: 0.0569\n",
      "per block magnitude pruning test accuracy: 97.10%, sparsity: 0.1022\n",
      "mean column mean pruning test accuracy: 97.09%, sparsity: 0.1051\n",
      "mean row mean pruning test accuracy: 97.16%, sparsity: 0.1022\n",
      "mean block mean pruning test accuracy: 97.10%, sparsity: 0.1022\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 97.02%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.10%, sparsity: 0.0006\n",
      "per block magnitude pruning test accuracy: 97.15%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.16%, sparsity: 0.0171\n",
      "mean row mean pruning test accuracy: 97.00%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.15%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 97.06%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.14%, sparsity: 0.0006\n",
      "per block magnitude pruning test accuracy: 97.15%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.15%, sparsity: 0.0017\n",
      "mean row mean pruning test accuracy: 97.09%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.15%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 97.15%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 97.06%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 97.14%, sparsity: 0.0006\n",
      "per block magnitude pruning test accuracy: 97.15%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 97.15%, sparsity: 0.0006\n",
      "mean row mean pruning test accuracy: 97.10%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 97.15%, sparsity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = tanhLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_mnist/tanh_{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2f1e0",
   "metadata": {},
   "source": [
    "# Fashion MINST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67907eba",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05041e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    # Underfit & brittle\n",
    "    'Tiny_Underfit':\n",
    "        {'hidden_size': [64],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 10,\n",
    "        'dropout': 0.0},\n",
    "    # Deep-narrow (depth sensitivity)\n",
    "    'Deep_Narrow':         \n",
    "        {'hidden_size': [128, 128, 128, 128, 128, 128, 128, 128],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 15,\n",
    "        'dropout': 0.2},\n",
    "    # Well-trained baseline\n",
    "    'Balanced':            \n",
    "        {'hidden_size': [512, 256],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 15,\n",
    "        'dropout': 0.2},\n",
    "    # Deep but still robust\n",
    "    'Balanced_Deep':       \n",
    "        {'hidden_size': [512, 256, 128, 64],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 20,\n",
    "        'dropout': 0.3},\n",
    "    # Overparameterized\n",
    "    'Wide':\n",
    "        {'hidden_size': [2048, 1024],\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 30,\n",
    "        'dropout': 0.0},\n",
    "    # Very overparameterized (optional, keep one)\n",
    "    'Very_Wide':          \n",
    "        {'hidden_size': [4096, 2048, 1024, 512],\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 50,\n",
    "        'dropout': 0.0},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c79006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.295874\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.573173\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.547540\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.497119\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.444866\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.436259\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.418241\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.545832\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.265554\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.320823\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.659058\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.503284\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.483713\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.316713\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.328209\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.359428\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.256873\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.318903\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.324721\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.234197\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.381556\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.396400\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.391923\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.233018\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.391389\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.304973\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.338290\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.459808\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.311549\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.314552\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299632\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.416298\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.977169\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.782425\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.718708\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.576706\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.595952\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.469917\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.597494\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.662849\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.619151\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.580535\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.593562\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.584720\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.541141\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.530009\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.521312\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.595258\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.569683\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.484802\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.561028\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.437101\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.577684\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.531039\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.436817\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.448600\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.522040\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.542903\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.315344\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.513909\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.425265\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.462408\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.470933\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.251346\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.419757\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.205753\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.251109\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.479036\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.352369\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.383096\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.364999\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.492382\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.410117\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.470660\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.227513\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.321978\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.522964\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.385329\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.585690\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.464918\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.398004\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.292984\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.503298\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.349893\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.407012\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.426841\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.336452\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.359006\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.282300\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.347354\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.209513\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.351389\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.446960\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.331302\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.294779\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.174439\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.328990\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.212410\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.276966\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.366814\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.307589\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.308009\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.271719\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.205456\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.158375\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.157979\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.200244\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.274988\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.367305\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.305059\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.281139\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.198598\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.250398\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.159009\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.228882\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.287285\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.266020\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.204027\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.231983\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.207962\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290202\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.825684\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.558569\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.501693\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.585729\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.452632\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.545266\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.517602\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.378163\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.378872\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.452453\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.462557\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.359607\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.402032\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.345286\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.374821\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.323678\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.470409\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.338547\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.455498\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.300597\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.409308\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.369362\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.347487\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.393073\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.240946\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.359027\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.329001\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.348733\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.334018\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.313138\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.214511\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.420530\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.305920\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.420574\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.300550\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.367221\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.332381\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.403013\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.279913\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.229467\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.311278\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.217553\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.372931\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.208485\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.202192\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.315473\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.369926\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.282713\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.325516\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.350515\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.242931\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.283633\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.245305\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.281472\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.320457\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.270118\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.244035\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.282009\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.157762\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296972\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.486539\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.406089\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.455051\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.370116\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.344885\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.318765\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.255936\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.526297\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.266129\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.203459\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.224862\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.199981\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.236213\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.258866\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.265662\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.332941\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.239666\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.272815\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.202328\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.361456\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.388072\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.316534\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.268214\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.280804\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.224515\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.203725\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.239571\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.148291\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.160943\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.292325\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.196808\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.090841\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.148067\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.150523\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.325132\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.134630\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.142048\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.200295\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.245120\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.176179\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.200524\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.148086\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.178457\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.145925\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.078792\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.139357\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.116394\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.133462\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.099269\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.119940\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.084171\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.128248\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.166321\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.140402\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.061417\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.124916\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.123008\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.105265\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.113374\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.057471\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.142263\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.122776\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.137012\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.066160\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.059686\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.073462\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.047990\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.159785\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.080168\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.049610\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.111106\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.249305\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.136673\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.129605\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.044739\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.077044\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.023897\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.056018\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.029299\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.033357\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.133492\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.039395\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.038919\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.086011\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.018525\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.029956\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.017623\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.061535\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.061924\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301716\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.426496\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.412701\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.419107\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.236324\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.287384\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.292366\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.309409\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.359279\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.414472\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.324588\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.355594\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.249700\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.368502\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.371372\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.357786\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.184075\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.328659\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.257329\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.209324\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.302838\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.149053\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.202303\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.193244\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.231479\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.246598\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.144192\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.120067\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.248640\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.255283\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.251657\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.172159\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.228423\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.248613\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.199662\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.160315\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.166838\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.192839\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.147445\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.138057\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.230738\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.145612\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.130507\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.179335\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.242130\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.118445\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.225521\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.135467\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.135694\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.155505\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.176659\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.160781\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.147916\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.152406\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.094575\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.136475\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.110627\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.062138\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.169791\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.097916\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.074294\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.131926\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.156807\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.099384\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.168914\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.048373\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.132722\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.200880\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.094526\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.059031\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.109624\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.069973\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.104528\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.114525\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.130345\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.073700\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.202150\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.035846\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.134075\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.106664\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.077473\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.081684\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.146549\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.127265\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.075653\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.113799\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.155506\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.032966\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.143846\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.048161\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.033516\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.102820\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.034673\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.064981\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.109980\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.098642\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.056117\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.078347\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.076482\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.092646\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.038851\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.050901\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.029209\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.054603\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.039430\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.033548\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.053100\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.113503\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.012668\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.013776\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.061240\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.037039\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.038248\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.046680\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.075988\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.043362\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.085523\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.010248\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.070172\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.026292\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.034043\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.030537\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.040949\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.024945\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.030561\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.030468\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.059768\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.052873\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.080552\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.041597\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.075945\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.064844\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.100306\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.044483\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.063918\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.027383\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.035140\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.053629\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.039588\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.128930\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.115499\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.085802\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.048598\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.013179\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.046270\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.015258\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.109845\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.035717\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.007558\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.067849\n"
     ]
    }
   ],
   "source": [
    "# Train all models\n",
    "all_results = {}\n",
    "\n",
    "datasets_name = 'fashionmnist'\n",
    "\n",
    "train_loader, test_loader, input_size, num_classes, meta = get_loaders(datasets_name, batch_size=128)\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input({input_size}) -> {' -> '.join(map(str, config['hidden_size']))} -> Output({num_classes})\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LinearModel(\n",
    "        input_size=input_size, \n",
    "        output_size=num_classes, \n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    os.makedirs(f'paper_{datasets_name}', exist_ok=True)\n",
    "    model.save(f'paper_{datasets_name}/{model_name}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7286cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.254416\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.556033\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.550090\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.562986\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.409066\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.290658\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.358999\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.379630\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.512687\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.256503\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.358518\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.473921\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.363141\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.413108\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.432265\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.489178\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.343425\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.428449\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.435833\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.365920\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.435757\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.368955\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.302159\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.371944\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.332573\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.434157\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.323803\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.260341\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.271915\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.423200\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296624\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.113475\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.883713\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.686272\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.555196\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.587037\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.610052\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.562065\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.494694\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.460688\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.514900\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.613819\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.417882\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.379737\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.484497\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.376465\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.531004\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.429458\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.412020\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.363614\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.367156\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.507161\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.385211\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.387678\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.338383\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.377265\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.306996\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.385771\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.365635\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.295897\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.179371\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.425100\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.385962\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.210201\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.318764\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.352603\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.298770\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.396989\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.363214\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.530598\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.322066\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.466136\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.281418\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.353117\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.383457\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302756\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.666325\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.490560\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.360043\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.378019\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.411980\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.369003\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.429522\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.353956\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.297387\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.405012\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.306576\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.273930\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.292003\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.417604\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.316893\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.259106\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.307838\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.187351\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.311683\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.199536\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.393107\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.212310\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.238730\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.251987\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.195662\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.294048\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.347843\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.260811\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.264886\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.138972\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.209773\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.315871\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.247283\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.206841\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.241689\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.237370\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.263666\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.146994\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.216001\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.219036\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.202152\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.149133\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.164599\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.191783\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309435\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.624176\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.569085\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.568374\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.474884\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.455670\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.445578\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.444241\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.352419\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.397049\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.389959\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.437808\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.272074\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.393562\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.454376\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.406181\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.339986\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.366103\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.579249\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.270616\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.281417\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.259424\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.328040\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.253582\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.298649\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.387810\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.347629\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.336192\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.433727\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.330509\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.244617\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.220973\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.378852\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.251272\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.347317\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.200611\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.302096\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.349922\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.335012\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.195098\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.149650\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.330658\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.276594\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.155613\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.310676\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.148761\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.238828\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.145900\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.129153\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.280744\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.238374\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.223837\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.186059\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.199199\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.232447\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.163132\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.178672\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.314742\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.197291\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.160863\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301964\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.347495\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.482508\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.580423\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.372836\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.275047\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.270183\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.346088\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.360019\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.278407\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.200553\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.417232\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.295803\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.203483\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.284591\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.254950\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.355860\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.204248\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.270164\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.172009\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.127811\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.186731\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.168557\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.258545\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.159476\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.360779\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.189406\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.131136\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.126991\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.137381\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.158003\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.230174\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.125933\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.166980\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.119684\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.099923\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.172684\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.172569\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.176620\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.102749\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.167914\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.173952\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.120109\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.075656\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.114000\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.091550\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.130579\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.132756\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.087738\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.067797\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.114522\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.083475\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.056753\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.052845\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.097560\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.074493\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.032761\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.088539\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.111036\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.073833\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.055068\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.091100\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.135569\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.067238\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.074087\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.041982\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.099147\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.041524\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.076494\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.062342\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.038038\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.102124\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.059696\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.068303\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.140914\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.072785\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.051440\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.065369\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.065581\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.026849\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.074258\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.062003\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.123529\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.102727\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.119504\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.030106\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.029012\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.102056\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.075094\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.067149\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304282\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.535765\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.403333\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.418675\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.321254\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.427749\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.364407\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.503152\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.158545\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.272212\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.478844\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.281945\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.283053\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.229712\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.261196\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.300406\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.249946\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.152820\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.286977\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.218826\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.205094\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.242539\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.192330\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.272292\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.214439\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.166120\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.264543\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.239557\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.143630\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.286743\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.153959\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.162870\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.209501\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.127448\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.133505\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.212928\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.149982\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.123418\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.147542\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.127448\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.159252\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.152231\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.106687\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.188000\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.140074\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.086195\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.049383\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.180944\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.118390\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.140031\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.118711\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.094213\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.091531\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.120300\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.073256\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.111358\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.172104\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.094987\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.092414\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.129336\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.151439\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.103373\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.053922\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.064661\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.086762\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.144496\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.048457\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.074333\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.064772\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.089436\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.180631\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.071872\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.142168\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.079017\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.047174\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.033689\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.058840\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.046388\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.115631\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.032558\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.158162\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.117337\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.054605\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.123416\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.092235\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.134722\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.113034\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.060457\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.091750\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.140528\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.091134\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.034012\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.077661\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.038850\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.072926\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.053561\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.021022\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.069557\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.082597\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.049052\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.031479\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.190471\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.030657\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.059507\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.075942\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.031761\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.112151\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.064730\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.030710\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.129888\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.288267\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.088588\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.081844\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.014636\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.030611\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.087257\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.036245\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.018306\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.027467\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.050129\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.071475\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.102823\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.078340\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.089253\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.049877\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.043370\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.015764\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.077524\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.024711\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.010040\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.025239\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.018388\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.004661\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.079416\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.058822\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.042024\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.065664\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.020300\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.049502\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.042806\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.027162\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.031777\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.039574\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.027399\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.013995\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.022636\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.032446\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.112630\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.080260\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.023890\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.388919\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.132631\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.897734\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.756510\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.595332\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.642571\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.543037\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.501583\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.519244\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.542606\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.461731\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.507530\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.381419\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.413326\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.403511\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.312976\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.378183\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.331483\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.377073\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.421341\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.440575\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.303100\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.443310\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.418797\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.341569\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.359826\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.399884\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.366668\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.556041\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.257821\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.410720\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.303720\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.233189\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.800800\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.747935\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.736505\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.735952\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.775864\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.698622\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.658587\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.737580\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.639847\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.648123\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.722574\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.699524\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.745745\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.717254\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.693135\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.684757\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.750620\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.673181\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.715464\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.715207\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.731759\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.706211\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.620982\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.567950\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.651836\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.369331\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.334924\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.422260\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.394992\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.241571\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.399986\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.278729\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.314301\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.277946\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.305825\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 1.118635\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.260331\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.058363\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.061167\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.055259\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.053125\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.051379\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.350916\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.832353\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.640905\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.647712\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.592715\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.571602\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.496286\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.533836\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.429739\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.276238\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.463807\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.482003\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.376131\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.354231\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.442389\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.455375\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.290255\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.393590\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.407956\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.336440\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.351149\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.318154\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.366688\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.359226\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.327263\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.306047\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.266515\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.367200\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.421378\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.397585\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.256590\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.403242\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.315878\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.286071\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.289772\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.361881\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.246066\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.339834\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.404346\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.290052\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.351462\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.235262\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.320858\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.371841\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.269095\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.403681\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.786616\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.391760\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.341569\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.188388\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.074838\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.160407\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.085613\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.000352\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.863869\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.808108\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.769809\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.755489\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.791875\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.821188\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.743889\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.727416\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.770606\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.682546\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.613927\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.793317\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.619131\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.564582\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.529380\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.636908\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.529509\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.485120\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.470915\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.554452\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.694435\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.618692\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.547977\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.467786\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.518326\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.692797\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.416653\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.594334\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.356662\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.375289\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.571166\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.567323\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.380222\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.534894\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.525869\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.398953\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.501143\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.519928\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.692411\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.444759\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.456698\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.484570\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.486939\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.477503\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.528962\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.371724\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.543698\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.412466\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.412892\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.399784\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.381168\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305454\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.363009\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.462334\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.307363\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.313026\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.511316\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.345519\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.258790\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.328885\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.324274\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.240612\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.226865\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.329120\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.320819\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.275795\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.296648\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.288626\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.267320\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.255592\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.262061\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.292689\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.268789\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.252017\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.221346\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.224606\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.177855\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.165389\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.116662\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.196581\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.272182\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.166815\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.198645\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.249380\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.148149\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.146840\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.193256\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.141021\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.174239\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.164002\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.146991\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.123862\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.213237\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.090506\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.204130\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.241361\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.096563\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.101470\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.089282\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.110268\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.094860\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.132442\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.096526\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.083790\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.143613\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.055353\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.148467\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.046794\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.115356\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.053692\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.111244\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.119933\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.063963\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.114177\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.089128\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.093717\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.070036\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.088006\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.096309\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.047122\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.117130\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.098357\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.089327\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.037707\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.077803\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.065869\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.107713\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.101972\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.043457\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.067281\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.050476\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.071604\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.056103\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.076730\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.068903\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.028515\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.046076\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.051451\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.022084\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.012563\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.060498\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.374749\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.542340\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.477818\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.514299\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.369954\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.451872\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.380441\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.364434\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.216002\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.350984\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.476156\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.363639\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.283233\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.270244\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.233282\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.341349\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.257596\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.300724\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.240898\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.315473\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.276728\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.281884\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.279720\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.165687\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.295960\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.188637\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.122414\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.149112\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.338358\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.196501\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.196501\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.172666\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.318584\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.105138\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.159884\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.166347\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.311462\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.184819\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.254949\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.167970\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.193840\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.103961\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.150061\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.223605\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.144176\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.201287\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.104088\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.224231\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.251896\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.204074\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.150577\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.086882\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.149027\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.142190\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.115622\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.147515\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.111583\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.102971\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.079244\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.121800\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.146088\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.119947\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.077993\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.084882\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.216717\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.113379\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.135677\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.077163\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.131960\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.148457\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.078965\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.099292\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.087836\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.090067\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.094306\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.074084\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.089308\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.146664\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.017469\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.096832\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.082895\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.044536\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.083571\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.047018\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.096993\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.013133\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.072832\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.101151\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.099035\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.098776\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.049612\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.083012\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.068332\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.057820\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.044963\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.098341\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.009707\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.112300\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.069362\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.084696\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.037188\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.078393\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.104446\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.084110\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.040529\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.053658\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.075268\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.080575\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.023146\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.049827\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.069699\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.031375\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.049675\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.050451\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.040432\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.084137\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.053765\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.014028\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.041926\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.031712\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.071384\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.029859\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.044756\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.077626\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.021442\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.015875\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.065480\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.017493\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.086400\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.035591\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.041441\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.039378\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.048228\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.026261\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.048006\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.047791\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.051688\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.059080\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.015192\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.070078\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.058362\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.012820\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.009561\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.019053\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.056773\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.086528\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.083294\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.019212\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.017796\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.025789\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303999\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.613388\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.531523\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.532393\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.589931\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.395003\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.409812\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.457660\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.375893\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.368155\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.442602\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.315908\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.387839\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.351710\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.465239\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.328949\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.401901\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.275002\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.415174\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.286077\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.323413\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.294468\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.251878\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.417313\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.273862\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.413353\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.318871\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.311725\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.235007\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.311036\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307652\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.913182\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.911116\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.800284\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.794137\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.782257\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.633094\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.543708\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.576990\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.542342\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.472910\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.591300\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.476888\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.587489\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.414671\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.641219\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.555815\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.388549\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.524505\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.341879\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.494156\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.491776\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.390012\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.315478\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.407592\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.408952\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.527060\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.722542\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.376629\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.446048\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.351857\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.398020\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.454597\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.376398\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.506672\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.356636\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.300214\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.353288\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.422164\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.400420\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.372037\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.649248\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.416585\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.422487\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.533683\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322301\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.517760\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.402113\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.282454\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.445802\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.368992\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.300997\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.293578\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.338468\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.355440\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.299768\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.420479\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.340453\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.401437\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.268306\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.538812\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.431743\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.314451\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.390785\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.187089\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.354784\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.214669\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.336941\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.321289\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.216012\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.284142\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.306993\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.381149\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.270516\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.344332\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.303290\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.342879\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.357392\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.294410\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.324687\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.307827\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.313985\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.253911\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.263536\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.275764\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.287623\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.204706\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.262289\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.262404\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.192362\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.319278\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.692286\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.528336\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.675999\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.654660\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.594618\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.482493\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.363516\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.558104\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.438545\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.420079\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.472011\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.477838\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.308784\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.442707\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.466478\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.325086\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.402363\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.350330\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.295575\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.409874\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.327556\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.453572\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.270364\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.324416\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.410913\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.351751\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.412299\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.314280\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.434646\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.310865\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.438277\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.462453\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.374302\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.217825\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.304330\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.355258\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.371485\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.389541\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.318093\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.371131\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.332385\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.284776\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.346767\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.370928\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.429277\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.365241\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.385964\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.289204\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.277736\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.181632\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.216283\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.287289\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.290931\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.430662\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.160023\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.341076\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.337663\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.187567\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.151801\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316080\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.613661\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.537413\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.407911\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.447471\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.427992\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.461562\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.384563\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.343039\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.276210\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.383432\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.264333\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.248977\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.242037\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.351081\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.338315\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.231091\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.284049\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.353135\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.320321\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.245925\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.258274\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.196160\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.230535\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.250016\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.296536\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.268796\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.275819\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.232557\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.281914\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.309866\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.314720\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.350644\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.270164\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.424753\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.331794\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.343174\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.305057\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.313141\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.195833\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.270340\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.369845\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.223259\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.420734\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.151183\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.418021\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.273093\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.400784\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.269020\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.242101\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.349898\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.233358\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.325925\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.209327\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.245257\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.281627\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.305016\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.327300\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.273297\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.275647\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.237351\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.291602\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.386730\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.195135\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.267335\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.283948\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.217705\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.339030\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.210376\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.171305\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.293222\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.206663\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.172712\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.243405\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.268868\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.227390\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.236568\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.154774\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.155933\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.203871\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.294192\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.280459\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.205797\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.245252\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.212435\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.269121\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.384178\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.328698\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.191927\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.219517\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307771\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.563809\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.415793\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.380722\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.525651\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.420269\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.418000\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.414396\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.365418\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.327700\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.646251\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.375536\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.410328\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.393425\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.423813\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.246047\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.319095\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.318049\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.268568\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.330328\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.582999\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.291900\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.337615\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.304891\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.464955\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.462089\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.410367\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.336455\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.404718\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.250000\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.319208\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.325288\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.346487\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.304192\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.366206\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.364501\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.313483\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.363082\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.381819\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.335614\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.301229\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.301619\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.250828\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.266183\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.258093\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.377319\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.250976\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.386800\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.264738\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.237014\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.186574\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.312343\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.349077\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.252387\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.348876\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.413790\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.161347\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.227664\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.270508\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.345283\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.223234\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.386457\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.288071\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.286189\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.275195\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.249791\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.330734\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.410941\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.349986\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.294101\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.242818\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.234013\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.211581\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.238565\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.381864\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.341586\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.296611\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.232531\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.263397\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.221061\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.364744\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.220826\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.387562\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.304203\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.295299\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.225404\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.429363\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.273540\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.331231\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.367130\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.203860\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.368656\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.308028\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.235508\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.462907\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.398309\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.286578\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.281403\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.383094\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.310378\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.321600\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.300705\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.200980\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.385752\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.478492\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.367956\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.178381\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.262854\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.259058\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.227810\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.344924\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.282298\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.283977\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.440310\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.313154\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.350119\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.366762\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.278466\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.422674\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.298021\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.282351\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.282240\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.330863\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.341957\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.369498\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.416067\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.291012\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.337931\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.418479\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.327662\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.220449\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.376703\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.277900\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.372391\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.447815\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.311749\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.454917\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.329827\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.306960\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.410059\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.421316\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.277732\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.428302\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.358841\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.303987\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.307932\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.344056\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.305217\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.398720\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.352956\n"
     ]
    }
   ],
   "source": [
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model1 = geluLinearModel(\n",
    "        input_size=input_size, \n",
    "        output_size=num_classes, \n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model1.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model1, device, train_loader, optimizer, epoch)\n",
    "    model1.save(f'paper_{datasets_name}/gelu_{model_name}.pth')\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model2 = SigmoidLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model2.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model2, device, train_loader, optimizer, epoch)\n",
    "    model2.save(f'paper_{datasets_name}/sigmoid_{model_name}.pth')\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model3 = tanhLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model3.parameters(), lr=config['lr'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        train_loss, train_accuracy = train(model3, device, train_loader, optimizer, epoch)\n",
    "    model3.save(f'paper_{datasets_name}/tanh_{model_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cdb29",
   "metadata": {},
   "source": [
    "# pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf20c425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 50,890\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%\n",
      "per column magnitude pruning test accuracy: 83.88%, sparsity: 0.3979\n",
      "per row magnitude pruning test accuracy: 84.31%, sparsity: 0.3624\n",
      "per block magnitude pruning test accuracy: 84.75%, sparsity: 0.3983\n",
      "mean column mean pruning test accuracy: 83.21%, sparsity: 0.3790\n",
      "mean row mean pruning test accuracy: 84.02%, sparsity: 0.3983\n",
      "mean block mean pruning test accuracy: 85.12%, sparsity: 0.3983\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 217,354\n",
      "model name: Deep_Narrow, test accuracy: 87.06%\n",
      "per column magnitude pruning test accuracy: 86.53%, sparsity: 0.3636\n",
      "per row magnitude pruning test accuracy: 86.82%, sparsity: 0.3466\n",
      "per block magnitude pruning test accuracy: 86.63%, sparsity: 0.3627\n",
      "mean column mean pruning test accuracy: 83.53%, sparsity: 0.3563\n",
      "mean row mean pruning test accuracy: 86.93%, sparsity: 0.3627\n",
      "mean block mean pruning test accuracy: 86.45%, sparsity: 0.3627\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 535,818\n",
      "model name: Balanced, test accuracy: 88.98%\n",
      "per column magnitude pruning test accuracy: 88.10%, sparsity: 0.3830\n",
      "per row magnitude pruning test accuracy: 88.62%, sparsity: 0.3533\n",
      "per block magnitude pruning test accuracy: 88.51%, sparsity: 0.3828\n",
      "mean column mean pruning test accuracy: 88.45%, sparsity: 0.3740\n",
      "mean row mean pruning test accuracy: 88.09%, sparsity: 0.3828\n",
      "mean block mean pruning test accuracy: 88.76%, sparsity: 0.3828\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 575,050\n",
      "model name: Balanced_Deep, test accuracy: 88.83%\n",
      "per column magnitude pruning test accuracy: 87.80%, sparsity: 0.3920\n",
      "per row magnitude pruning test accuracy: 86.98%, sparsity: 0.3606\n",
      "per block magnitude pruning test accuracy: 87.88%, sparsity: 0.3901\n",
      "mean column mean pruning test accuracy: 88.16%, sparsity: 0.3805\n",
      "mean row mean pruning test accuracy: 88.57%, sparsity: 0.3901\n",
      "mean block mean pruning test accuracy: 88.50%, sparsity: 0.3901\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 3,716,106\n",
      "model name: Wide, test accuracy: 89.35%\n",
      "per column magnitude pruning test accuracy: 89.33%, sparsity: 0.5924\n",
      "per row magnitude pruning test accuracy: 87.52%, sparsity: 0.4641\n",
      "per block magnitude pruning test accuracy: 89.20%, sparsity: 0.5914\n",
      "mean column mean pruning test accuracy: 87.92%, sparsity: 0.4774\n",
      "mean row mean pruning test accuracy: 87.28%, sparsity: 0.5914\n",
      "mean block mean pruning test accuracy: 89.05%, sparsity: 0.5914\n",
      "\n",
      "============================================================\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 14,234,122\n",
      "model name: Very_Wide, test accuracy: 89.23%\n",
      "per column magnitude pruning test accuracy: 88.12%, sparsity: 0.6444\n",
      "per row magnitude pruning test accuracy: 87.07%, sparsity: 0.4892\n",
      "per block magnitude pruning test accuracy: 89.05%, sparsity: 0.6459\n",
      "mean column mean pruning test accuracy: 86.25%, sparsity: 0.5035\n",
      "mean row mean pruning test accuracy: 85.37%, sparsity: 0.6459\n",
      "mean block mean pruning test accuracy: 88.24%, sparsity: 0.6459\n"
     ]
    }
   ],
   "source": [
    "# Results storage\n",
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_{datasets_name}/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    \n",
    "    # magnitude\n",
    "    pc_model, pc_neff = model_pc(model, renormalize=False, beta=1.0, method='magnitude')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pc_1_magnitude\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "    \n",
    "    pr_model, pr_neff = model_pr(model, renormalize=False, beta=1.0, method='magnitude')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pr_1_magnitude\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "    \n",
    "    pb_model, pb_neff = model_block(model, renormalize=False, beta=1.0, method='magnitude')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pb_1_magnitude\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "\n",
    "    # mean\n",
    "    mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=1.0, method='mean')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pc_1_mean\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "\n",
    "    mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=1.0, method='mean')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pr_1_mean\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "\n",
    "    mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=1.0, method='mean')\n",
    "    test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "    result['model_name'].append(f\"{model_name}_pb_1_mean\")\n",
    "    result['test_accuracy'].append(accuracy_mean)\n",
    "    result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "\n",
    "    # summary\n",
    "    print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%\")\n",
    "    print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "    print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "    print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "    print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "    print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "    print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0025c1",
   "metadata": {},
   "source": [
    "# BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26aa1ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 50,890\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 60.83%, sparsity: 0.7785\n",
      "per row magnitude pruning test accuracy: 64.52%, sparsity: 0.7710\n",
      "per block magnitude pruning test accuracy: 62.61%, sparsity: 0.7787\n",
      "mean column mean pruning test accuracy: 69.03%, sparsity: 0.7721\n",
      "mean row mean pruning test accuracy: 62.52%, sparsity: 0.7787\n",
      "mean block mean pruning test accuracy: 65.50%, sparsity: 0.7787\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 51.43%, sparsity: 0.7295\n",
      "per row magnitude pruning test accuracy: 70.54%, sparsity: 0.7181\n",
      "per block magnitude pruning test accuracy: 67.00%, sparsity: 0.7297\n",
      "mean column mean pruning test accuracy: 67.14%, sparsity: 0.7214\n",
      "mean row mean pruning test accuracy: 66.31%, sparsity: 0.7297\n",
      "mean block mean pruning test accuracy: 65.74%, sparsity: 0.7297\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 73.54%, sparsity: 0.6696\n",
      "per row magnitude pruning test accuracy: 68.62%, sparsity: 0.6538\n",
      "per block magnitude pruning test accuracy: 67.27%, sparsity: 0.6698\n",
      "mean column mean pruning test accuracy: 70.60%, sparsity: 0.6595\n",
      "mean row mean pruning test accuracy: 66.85%, sparsity: 0.6698\n",
      "mean block mean pruning test accuracy: 71.72%, sparsity: 0.6698\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 72.50%, sparsity: 0.5964\n",
      "per row magnitude pruning test accuracy: 72.65%, sparsity: 0.5756\n",
      "per block magnitude pruning test accuracy: 78.46%, sparsity: 0.5967\n",
      "mean column mean pruning test accuracy: 77.07%, sparsity: 0.5839\n",
      "mean row mean pruning test accuracy: 76.07%, sparsity: 0.5967\n",
      "mean block mean pruning test accuracy: 81.26%, sparsity: 0.5967\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 81.45%, sparsity: 0.5070\n",
      "per row magnitude pruning test accuracy: 81.67%, sparsity: 0.4797\n",
      "per block magnitude pruning test accuracy: 81.78%, sparsity: 0.5074\n",
      "mean column mean pruning test accuracy: 78.32%, sparsity: 0.4917\n",
      "mean row mean pruning test accuracy: 79.70%, sparsity: 0.5074\n",
      "mean block mean pruning test accuracy: 84.16%, sparsity: 0.5074\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 83.88%, sparsity: 0.3979\n",
      "per row magnitude pruning test accuracy: 84.31%, sparsity: 0.3624\n",
      "per block magnitude pruning test accuracy: 84.75%, sparsity: 0.3983\n",
      "mean column mean pruning test accuracy: 83.21%, sparsity: 0.3790\n",
      "mean row mean pruning test accuracy: 84.02%, sparsity: 0.3983\n",
      "mean block mean pruning test accuracy: 85.12%, sparsity: 0.3983\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 85.59%, sparsity: 0.2646\n",
      "per row magnitude pruning test accuracy: 86.12%, sparsity: 0.2196\n",
      "per block magnitude pruning test accuracy: 85.52%, sparsity: 0.2651\n",
      "mean column mean pruning test accuracy: 85.03%, sparsity: 0.2414\n",
      "mean row mean pruning test accuracy: 85.16%, sparsity: 0.2651\n",
      "mean block mean pruning test accuracy: 86.28%, sparsity: 0.2651\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 85.56%, sparsity: 0.1017\n",
      "per row magnitude pruning test accuracy: 86.36%, sparsity: 0.0598\n",
      "per block magnitude pruning test accuracy: 86.23%, sparsity: 0.1025\n",
      "mean column mean pruning test accuracy: 85.45%, sparsity: 0.0795\n",
      "mean row mean pruning test accuracy: 86.32%, sparsity: 0.1025\n",
      "mean block mean pruning test accuracy: 86.28%, sparsity: 0.1025\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 85.50%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 86.28%, sparsity: 0.0174\n",
      "per block magnitude pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 85.54%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 86.22%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 85.57%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 86.28%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 85.63%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.24%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 85.57%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 86.28%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 85.63%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 86.24%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 217,354\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 22.36%, sparsity: 0.7659\n",
      "per row magnitude pruning test accuracy: 19.18%, sparsity: 0.7624\n",
      "per block magnitude pruning test accuracy: 27.68%, sparsity: 0.7656\n",
      "mean column mean pruning test accuracy: 44.46%, sparsity: 0.7647\n",
      "mean row mean pruning test accuracy: 40.87%, sparsity: 0.7656\n",
      "mean block mean pruning test accuracy: 55.16%, sparsity: 0.7656\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 43.89%, sparsity: 0.7140\n",
      "per row magnitude pruning test accuracy: 50.66%, sparsity: 0.7087\n",
      "per block magnitude pruning test accuracy: 54.45%, sparsity: 0.7136\n",
      "mean column mean pruning test accuracy: 80.48%, sparsity: 0.7120\n",
      "mean row mean pruning test accuracy: 73.41%, sparsity: 0.7136\n",
      "mean block mean pruning test accuracy: 78.11%, sparsity: 0.7136\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 74.76%, sparsity: 0.6507\n",
      "per row magnitude pruning test accuracy: 73.86%, sparsity: 0.6433\n",
      "per block magnitude pruning test accuracy: 75.71%, sparsity: 0.6502\n",
      "mean column mean pruning test accuracy: 80.99%, sparsity: 0.6478\n",
      "mean row mean pruning test accuracy: 82.82%, sparsity: 0.6502\n",
      "mean block mean pruning test accuracy: 84.70%, sparsity: 0.6502\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 83.57%, sparsity: 0.5734\n",
      "per row magnitude pruning test accuracy: 81.30%, sparsity: 0.5634\n",
      "per block magnitude pruning test accuracy: 83.71%, sparsity: 0.5728\n",
      "mean column mean pruning test accuracy: 81.45%, sparsity: 0.5692\n",
      "mean row mean pruning test accuracy: 85.72%, sparsity: 0.5728\n",
      "mean block mean pruning test accuracy: 86.10%, sparsity: 0.5728\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 86.41%, sparsity: 0.4789\n",
      "per row magnitude pruning test accuracy: 86.39%, sparsity: 0.4659\n",
      "per block magnitude pruning test accuracy: 86.28%, sparsity: 0.4782\n",
      "mean column mean pruning test accuracy: 82.40%, sparsity: 0.4734\n",
      "mean row mean pruning test accuracy: 86.47%, sparsity: 0.4782\n",
      "mean block mean pruning test accuracy: 86.32%, sparsity: 0.4782\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 86.53%, sparsity: 0.3636\n",
      "per row magnitude pruning test accuracy: 86.82%, sparsity: 0.3466\n",
      "per block magnitude pruning test accuracy: 86.63%, sparsity: 0.3627\n",
      "mean column mean pruning test accuracy: 83.53%, sparsity: 0.3563\n",
      "mean row mean pruning test accuracy: 86.93%, sparsity: 0.3627\n",
      "mean block mean pruning test accuracy: 86.45%, sparsity: 0.3627\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 87.03%, sparsity: 0.2227\n",
      "per row magnitude pruning test accuracy: 87.13%, sparsity: 0.2012\n",
      "per block magnitude pruning test accuracy: 86.97%, sparsity: 0.2216\n",
      "mean column mean pruning test accuracy: 85.73%, sparsity: 0.2132\n",
      "mean row mean pruning test accuracy: 87.03%, sparsity: 0.2216\n",
      "mean block mean pruning test accuracy: 87.10%, sparsity: 0.2216\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 87.16%, sparsity: 0.0509\n",
      "per row magnitude pruning test accuracy: 87.08%, sparsity: 0.0403\n",
      "per block magnitude pruning test accuracy: 87.14%, sparsity: 0.0496\n",
      "mean column mean pruning test accuracy: 87.06%, sparsity: 0.0519\n",
      "mean row mean pruning test accuracy: 87.11%, sparsity: 0.0496\n",
      "mean block mean pruning test accuracy: 87.16%, sparsity: 0.0496\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 87.12%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 87.08%, sparsity: 0.0086\n",
      "per block magnitude pruning test accuracy: 87.06%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 87.10%, sparsity: 0.0053\n",
      "mean row mean pruning test accuracy: 87.13%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 87.07%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 87.11%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 87.06%, sparsity: 0.0084\n",
      "per block magnitude pruning test accuracy: 87.06%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 87.07%, sparsity: 0.0048\n",
      "mean row mean pruning test accuracy: 87.13%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 87.07%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.06%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 87.11%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 87.06%, sparsity: 0.0084\n",
      "per block magnitude pruning test accuracy: 87.06%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 87.07%, sparsity: 0.0048\n",
      "mean row mean pruning test accuracy: 87.13%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 87.07%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 535,818\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 82.18%, sparsity: 0.7730\n",
      "per row magnitude pruning test accuracy: 78.64%, sparsity: 0.7630\n",
      "per block magnitude pruning test accuracy: 82.12%, sparsity: 0.7729\n",
      "mean column mean pruning test accuracy: 79.71%, sparsity: 0.7702\n",
      "mean row mean pruning test accuracy: 84.24%, sparsity: 0.7729\n",
      "mean block mean pruning test accuracy: 80.46%, sparsity: 0.7729\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 85.42%, sparsity: 0.7227\n",
      "per row magnitude pruning test accuracy: 84.94%, sparsity: 0.7102\n",
      "per block magnitude pruning test accuracy: 84.72%, sparsity: 0.7227\n",
      "mean column mean pruning test accuracy: 82.67%, sparsity: 0.7191\n",
      "mean row mean pruning test accuracy: 86.22%, sparsity: 0.7227\n",
      "mean block mean pruning test accuracy: 86.31%, sparsity: 0.7227\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 87.13%, sparsity: 0.6614\n",
      "per row magnitude pruning test accuracy: 87.13%, sparsity: 0.6457\n",
      "per block magnitude pruning test accuracy: 87.45%, sparsity: 0.6613\n",
      "mean column mean pruning test accuracy: 85.21%, sparsity: 0.6568\n",
      "mean row mean pruning test accuracy: 85.92%, sparsity: 0.6613\n",
      "mean block mean pruning test accuracy: 87.94%, sparsity: 0.6613\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 88.00%, sparsity: 0.5864\n",
      "per row magnitude pruning test accuracy: 88.49%, sparsity: 0.5670\n",
      "per block magnitude pruning test accuracy: 88.22%, sparsity: 0.5863\n",
      "mean column mean pruning test accuracy: 85.57%, sparsity: 0.5806\n",
      "mean row mean pruning test accuracy: 86.70%, sparsity: 0.5863\n",
      "mean block mean pruning test accuracy: 88.19%, sparsity: 0.5863\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 87.68%, sparsity: 0.4948\n",
      "per row magnitude pruning test accuracy: 88.56%, sparsity: 0.4708\n",
      "per block magnitude pruning test accuracy: 88.60%, sparsity: 0.4947\n",
      "mean column mean pruning test accuracy: 87.87%, sparsity: 0.4876\n",
      "mean row mean pruning test accuracy: 87.39%, sparsity: 0.4947\n",
      "mean block mean pruning test accuracy: 88.47%, sparsity: 0.4947\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 88.10%, sparsity: 0.3830\n",
      "per row magnitude pruning test accuracy: 88.62%, sparsity: 0.3533\n",
      "per block magnitude pruning test accuracy: 88.51%, sparsity: 0.3828\n",
      "mean column mean pruning test accuracy: 88.45%, sparsity: 0.3740\n",
      "mean row mean pruning test accuracy: 88.09%, sparsity: 0.3828\n",
      "mean block mean pruning test accuracy: 88.76%, sparsity: 0.3828\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 88.83%, sparsity: 0.2463\n",
      "per row magnitude pruning test accuracy: 88.95%, sparsity: 0.2099\n",
      "per block magnitude pruning test accuracy: 88.83%, sparsity: 0.2461\n",
      "mean column mean pruning test accuracy: 88.73%, sparsity: 0.2352\n",
      "mean row mean pruning test accuracy: 88.78%, sparsity: 0.2461\n",
      "mean block mean pruning test accuracy: 88.79%, sparsity: 0.2461\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 88.92%, sparsity: 0.0795\n",
      "per row magnitude pruning test accuracy: 88.87%, sparsity: 0.0426\n",
      "per block magnitude pruning test accuracy: 88.95%, sparsity: 0.0795\n",
      "mean column mean pruning test accuracy: 88.81%, sparsity: 0.0683\n",
      "mean row mean pruning test accuracy: 88.90%, sparsity: 0.0795\n",
      "mean block mean pruning test accuracy: 88.94%, sparsity: 0.0795\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 88.88%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 88.97%, sparsity: 0.0029\n",
      "per block magnitude pruning test accuracy: 88.98%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 88.88%, sparsity: 0.0019\n",
      "mean row mean pruning test accuracy: 88.99%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 88.98%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 88.89%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 88.97%, sparsity: 0.0029\n",
      "per block magnitude pruning test accuracy: 88.98%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 88.88%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 88.99%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 88.98%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 88.98%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 88.89%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 88.97%, sparsity: 0.0029\n",
      "per block magnitude pruning test accuracy: 88.98%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 88.88%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 88.99%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 88.98%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 575,050\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 69.50%, sparsity: 0.7763\n",
      "per row magnitude pruning test accuracy: 65.09%, sparsity: 0.7658\n",
      "per block magnitude pruning test accuracy: 58.69%, sparsity: 0.7756\n",
      "mean column mean pruning test accuracy: 70.98%, sparsity: 0.7726\n",
      "mean row mean pruning test accuracy: 66.72%, sparsity: 0.7756\n",
      "mean block mean pruning test accuracy: 63.76%, sparsity: 0.7756\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 73.60%, sparsity: 0.7268\n",
      "per row magnitude pruning test accuracy: 71.62%, sparsity: 0.7135\n",
      "per block magnitude pruning test accuracy: 81.48%, sparsity: 0.7260\n",
      "mean column mean pruning test accuracy: 77.98%, sparsity: 0.7221\n",
      "mean row mean pruning test accuracy: 79.00%, sparsity: 0.7260\n",
      "mean block mean pruning test accuracy: 77.09%, sparsity: 0.7260\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 82.84%, sparsity: 0.6663\n",
      "per row magnitude pruning test accuracy: 82.94%, sparsity: 0.6498\n",
      "per block magnitude pruning test accuracy: 86.48%, sparsity: 0.6653\n",
      "mean column mean pruning test accuracy: 83.52%, sparsity: 0.6604\n",
      "mean row mean pruning test accuracy: 83.54%, sparsity: 0.6653\n",
      "mean block mean pruning test accuracy: 81.92%, sparsity: 0.6653\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 85.91%, sparsity: 0.5925\n",
      "per row magnitude pruning test accuracy: 86.80%, sparsity: 0.5719\n",
      "per block magnitude pruning test accuracy: 85.11%, sparsity: 0.5912\n",
      "mean column mean pruning test accuracy: 79.71%, sparsity: 0.5850\n",
      "mean row mean pruning test accuracy: 86.80%, sparsity: 0.5912\n",
      "mean block mean pruning test accuracy: 86.69%, sparsity: 0.5912\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 86.41%, sparsity: 0.5023\n",
      "per row magnitude pruning test accuracy: 85.08%, sparsity: 0.4768\n",
      "per block magnitude pruning test accuracy: 87.92%, sparsity: 0.5007\n",
      "mean column mean pruning test accuracy: 86.08%, sparsity: 0.4929\n",
      "mean row mean pruning test accuracy: 87.80%, sparsity: 0.5007\n",
      "mean block mean pruning test accuracy: 87.51%, sparsity: 0.5007\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 87.80%, sparsity: 0.3920\n",
      "per row magnitude pruning test accuracy: 86.98%, sparsity: 0.3606\n",
      "per block magnitude pruning test accuracy: 87.88%, sparsity: 0.3901\n",
      "mean column mean pruning test accuracy: 88.16%, sparsity: 0.3805\n",
      "mean row mean pruning test accuracy: 88.57%, sparsity: 0.3901\n",
      "mean block mean pruning test accuracy: 88.50%, sparsity: 0.3901\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 88.64%, sparsity: 0.2574\n",
      "per row magnitude pruning test accuracy: 88.64%, sparsity: 0.2188\n",
      "per block magnitude pruning test accuracy: 88.62%, sparsity: 0.2551\n",
      "mean column mean pruning test accuracy: 88.63%, sparsity: 0.2431\n",
      "mean row mean pruning test accuracy: 88.83%, sparsity: 0.2551\n",
      "mean block mean pruning test accuracy: 88.61%, sparsity: 0.2551\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 88.53%, sparsity: 0.0930\n",
      "per row magnitude pruning test accuracy: 88.76%, sparsity: 0.0510\n",
      "per block magnitude pruning test accuracy: 88.79%, sparsity: 0.0902\n",
      "mean column mean pruning test accuracy: 88.76%, sparsity: 0.0777\n",
      "mean row mean pruning test accuracy: 88.98%, sparsity: 0.0902\n",
      "mean block mean pruning test accuracy: 88.80%, sparsity: 0.0902\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 88.80%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 88.84%, sparsity: 0.0031\n",
      "per block magnitude pruning test accuracy: 88.83%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 88.85%, sparsity: 0.0026\n",
      "mean row mean pruning test accuracy: 88.83%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 88.84%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 88.87%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 88.84%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 88.83%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 88.85%, sparsity: 0.0017\n",
      "mean row mean pruning test accuracy: 88.83%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 88.84%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 88.83%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 88.87%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 88.84%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 88.83%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 88.85%, sparsity: 0.0017\n",
      "mean row mean pruning test accuracy: 88.83%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 88.84%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 3,716,106\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 77.83%, sparsity: 0.8501\n",
      "per row magnitude pruning test accuracy: 76.46%, sparsity: 0.8032\n",
      "per block magnitude pruning test accuracy: 80.85%, sparsity: 0.8497\n",
      "mean column mean pruning test accuracy: 72.95%, sparsity: 0.8080\n",
      "mean row mean pruning test accuracy: 72.12%, sparsity: 0.8497\n",
      "mean block mean pruning test accuracy: 80.93%, sparsity: 0.8497\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 82.29%, sparsity: 0.8169\n",
      "per row magnitude pruning test accuracy: 78.61%, sparsity: 0.7595\n",
      "per block magnitude pruning test accuracy: 84.84%, sparsity: 0.8164\n",
      "mean column mean pruning test accuracy: 72.45%, sparsity: 0.7654\n",
      "mean row mean pruning test accuracy: 78.15%, sparsity: 0.8164\n",
      "mean block mean pruning test accuracy: 83.24%, sparsity: 0.8164\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 85.97%, sparsity: 0.7763\n",
      "per row magnitude pruning test accuracy: 80.83%, sparsity: 0.7061\n",
      "per block magnitude pruning test accuracy: 86.78%, sparsity: 0.7758\n",
      "mean column mean pruning test accuracy: 80.91%, sparsity: 0.7134\n",
      "mean row mean pruning test accuracy: 79.47%, sparsity: 0.7758\n",
      "mean block mean pruning test accuracy: 85.31%, sparsity: 0.7758\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 86.81%, sparsity: 0.7268\n",
      "per row magnitude pruning test accuracy: 82.68%, sparsity: 0.6410\n",
      "per block magnitude pruning test accuracy: 88.08%, sparsity: 0.7261\n",
      "mean column mean pruning test accuracy: 84.61%, sparsity: 0.6498\n",
      "mean row mean pruning test accuracy: 82.71%, sparsity: 0.7261\n",
      "mean block mean pruning test accuracy: 87.09%, sparsity: 0.7261\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 88.61%, sparsity: 0.6663\n",
      "per row magnitude pruning test accuracy: 85.06%, sparsity: 0.5613\n",
      "per block magnitude pruning test accuracy: 88.71%, sparsity: 0.6655\n",
      "mean column mean pruning test accuracy: 86.42%, sparsity: 0.5722\n",
      "mean row mean pruning test accuracy: 84.74%, sparsity: 0.6655\n",
      "mean block mean pruning test accuracy: 88.27%, sparsity: 0.6655\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 89.33%, sparsity: 0.5924\n",
      "per row magnitude pruning test accuracy: 87.52%, sparsity: 0.4641\n",
      "per block magnitude pruning test accuracy: 89.20%, sparsity: 0.5914\n",
      "mean column mean pruning test accuracy: 87.92%, sparsity: 0.4774\n",
      "mean row mean pruning test accuracy: 87.28%, sparsity: 0.5914\n",
      "mean block mean pruning test accuracy: 89.05%, sparsity: 0.5914\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 88.91%, sparsity: 0.5022\n",
      "per row magnitude pruning test accuracy: 89.15%, sparsity: 0.3454\n",
      "per block magnitude pruning test accuracy: 89.25%, sparsity: 0.5009\n",
      "mean column mean pruning test accuracy: 88.89%, sparsity: 0.3616\n",
      "mean row mean pruning test accuracy: 88.51%, sparsity: 0.5009\n",
      "mean block mean pruning test accuracy: 89.26%, sparsity: 0.5009\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 88.84%, sparsity: 0.3919\n",
      "per row magnitude pruning test accuracy: 89.50%, sparsity: 0.2036\n",
      "per block magnitude pruning test accuracy: 89.46%, sparsity: 0.3904\n",
      "mean column mean pruning test accuracy: 89.36%, sparsity: 0.2260\n",
      "mean row mean pruning test accuracy: 88.88%, sparsity: 0.3904\n",
      "mean block mean pruning test accuracy: 89.16%, sparsity: 0.3904\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 89.06%, sparsity: 0.2573\n",
      "per row magnitude pruning test accuracy: 89.21%, sparsity: 0.0812\n",
      "per block magnitude pruning test accuracy: 89.27%, sparsity: 0.2555\n",
      "mean column mean pruning test accuracy: 89.28%, sparsity: 0.1276\n",
      "mean row mean pruning test accuracy: 89.16%, sparsity: 0.2555\n",
      "mean block mean pruning test accuracy: 89.28%, sparsity: 0.2555\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 89.14%, sparsity: 0.0931\n",
      "per row magnitude pruning test accuracy: 89.34%, sparsity: 0.0202\n",
      "per block magnitude pruning test accuracy: 89.35%, sparsity: 0.0913\n",
      "mean column mean pruning test accuracy: 89.19%, sparsity: 0.0450\n",
      "mean row mean pruning test accuracy: 89.25%, sparsity: 0.0913\n",
      "mean block mean pruning test accuracy: 89.33%, sparsity: 0.0913\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.35%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 89.18%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.34%, sparsity: 0.0013\n",
      "per block magnitude pruning test accuracy: 89.35%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.21%, sparsity: 0.0019\n",
      "mean row mean pruning test accuracy: 89.32%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.35%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 14,234,122\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 61.39%, sparsity: 0.8692\n",
      "per row magnitude pruning test accuracy: 69.51%, sparsity: 0.8123\n",
      "per block magnitude pruning test accuracy: 73.96%, sparsity: 0.8697\n",
      "mean column mean pruning test accuracy: 52.00%, sparsity: 0.8175\n",
      "mean row mean pruning test accuracy: 79.92%, sparsity: 0.8697\n",
      "mean block mean pruning test accuracy: 82.06%, sparsity: 0.8697\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 60.34%, sparsity: 0.8402\n",
      "per row magnitude pruning test accuracy: 71.34%, sparsity: 0.7707\n",
      "per block magnitude pruning test accuracy: 76.23%, sparsity: 0.8409\n",
      "mean column mean pruning test accuracy: 65.96%, sparsity: 0.7771\n",
      "mean row mean pruning test accuracy: 81.34%, sparsity: 0.8409\n",
      "mean block mean pruning test accuracy: 83.54%, sparsity: 0.8409\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 59.23%, sparsity: 0.8048\n",
      "per row magnitude pruning test accuracy: 74.76%, sparsity: 0.7198\n",
      "per block magnitude pruning test accuracy: 78.43%, sparsity: 0.8057\n",
      "mean column mean pruning test accuracy: 78.12%, sparsity: 0.7277\n",
      "mean row mean pruning test accuracy: 82.21%, sparsity: 0.8057\n",
      "mean block mean pruning test accuracy: 84.99%, sparsity: 0.8057\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 69.60%, sparsity: 0.7616\n",
      "per row magnitude pruning test accuracy: 75.06%, sparsity: 0.6577\n",
      "per block magnitude pruning test accuracy: 79.46%, sparsity: 0.7626\n",
      "mean column mean pruning test accuracy: 82.16%, sparsity: 0.6673\n",
      "mean row mean pruning test accuracy: 83.08%, sparsity: 0.7626\n",
      "mean block mean pruning test accuracy: 86.35%, sparsity: 0.7626\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 78.00%, sparsity: 0.7088\n",
      "per row magnitude pruning test accuracy: 84.05%, sparsity: 0.5819\n",
      "per block magnitude pruning test accuracy: 89.15%, sparsity: 0.7101\n",
      "mean column mean pruning test accuracy: 84.84%, sparsity: 0.5936\n",
      "mean row mean pruning test accuracy: 84.23%, sparsity: 0.7101\n",
      "mean block mean pruning test accuracy: 87.40%, sparsity: 0.7101\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 88.12%, sparsity: 0.6444\n",
      "per row magnitude pruning test accuracy: 87.07%, sparsity: 0.4892\n",
      "per block magnitude pruning test accuracy: 89.05%, sparsity: 0.6459\n",
      "mean column mean pruning test accuracy: 86.25%, sparsity: 0.5035\n",
      "mean row mean pruning test accuracy: 85.37%, sparsity: 0.6459\n",
      "mean block mean pruning test accuracy: 88.24%, sparsity: 0.6459\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 88.97%, sparsity: 0.5656\n",
      "per row magnitude pruning test accuracy: 88.66%, sparsity: 0.3761\n",
      "per block magnitude pruning test accuracy: 89.14%, sparsity: 0.5675\n",
      "mean column mean pruning test accuracy: 87.84%, sparsity: 0.3936\n",
      "mean row mean pruning test accuracy: 87.47%, sparsity: 0.5675\n",
      "mean block mean pruning test accuracy: 88.90%, sparsity: 0.5675\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 88.99%, sparsity: 0.4695\n",
      "per row magnitude pruning test accuracy: 89.20%, sparsity: 0.2399\n",
      "per block magnitude pruning test accuracy: 89.10%, sparsity: 0.4717\n",
      "mean column mean pruning test accuracy: 89.03%, sparsity: 0.2638\n",
      "mean row mean pruning test accuracy: 88.49%, sparsity: 0.4717\n",
      "mean block mean pruning test accuracy: 88.94%, sparsity: 0.4717\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 89.03%, sparsity: 0.3520\n",
      "per row magnitude pruning test accuracy: 89.21%, sparsity: 0.1045\n",
      "per block magnitude pruning test accuracy: 89.20%, sparsity: 0.3548\n",
      "mean column mean pruning test accuracy: 89.11%, sparsity: 0.1622\n",
      "mean row mean pruning test accuracy: 89.07%, sparsity: 0.3548\n",
      "mean block mean pruning test accuracy: 89.13%, sparsity: 0.3548\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 89.09%, sparsity: 0.2112\n",
      "per row magnitude pruning test accuracy: 89.23%, sparsity: 0.0226\n",
      "per block magnitude pruning test accuracy: 89.24%, sparsity: 0.2142\n",
      "mean column mean pruning test accuracy: 89.15%, sparsity: 0.0817\n",
      "mean row mean pruning test accuracy: 89.23%, sparsity: 0.2142\n",
      "mean block mean pruning test accuracy: 89.21%, sparsity: 0.2142\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.23%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 89.09%, sparsity: 0.1023\n",
      "per row magnitude pruning test accuracy: 89.23%, sparsity: 0.0019\n",
      "per block magnitude pruning test accuracy: 89.23%, sparsity: 0.1047\n",
      "mean column mean pruning test accuracy: 89.18%, sparsity: 0.0254\n",
      "mean row mean pruning test accuracy: 89.24%, sparsity: 0.1047\n",
      "mean block mean pruning test accuracy: 89.23%, sparsity: 0.1047\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = LinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_{datasets_name}/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c887d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 50,890\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 46.37%, sparsity: 0.7753\n",
      "per row magnitude pruning test accuracy: 61.09%, sparsity: 0.7682\n",
      "per block magnitude pruning test accuracy: 67.97%, sparsity: 0.7754\n",
      "mean column mean pruning test accuracy: 61.98%, sparsity: 0.7738\n",
      "mean row mean pruning test accuracy: 62.11%, sparsity: 0.7754\n",
      "mean block mean pruning test accuracy: 68.44%, sparsity: 0.7754\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 53.25%, sparsity: 0.7256\n",
      "per row magnitude pruning test accuracy: 68.89%, sparsity: 0.7148\n",
      "per block magnitude pruning test accuracy: 75.25%, sparsity: 0.7257\n",
      "mean column mean pruning test accuracy: 75.06%, sparsity: 0.7236\n",
      "mean row mean pruning test accuracy: 75.34%, sparsity: 0.7257\n",
      "mean block mean pruning test accuracy: 72.35%, sparsity: 0.7257\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 74.64%, sparsity: 0.6648\n",
      "per row magnitude pruning test accuracy: 76.81%, sparsity: 0.6498\n",
      "per block magnitude pruning test accuracy: 79.49%, sparsity: 0.6650\n",
      "mean column mean pruning test accuracy: 80.18%, sparsity: 0.6623\n",
      "mean row mean pruning test accuracy: 78.73%, sparsity: 0.6650\n",
      "mean block mean pruning test accuracy: 79.09%, sparsity: 0.6650\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 76.99%, sparsity: 0.5906\n",
      "per row magnitude pruning test accuracy: 82.45%, sparsity: 0.5708\n",
      "per block magnitude pruning test accuracy: 82.41%, sparsity: 0.5908\n",
      "mean column mean pruning test accuracy: 81.37%, sparsity: 0.5874\n",
      "mean row mean pruning test accuracy: 80.38%, sparsity: 0.5908\n",
      "mean block mean pruning test accuracy: 82.50%, sparsity: 0.5908\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 79.36%, sparsity: 0.4999\n",
      "per row magnitude pruning test accuracy: 82.26%, sparsity: 0.4737\n",
      "per block magnitude pruning test accuracy: 83.00%, sparsity: 0.5002\n",
      "mean column mean pruning test accuracy: 85.15%, sparsity: 0.4958\n",
      "mean row mean pruning test accuracy: 81.66%, sparsity: 0.5002\n",
      "mean block mean pruning test accuracy: 82.14%, sparsity: 0.5002\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 84.30%, sparsity: 0.3892\n",
      "per row magnitude pruning test accuracy: 84.54%, sparsity: 0.3550\n",
      "per block magnitude pruning test accuracy: 84.00%, sparsity: 0.3896\n",
      "mean column mean pruning test accuracy: 84.52%, sparsity: 0.3839\n",
      "mean row mean pruning test accuracy: 82.51%, sparsity: 0.3896\n",
      "mean block mean pruning test accuracy: 84.40%, sparsity: 0.3896\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 86.00%, sparsity: 0.2540\n",
      "per row magnitude pruning test accuracy: 85.24%, sparsity: 0.2107\n",
      "per block magnitude pruning test accuracy: 86.25%, sparsity: 0.2544\n",
      "mean column mean pruning test accuracy: 85.39%, sparsity: 0.2476\n",
      "mean row mean pruning test accuracy: 86.32%, sparsity: 0.2544\n",
      "mean block mean pruning test accuracy: 86.20%, sparsity: 0.2544\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 85.99%, sparsity: 0.0889\n",
      "per row magnitude pruning test accuracy: 86.64%, sparsity: 0.0518\n",
      "per block magnitude pruning test accuracy: 86.49%, sparsity: 0.0898\n",
      "mean column mean pruning test accuracy: 85.61%, sparsity: 0.0815\n",
      "mean row mean pruning test accuracy: 86.61%, sparsity: 0.0898\n",
      "mean block mean pruning test accuracy: 86.51%, sparsity: 0.0898\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 86.41%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 86.53%, sparsity: 0.0169\n",
      "per block magnitude pruning test accuracy: 86.49%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 85.60%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 86.48%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 86.48%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 86.33%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 86.53%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 86.49%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 85.92%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 86.48%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 86.48%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 86.49%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 86.36%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 86.53%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 86.49%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 85.92%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 86.48%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 86.48%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 217,354\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 10.15%, sparsity: 0.7669\n",
      "per row magnitude pruning test accuracy: 26.24%, sparsity: 0.7618\n",
      "per block magnitude pruning test accuracy: 20.85%, sparsity: 0.7661\n",
      "mean column mean pruning test accuracy: 20.00%, sparsity: 0.7650\n",
      "mean row mean pruning test accuracy: 37.44%, sparsity: 0.7661\n",
      "mean block mean pruning test accuracy: 38.12%, sparsity: 0.7661\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 29.75%, sparsity: 0.7153\n",
      "per row magnitude pruning test accuracy: 42.57%, sparsity: 0.7081\n",
      "per block magnitude pruning test accuracy: 36.68%, sparsity: 0.7142\n",
      "mean column mean pruning test accuracy: 68.72%, sparsity: 0.7125\n",
      "mean row mean pruning test accuracy: 50.32%, sparsity: 0.7142\n",
      "mean block mean pruning test accuracy: 68.48%, sparsity: 0.7142\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 58.36%, sparsity: 0.6523\n",
      "per row magnitude pruning test accuracy: 79.50%, sparsity: 0.6426\n",
      "per block magnitude pruning test accuracy: 73.81%, sparsity: 0.6510\n",
      "mean column mean pruning test accuracy: 83.46%, sparsity: 0.6483\n",
      "mean row mean pruning test accuracy: 64.90%, sparsity: 0.6510\n",
      "mean block mean pruning test accuracy: 84.86%, sparsity: 0.6510\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 83.09%, sparsity: 0.5753\n",
      "per row magnitude pruning test accuracy: 80.53%, sparsity: 0.5624\n",
      "per block magnitude pruning test accuracy: 84.10%, sparsity: 0.5737\n",
      "mean column mean pruning test accuracy: 82.98%, sparsity: 0.5699\n",
      "mean row mean pruning test accuracy: 71.07%, sparsity: 0.5737\n",
      "mean block mean pruning test accuracy: 87.37%, sparsity: 0.5737\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 87.68%, sparsity: 0.4813\n",
      "per row magnitude pruning test accuracy: 87.24%, sparsity: 0.4648\n",
      "per block magnitude pruning test accuracy: 87.11%, sparsity: 0.4793\n",
      "mean column mean pruning test accuracy: 82.99%, sparsity: 0.4742\n",
      "mean row mean pruning test accuracy: 87.13%, sparsity: 0.4793\n",
      "mean block mean pruning test accuracy: 87.45%, sparsity: 0.4793\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 87.74%, sparsity: 0.3664\n",
      "per row magnitude pruning test accuracy: 87.73%, sparsity: 0.3453\n",
      "per block magnitude pruning test accuracy: 87.85%, sparsity: 0.3640\n",
      "mean column mean pruning test accuracy: 84.10%, sparsity: 0.3571\n",
      "mean row mean pruning test accuracy: 87.82%, sparsity: 0.3640\n",
      "mean block mean pruning test accuracy: 87.29%, sparsity: 0.3640\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 87.58%, sparsity: 0.2261\n",
      "per row magnitude pruning test accuracy: 87.81%, sparsity: 0.1996\n",
      "per block magnitude pruning test accuracy: 87.84%, sparsity: 0.2232\n",
      "mean column mean pruning test accuracy: 85.86%, sparsity: 0.2144\n",
      "mean row mean pruning test accuracy: 87.81%, sparsity: 0.2232\n",
      "mean block mean pruning test accuracy: 87.47%, sparsity: 0.2232\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 87.84%, sparsity: 0.0568\n",
      "per row magnitude pruning test accuracy: 87.98%, sparsity: 0.0386\n",
      "per block magnitude pruning test accuracy: 87.94%, sparsity: 0.0535\n",
      "mean column mean pruning test accuracy: 87.46%, sparsity: 0.0528\n",
      "mean row mean pruning test accuracy: 87.94%, sparsity: 0.0535\n",
      "mean block mean pruning test accuracy: 88.02%, sparsity: 0.0535\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 87.90%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 87.97%, sparsity: 0.0085\n",
      "per block magnitude pruning test accuracy: 87.97%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 87.94%, sparsity: 0.0059\n",
      "mean row mean pruning test accuracy: 87.99%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 87.97%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 87.91%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 87.97%, sparsity: 0.0084\n",
      "per block magnitude pruning test accuracy: 87.97%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 87.95%, sparsity: 0.0048\n",
      "mean row mean pruning test accuracy: 87.99%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 87.97%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 87.97%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 87.91%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 87.97%, sparsity: 0.0084\n",
      "per block magnitude pruning test accuracy: 87.97%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 87.95%, sparsity: 0.0048\n",
      "mean row mean pruning test accuracy: 87.99%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 87.97%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 535,818\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 83.28%, sparsity: 0.7739\n",
      "per row magnitude pruning test accuracy: 82.32%, sparsity: 0.7631\n",
      "per block magnitude pruning test accuracy: 83.29%, sparsity: 0.7736\n",
      "mean column mean pruning test accuracy: 78.19%, sparsity: 0.7719\n",
      "mean row mean pruning test accuracy: 84.36%, sparsity: 0.7736\n",
      "mean block mean pruning test accuracy: 80.89%, sparsity: 0.7736\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 85.19%, sparsity: 0.7239\n",
      "per row magnitude pruning test accuracy: 85.75%, sparsity: 0.7103\n",
      "per block magnitude pruning test accuracy: 86.66%, sparsity: 0.7235\n",
      "mean column mean pruning test accuracy: 82.35%, sparsity: 0.7213\n",
      "mean row mean pruning test accuracy: 86.68%, sparsity: 0.7235\n",
      "mean block mean pruning test accuracy: 85.25%, sparsity: 0.7235\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 86.58%, sparsity: 0.6627\n",
      "per row magnitude pruning test accuracy: 87.21%, sparsity: 0.6458\n",
      "per block magnitude pruning test accuracy: 87.46%, sparsity: 0.6623\n",
      "mean column mean pruning test accuracy: 86.12%, sparsity: 0.6594\n",
      "mean row mean pruning test accuracy: 87.59%, sparsity: 0.6623\n",
      "mean block mean pruning test accuracy: 87.03%, sparsity: 0.6623\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 87.87%, sparsity: 0.5881\n",
      "per row magnitude pruning test accuracy: 88.16%, sparsity: 0.5671\n",
      "per block magnitude pruning test accuracy: 88.42%, sparsity: 0.5875\n",
      "mean column mean pruning test accuracy: 87.30%, sparsity: 0.5838\n",
      "mean row mean pruning test accuracy: 88.55%, sparsity: 0.5875\n",
      "mean block mean pruning test accuracy: 88.22%, sparsity: 0.5875\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 88.68%, sparsity: 0.4969\n",
      "per row magnitude pruning test accuracy: 88.75%, sparsity: 0.4709\n",
      "per block magnitude pruning test accuracy: 88.67%, sparsity: 0.4962\n",
      "mean column mean pruning test accuracy: 88.40%, sparsity: 0.4916\n",
      "mean row mean pruning test accuracy: 89.04%, sparsity: 0.4962\n",
      "mean block mean pruning test accuracy: 88.58%, sparsity: 0.4962\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 89.16%, sparsity: 0.3855\n",
      "per row magnitude pruning test accuracy: 89.14%, sparsity: 0.3535\n",
      "per block magnitude pruning test accuracy: 89.20%, sparsity: 0.3847\n",
      "mean column mean pruning test accuracy: 89.03%, sparsity: 0.3788\n",
      "mean row mean pruning test accuracy: 89.29%, sparsity: 0.3847\n",
      "mean block mean pruning test accuracy: 89.25%, sparsity: 0.3847\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 88.91%, sparsity: 0.2494\n",
      "per row magnitude pruning test accuracy: 89.28%, sparsity: 0.2101\n",
      "per block magnitude pruning test accuracy: 89.35%, sparsity: 0.2484\n",
      "mean column mean pruning test accuracy: 89.21%, sparsity: 0.2411\n",
      "mean row mean pruning test accuracy: 89.30%, sparsity: 0.2484\n",
      "mean block mean pruning test accuracy: 89.27%, sparsity: 0.2484\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 89.16%, sparsity: 0.0833\n",
      "per row magnitude pruning test accuracy: 89.50%, sparsity: 0.0420\n",
      "per block magnitude pruning test accuracy: 89.45%, sparsity: 0.0824\n",
      "mean column mean pruning test accuracy: 89.30%, sparsity: 0.0735\n",
      "mean row mean pruning test accuracy: 89.47%, sparsity: 0.0824\n",
      "mean block mean pruning test accuracy: 89.42%, sparsity: 0.0824\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 89.31%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.38%, sparsity: 0.0029\n",
      "per block magnitude pruning test accuracy: 89.38%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.36%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 89.36%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.38%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 89.29%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.38%, sparsity: 0.0029\n",
      "per block magnitude pruning test accuracy: 89.38%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.36%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 89.36%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.38%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 89.38%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 89.29%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.38%, sparsity: 0.0029\n",
      "per block magnitude pruning test accuracy: 89.38%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.36%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 89.36%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.38%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 575,050\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 75.33%, sparsity: 0.7763\n",
      "per row magnitude pruning test accuracy: 76.20%, sparsity: 0.7650\n",
      "per block magnitude pruning test accuracy: 76.81%, sparsity: 0.7751\n",
      "mean column mean pruning test accuracy: 80.99%, sparsity: 0.7739\n",
      "mean row mean pruning test accuracy: 83.34%, sparsity: 0.7751\n",
      "mean block mean pruning test accuracy: 83.83%, sparsity: 0.7751\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 82.72%, sparsity: 0.7268\n",
      "per row magnitude pruning test accuracy: 80.96%, sparsity: 0.7126\n",
      "per block magnitude pruning test accuracy: 82.97%, sparsity: 0.7253\n",
      "mean column mean pruning test accuracy: 81.74%, sparsity: 0.7236\n",
      "mean row mean pruning test accuracy: 84.76%, sparsity: 0.7253\n",
      "mean block mean pruning test accuracy: 83.41%, sparsity: 0.7253\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 85.38%, sparsity: 0.6663\n",
      "per row magnitude pruning test accuracy: 83.39%, sparsity: 0.6487\n",
      "per block magnitude pruning test accuracy: 84.90%, sparsity: 0.6645\n",
      "mean column mean pruning test accuracy: 78.24%, sparsity: 0.6622\n",
      "mean row mean pruning test accuracy: 86.31%, sparsity: 0.6645\n",
      "mean block mean pruning test accuracy: 87.41%, sparsity: 0.6645\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 87.60%, sparsity: 0.5925\n",
      "per row magnitude pruning test accuracy: 88.32%, sparsity: 0.5706\n",
      "per block magnitude pruning test accuracy: 88.96%, sparsity: 0.5902\n",
      "mean column mean pruning test accuracy: 85.76%, sparsity: 0.5873\n",
      "mean row mean pruning test accuracy: 87.78%, sparsity: 0.5902\n",
      "mean block mean pruning test accuracy: 88.77%, sparsity: 0.5902\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 88.49%, sparsity: 0.5022\n",
      "per row magnitude pruning test accuracy: 89.15%, sparsity: 0.4752\n",
      "per block magnitude pruning test accuracy: 88.97%, sparsity: 0.4995\n",
      "mean column mean pruning test accuracy: 88.02%, sparsity: 0.4957\n",
      "mean row mean pruning test accuracy: 88.71%, sparsity: 0.4995\n",
      "mean block mean pruning test accuracy: 89.01%, sparsity: 0.4995\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 88.02%, sparsity: 0.3920\n",
      "per row magnitude pruning test accuracy: 89.08%, sparsity: 0.3586\n",
      "per block magnitude pruning test accuracy: 88.92%, sparsity: 0.3887\n",
      "mean column mean pruning test accuracy: 88.79%, sparsity: 0.3839\n",
      "mean row mean pruning test accuracy: 89.09%, sparsity: 0.3887\n",
      "mean block mean pruning test accuracy: 89.26%, sparsity: 0.3887\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 89.36%, sparsity: 0.2574\n",
      "per row magnitude pruning test accuracy: 89.44%, sparsity: 0.2163\n",
      "per block magnitude pruning test accuracy: 89.40%, sparsity: 0.2533\n",
      "mean column mean pruning test accuracy: 89.13%, sparsity: 0.2473\n",
      "mean row mean pruning test accuracy: 89.34%, sparsity: 0.2533\n",
      "mean block mean pruning test accuracy: 89.26%, sparsity: 0.2533\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 89.32%, sparsity: 0.0930\n",
      "per row magnitude pruning test accuracy: 89.43%, sparsity: 0.0483\n",
      "per block magnitude pruning test accuracy: 89.51%, sparsity: 0.0881\n",
      "mean column mean pruning test accuracy: 89.51%, sparsity: 0.0812\n",
      "mean row mean pruning test accuracy: 89.58%, sparsity: 0.0881\n",
      "mean block mean pruning test accuracy: 89.46%, sparsity: 0.0881\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 89.45%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.45%, sparsity: 0.0031\n",
      "per block magnitude pruning test accuracy: 89.46%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.58%, sparsity: 0.0025\n",
      "mean row mean pruning test accuracy: 89.44%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.45%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 89.45%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.46%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 89.46%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.57%, sparsity: 0.0017\n",
      "mean row mean pruning test accuracy: 89.44%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.45%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced_Deep, test accuracy: 89.46%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 89.44%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.46%, sparsity: 0.0030\n",
      "per block magnitude pruning test accuracy: 89.46%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.57%, sparsity: 0.0017\n",
      "mean row mean pruning test accuracy: 89.44%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.45%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 3,716,106\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 75.27%, sparsity: 0.8189\n",
      "per row magnitude pruning test accuracy: 72.90%, sparsity: 0.7926\n",
      "per block magnitude pruning test accuracy: 79.64%, sparsity: 0.8188\n",
      "mean column mean pruning test accuracy: 65.62%, sparsity: 0.8012\n",
      "mean row mean pruning test accuracy: 70.15%, sparsity: 0.8188\n",
      "mean block mean pruning test accuracy: 73.00%, sparsity: 0.8188\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 79.08%, sparsity: 0.7789\n",
      "per row magnitude pruning test accuracy: 75.64%, sparsity: 0.7465\n",
      "per block magnitude pruning test accuracy: 83.48%, sparsity: 0.7786\n",
      "mean column mean pruning test accuracy: 67.07%, sparsity: 0.7571\n",
      "mean row mean pruning test accuracy: 71.69%, sparsity: 0.7786\n",
      "mean block mean pruning test accuracy: 78.55%, sparsity: 0.7786\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 84.65%, sparsity: 0.7299\n",
      "per row magnitude pruning test accuracy: 79.05%, sparsity: 0.6903\n",
      "per block magnitude pruning test accuracy: 85.41%, sparsity: 0.7296\n",
      "mean column mean pruning test accuracy: 75.73%, sparsity: 0.7032\n",
      "mean row mean pruning test accuracy: 73.48%, sparsity: 0.7296\n",
      "mean block mean pruning test accuracy: 82.57%, sparsity: 0.7296\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 86.60%, sparsity: 0.6701\n",
      "per row magnitude pruning test accuracy: 83.05%, sparsity: 0.6216\n",
      "per block magnitude pruning test accuracy: 87.14%, sparsity: 0.6698\n",
      "mean column mean pruning test accuracy: 81.49%, sparsity: 0.6374\n",
      "mean row mean pruning test accuracy: 75.87%, sparsity: 0.6698\n",
      "mean block mean pruning test accuracy: 83.96%, sparsity: 0.6698\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 87.40%, sparsity: 0.5971\n",
      "per row magnitude pruning test accuracy: 84.99%, sparsity: 0.5377\n",
      "per block magnitude pruning test accuracy: 88.12%, sparsity: 0.5967\n",
      "mean column mean pruning test accuracy: 83.89%, sparsity: 0.5571\n",
      "mean row mean pruning test accuracy: 79.53%, sparsity: 0.5967\n",
      "mean block mean pruning test accuracy: 86.50%, sparsity: 0.5967\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 88.63%, sparsity: 0.5079\n",
      "per row magnitude pruning test accuracy: 87.96%, sparsity: 0.4353\n",
      "per block magnitude pruning test accuracy: 88.80%, sparsity: 0.5074\n",
      "mean column mean pruning test accuracy: 86.27%, sparsity: 0.4589\n",
      "mean row mean pruning test accuracy: 86.56%, sparsity: 0.5074\n",
      "mean block mean pruning test accuracy: 88.07%, sparsity: 0.5074\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 88.99%, sparsity: 0.3989\n",
      "per row magnitude pruning test accuracy: 89.12%, sparsity: 0.3102\n",
      "per block magnitude pruning test accuracy: 89.05%, sparsity: 0.3983\n",
      "mean column mean pruning test accuracy: 88.04%, sparsity: 0.3390\n",
      "mean row mean pruning test accuracy: 88.71%, sparsity: 0.3983\n",
      "mean block mean pruning test accuracy: 88.77%, sparsity: 0.3983\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 89.27%, sparsity: 0.2658\n",
      "per row magnitude pruning test accuracy: 89.23%, sparsity: 0.1576\n",
      "per block magnitude pruning test accuracy: 89.28%, sparsity: 0.2651\n",
      "mean column mean pruning test accuracy: 88.93%, sparsity: 0.1934\n",
      "mean row mean pruning test accuracy: 88.85%, sparsity: 0.2651\n",
      "mean block mean pruning test accuracy: 89.16%, sparsity: 0.2651\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 89.22%, sparsity: 0.1034\n",
      "per row magnitude pruning test accuracy: 89.24%, sparsity: 0.0489\n",
      "per block magnitude pruning test accuracy: 89.28%, sparsity: 0.1027\n",
      "mean column mean pruning test accuracy: 89.24%, sparsity: 0.0734\n",
      "mean row mean pruning test accuracy: 89.14%, sparsity: 0.1027\n",
      "mean block mean pruning test accuracy: 89.31%, sparsity: 0.1027\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 89.17%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.27%, sparsity: 0.0037\n",
      "per block magnitude pruning test accuracy: 89.28%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.17%, sparsity: 0.0036\n",
      "mean row mean pruning test accuracy: 89.23%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.28%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Wide, test accuracy: 89.28%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 89.19%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.28%, sparsity: 0.0010\n",
      "per block magnitude pruning test accuracy: 89.28%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.13%, sparsity: 0.0008\n",
      "mean row mean pruning test accuracy: 89.27%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.28%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 14,234,122\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 49.45%, sparsity: 0.8115\n",
      "per row magnitude pruning test accuracy: 61.41%, sparsity: 0.7879\n",
      "per block magnitude pruning test accuracy: 60.45%, sparsity: 0.8106\n",
      "mean column mean pruning test accuracy: 40.97%, sparsity: 0.8014\n",
      "mean row mean pruning test accuracy: 38.70%, sparsity: 0.8106\n",
      "mean block mean pruning test accuracy: 61.73%, sparsity: 0.8106\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 56.87%, sparsity: 0.7698\n",
      "per row magnitude pruning test accuracy: 66.76%, sparsity: 0.7408\n",
      "per block magnitude pruning test accuracy: 72.02%, sparsity: 0.7687\n",
      "mean column mean pruning test accuracy: 46.63%, sparsity: 0.7573\n",
      "mean row mean pruning test accuracy: 51.93%, sparsity: 0.7687\n",
      "mean block mean pruning test accuracy: 67.63%, sparsity: 0.7687\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 68.60%, sparsity: 0.7188\n",
      "per row magnitude pruning test accuracy: 72.08%, sparsity: 0.6834\n",
      "per block magnitude pruning test accuracy: 83.18%, sparsity: 0.7174\n",
      "mean column mean pruning test accuracy: 57.11%, sparsity: 0.7035\n",
      "mean row mean pruning test accuracy: 58.73%, sparsity: 0.7174\n",
      "mean block mean pruning test accuracy: 75.21%, sparsity: 0.7174\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 75.76%, sparsity: 0.6565\n",
      "per row magnitude pruning test accuracy: 79.75%, sparsity: 0.6132\n",
      "per block magnitude pruning test accuracy: 85.50%, sparsity: 0.6549\n",
      "mean column mean pruning test accuracy: 72.38%, sparsity: 0.6378\n",
      "mean row mean pruning test accuracy: 71.71%, sparsity: 0.6549\n",
      "mean block mean pruning test accuracy: 78.57%, sparsity: 0.6549\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 82.40%, sparsity: 0.5805\n",
      "per row magnitude pruning test accuracy: 84.13%, sparsity: 0.5275\n",
      "per block magnitude pruning test accuracy: 87.79%, sparsity: 0.5785\n",
      "mean column mean pruning test accuracy: 80.16%, sparsity: 0.5576\n",
      "mean row mean pruning test accuracy: 74.53%, sparsity: 0.5785\n",
      "mean block mean pruning test accuracy: 83.93%, sparsity: 0.5785\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 87.90%, sparsity: 0.4876\n",
      "per row magnitude pruning test accuracy: 87.19%, sparsity: 0.4229\n",
      "per block magnitude pruning test accuracy: 89.37%, sparsity: 0.4851\n",
      "mean column mean pruning test accuracy: 85.87%, sparsity: 0.4596\n",
      "mean row mean pruning test accuracy: 77.95%, sparsity: 0.4851\n",
      "mean block mean pruning test accuracy: 86.10%, sparsity: 0.4851\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 88.72%, sparsity: 0.3742\n",
      "per row magnitude pruning test accuracy: 89.16%, sparsity: 0.2950\n",
      "per block magnitude pruning test accuracy: 89.70%, sparsity: 0.3712\n",
      "mean column mean pruning test accuracy: 88.13%, sparsity: 0.3398\n",
      "mean row mean pruning test accuracy: 83.01%, sparsity: 0.3712\n",
      "mean block mean pruning test accuracy: 88.17%, sparsity: 0.3712\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 89.39%, sparsity: 0.2356\n",
      "per row magnitude pruning test accuracy: 89.64%, sparsity: 0.1389\n",
      "per block magnitude pruning test accuracy: 89.57%, sparsity: 0.2319\n",
      "mean column mean pruning test accuracy: 89.36%, sparsity: 0.1948\n",
      "mean row mean pruning test accuracy: 89.23%, sparsity: 0.2319\n",
      "mean block mean pruning test accuracy: 89.07%, sparsity: 0.2319\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 89.55%, sparsity: 0.0887\n",
      "per row magnitude pruning test accuracy: 89.52%, sparsity: 0.0302\n",
      "per block magnitude pruning test accuracy: 89.53%, sparsity: 0.0840\n",
      "mean column mean pruning test accuracy: 89.45%, sparsity: 0.0785\n",
      "mean row mean pruning test accuracy: 89.56%, sparsity: 0.0840\n",
      "mean block mean pruning test accuracy: 89.60%, sparsity: 0.0840\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 89.59%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.56%, sparsity: 0.0032\n",
      "per block magnitude pruning test accuracy: 89.55%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.61%, sparsity: 0.0112\n",
      "mean row mean pruning test accuracy: 89.60%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.55%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Very_Wide, test accuracy: 89.55%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 89.54%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 89.55%, sparsity: 0.0006\n",
      "per block magnitude pruning test accuracy: 89.55%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 89.56%, sparsity: 0.0007\n",
      "mean row mean pruning test accuracy: 89.54%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 89.55%, sparsity: 0.0000\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = geluLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_{datasets_name}/gelu_{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c63c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 50,890\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 18.10%, sparsity: 0.7785\n",
      "per row magnitude pruning test accuracy: 26.55%, sparsity: 0.7710\n",
      "per block magnitude pruning test accuracy: 33.76%, sparsity: 0.7787\n",
      "mean column mean pruning test accuracy: 28.04%, sparsity: 0.7721\n",
      "mean row mean pruning test accuracy: 55.87%, sparsity: 0.7787\n",
      "mean block mean pruning test accuracy: 34.01%, sparsity: 0.7787\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 18.43%, sparsity: 0.7295\n",
      "per row magnitude pruning test accuracy: 34.65%, sparsity: 0.7181\n",
      "per block magnitude pruning test accuracy: 37.66%, sparsity: 0.7297\n",
      "mean column mean pruning test accuracy: 34.54%, sparsity: 0.7214\n",
      "mean row mean pruning test accuracy: 56.90%, sparsity: 0.7297\n",
      "mean block mean pruning test accuracy: 53.41%, sparsity: 0.7297\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 26.22%, sparsity: 0.6696\n",
      "per row magnitude pruning test accuracy: 31.45%, sparsity: 0.6538\n",
      "per block magnitude pruning test accuracy: 42.47%, sparsity: 0.6698\n",
      "mean column mean pruning test accuracy: 30.36%, sparsity: 0.6595\n",
      "mean row mean pruning test accuracy: 60.16%, sparsity: 0.6698\n",
      "mean block mean pruning test accuracy: 47.65%, sparsity: 0.6698\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 30.62%, sparsity: 0.5964\n",
      "per row magnitude pruning test accuracy: 47.49%, sparsity: 0.5756\n",
      "per block magnitude pruning test accuracy: 41.56%, sparsity: 0.5967\n",
      "mean column mean pruning test accuracy: 44.09%, sparsity: 0.5839\n",
      "mean row mean pruning test accuracy: 72.35%, sparsity: 0.5967\n",
      "mean block mean pruning test accuracy: 67.55%, sparsity: 0.5967\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 49.35%, sparsity: 0.5070\n",
      "per row magnitude pruning test accuracy: 44.38%, sparsity: 0.4797\n",
      "per block magnitude pruning test accuracy: 54.02%, sparsity: 0.5074\n",
      "mean column mean pruning test accuracy: 67.37%, sparsity: 0.4917\n",
      "mean row mean pruning test accuracy: 74.04%, sparsity: 0.5074\n",
      "mean block mean pruning test accuracy: 63.70%, sparsity: 0.5074\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 51.18%, sparsity: 0.3979\n",
      "per row magnitude pruning test accuracy: 52.91%, sparsity: 0.3624\n",
      "per block magnitude pruning test accuracy: 48.45%, sparsity: 0.3983\n",
      "mean column mean pruning test accuracy: 69.65%, sparsity: 0.3790\n",
      "mean row mean pruning test accuracy: 74.08%, sparsity: 0.3983\n",
      "mean block mean pruning test accuracy: 63.71%, sparsity: 0.3983\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 57.04%, sparsity: 0.2646\n",
      "per row magnitude pruning test accuracy: 59.79%, sparsity: 0.2196\n",
      "per block magnitude pruning test accuracy: 58.61%, sparsity: 0.2651\n",
      "mean column mean pruning test accuracy: 68.79%, sparsity: 0.2414\n",
      "mean row mean pruning test accuracy: 72.93%, sparsity: 0.2651\n",
      "mean block mean pruning test accuracy: 67.11%, sparsity: 0.2651\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 58.56%, sparsity: 0.1017\n",
      "per row magnitude pruning test accuracy: 60.22%, sparsity: 0.0598\n",
      "per block magnitude pruning test accuracy: 60.47%, sparsity: 0.1025\n",
      "mean column mean pruning test accuracy: 63.84%, sparsity: 0.0795\n",
      "mean row mean pruning test accuracy: 63.28%, sparsity: 0.1025\n",
      "mean block mean pruning test accuracy: 63.18%, sparsity: 0.1025\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 58.30%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 60.27%, sparsity: 0.0174\n",
      "per block magnitude pruning test accuracy: 60.40%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 62.47%, sparsity: 0.0016\n",
      "mean row mean pruning test accuracy: 61.00%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 60.43%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 58.58%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 60.27%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 60.40%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 61.68%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 60.98%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 60.43%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Tiny_Underfit, test accuracy: 60.40%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 58.58%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 60.27%, sparsity: 0.0167\n",
      "per block magnitude pruning test accuracy: 60.40%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 61.68%, sparsity: 0.0015\n",
      "mean row mean pruning test accuracy: 60.98%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 60.43%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 217,354\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.7659\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.7624\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.7656\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.7647\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.7656\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.7656\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 0.4493289589881897\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.7140\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.7087\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.7136\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.7120\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.7136\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.7136\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 0.5488116145133972\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.6507\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.6433\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.6502\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.6478\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.6502\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.6502\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 0.6703200340270996\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.5734\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.5634\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.5728\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.5692\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.5728\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.5728\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 0.8187307715415955\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.4789\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.4659\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.4782\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.4734\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.4782\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.4782\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 1.0\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.3636\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.3466\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.3627\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.3563\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.3627\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.3627\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 1.2214027643203735\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.2227\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.2012\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.2216\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.2132\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.2216\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.2216\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 1.4918246269226074\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.0509\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.0403\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.0496\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.0519\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.0496\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.0496\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 1.822118878364563\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.0086\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.0053\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 2.22554087638855\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.0084\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.0048\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "\t\n",
      "========================================\n",
      "model name: Deep_Narrow, test accuracy: 10.00%, beta: 2.7182817459106445\n",
      "per column magnitude pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "per row magnitude pruning test accuracy: 10.00%, sparsity: 0.0084\n",
      "per block magnitude pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "mean column mean pruning test accuracy: 10.00%, sparsity: 0.0048\n",
      "mean row mean pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "mean block mean pruning test accuracy: 10.00%, sparsity: 0.0000\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 535,818\n",
      "\t\n",
      "========================================\n",
      "model name: Balanced, test accuracy: 40.67%, beta: 0.3678794503211975\n",
      "per column magnitude pruning test accuracy: 10.02%, sparsity: 0.7730\n",
      "per row magnitude pruning test accuracy: 15.49%, sparsity: 0.7630\n",
      "per block magnitude pruning test accuracy: 13.99%, sparsity: 0.7729\n",
      "mean column mean pruning test accuracy: 14.59%, sparsity: 0.7702\n",
      "mean row mean pruning test accuracy: 44.60%, sparsity: 0.7729\n",
      "mean block mean pruning test accuracy: 13.28%, sparsity: 0.7729\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m beta \u001b[38;5;129;01min\u001b[39;00m betas:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# magnitude\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     pc_model, pc_neff \u001b[38;5;241m=\u001b[39m model_pc(model, renormalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, beta\u001b[38;5;241m=\u001b[39mbeta, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmagnitude\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m     test_loss, test_accuracy, accuracy_mean \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_magnitude\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(accuracy_mean)\n",
      "Cell \u001b[1;32mIn[4], line 70\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, device, test_loader, times)\u001b[0m\n\u001b[0;32m     68\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     71\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     72\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1443\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1445\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = SigmoidLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_{datasets_name}/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01af35f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 50,890\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m test_loss, test_accuracy, origin_test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(model_name)\n\u001b[0;32m     31\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(origin_test_accuracy)\n",
      "Cell \u001b[1;32mIn[4], line 70\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, device, test_loader, times)\u001b[0m\n\u001b[0;32m     68\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     71\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     72\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1443\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1445\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1287\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = tanhLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_{datasets_name}/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    \n",
    "    result['model_name'].append(model_name)\n",
    "    result['test_accuracy'].append(origin_test_accuracy)\n",
    "    result['model_sparsity'].append(0.0)\n",
    "    result['shadow_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    coefficients = torch.linspace(-1, 1, 11)\n",
    "    betas = torch.exp(coefficients).tolist()\n",
    "\n",
    "    for beta in betas:\n",
    "        # magnitude\n",
    "        pc_model, pc_neff = model_pc(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pr_model, pr_neff = model_pr(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        pb_model, pb_neff = model_block(model, renormalize=False, beta=beta, method='magnitude')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_magnitude\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # mean\n",
    "        mean_pc_model, mean_pc_neff = model_pc(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pc_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pc_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pc_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pr_model, mean_pr_neff = model_pr(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pr_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pr_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pr_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        mean_pb_model, mean_pb_neff = model_block(model, renormalize=False, beta=beta, method='mean')\n",
    "        test_loss, test_accuracy, accuracy_mean = test(mean_pb_model, device, test_loader, times=5)\n",
    "        result['model_name'].append(f\"{model_name}_pb_{beta}_mean\")\n",
    "        result['test_accuracy'].append(accuracy_mean)\n",
    "        result['model_sparsity'].append(model_sparsity(mean_pb_model))\n",
    "        result['shadow_accuracy'].append(test_accuracy)\n",
    "\n",
    "        # summary\n",
    "        print('\\t')\n",
    "        print('='*40)\n",
    "        print(f\"model name: {model_name}, test accuracy: {origin_test_accuracy:.2f}%, beta: {beta}\")\n",
    "        print(f\"per column magnitude pruning test accuracy: {result['test_accuracy'][-6]:.2f}%, sparsity: {result['model_sparsity'][-4]:.4f}\")\n",
    "        print(f\"per row magnitude pruning test accuracy: {result['test_accuracy'][-5]:.2f}%, sparsity: {result['model_sparsity'][-3]:.4f}\")\n",
    "        print(f\"per block magnitude pruning test accuracy: {result['test_accuracy'][-4]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean column mean pruning test accuracy: {result['test_accuracy'][-3]:.2f}%, sparsity: {result['model_sparsity'][-2]:.4f}\")\n",
    "        print(f\"mean row mean pruning test accuracy: {result['test_accuracy'][-2]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")\n",
    "        print(f\"mean block mean pruning test accuracy: {result['test_accuracy'][-1]:.2f}%, sparsity: {result['model_sparsity'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4037533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 50,890\n",
      "Test accuracy: 70.06%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 217,354\n",
      "Test accuracy: 41.67%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 535,818\n",
      "Test accuracy: 73.81%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 575,050\n",
      "Test accuracy: 66.39%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 3,716,106\n",
      "Test accuracy: 14.35%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 14,234,122\n",
      "Test accuracy: 10.01%\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = tanhLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_{datasets_name}/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    print(f\"Test accuracy: {origin_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc4b456a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 10, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 50,890\n",
      "Test accuracy: 60.40%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> 128 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 217,354\n",
      "Test accuracy: 10.00%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 15, Dropout: 0.2\n",
      "============================================================\n",
      "Total parameters: 535,818\n",
      "Test accuracy: 40.67%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 512 -> 256 -> 128 -> 64 -> Output(10)\n",
      "Learning rate: 0.0003, Epochs: 20, Dropout: 0.3\n",
      "============================================================\n",
      "Total parameters: 575,050\n",
      "Test accuracy: 10.00%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 2048 -> 1024 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 30, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 3,716,106\n",
      "Test accuracy: 66.87%\n",
      "\n",
      "============================================================\n",
      "Architecture: Input(784) -> 4096 -> 2048 -> 1024 -> 512 -> Output(10)\n",
      "Learning rate: 0.001, Epochs: 50, Dropout: 0.0\n",
      "============================================================\n",
      "Total parameters: 14,234,122\n",
      "Test accuracy: 16.04%\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'model_name': [],\n",
    "    'test_accuracy': [],\n",
    "    'model_sparsity': [],\n",
    "    'shadow_accuracy': []\n",
    "}\n",
    "\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Architecture: Input(784) -> {' -> '.join(map(str, config['hidden_size']))} -> Output(10)\")\n",
    "    print(f\"Learning rate: {config['lr']}, Epochs: {config['epochs']}, Dropout: {config['dropout']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = SigmoidLinearModel(\n",
    "        input_size=input_size,\n",
    "        output_size=num_classes,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    ).to(device)\n",
    "    model.load(f'paper_{datasets_name}/{model_name}.pth')\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    test_loss, test_accuracy, origin_test_accuracy = test(model, device, test_loader, times=5)\n",
    "    print(f\"Test accuracy: {origin_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82ebcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
