{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2fad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "folder = \"test_results\"\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d255b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pc(module:nn.Module, beta=1.0, method='magnitude') -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    if method == 'mean':\n",
    "        x = x - x.mean(dim=0, keepdim=True)\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = 1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0)\n",
    "    r_neff = torch.floor(beta * neff)\n",
    "    r_neff = r_neff.clamp(min=1, max=output_size-1)\n",
    "\n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < r_neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "def model_pc(model, renormalize=False, beta=1.0, method='magnitude'):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask, neff = mask_pc(module, beta=beta, method=method)\n",
    "            mask = mask.to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                pre = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    post = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                    module.weight.mul_(pre / post)\n",
    "    return model, neff\n",
    "\n",
    "def mask_pr(module:nn.Module, beta=1.0, method='magnitude') -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    if method == 'mean':\n",
    "        x = x - x.mean(dim=1, keepdim=True)\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=1, keepdim=True)\n",
    "    neff = 1/torch.sum((x_norm ** 2), dim=1, keepdim=True).squeeze(0)\n",
    "    r_neff = torch.floor(beta * neff)\n",
    "    r_neff = r_neff.clamp(min=1, max=input_size-1)\n",
    "\n",
    "    _, indices = torch.sort(x_norm, dim=1, descending=True)\n",
    "    range_tensor = torch.arange(input_size, device=x.device).unsqueeze(0).expand(output_size, -1)\n",
    "    sorted_mask = range_tensor < r_neff\n",
    "\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(1, indices, sorted_mask)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "def model_pr(model, renormalize=False, beta=1.0, method='magnitude'):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask, neff = mask_pr(module, beta=beta, method=method)\n",
    "            mask = mask.to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                pre = module.weight.abs().sum(dim=1, keepdim=True)\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    post = module.weight.abs().sum(dim=1, keepdim=True)\n",
    "                    module.weight.mul_(pre / post)\n",
    "    return model, neff\n",
    "\n",
    "def mask_block(module:nn.Module, beta=1.0, method='magnitude') -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    x = x.view(-1)\n",
    "    if method == 'mean':\n",
    "        x = x - torch.mean(x)\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x))\n",
    "    neff = 1/torch.sum((x_norm ** 2))\n",
    "    r_neff = torch.floor(beta * neff)\n",
    "    r_neff = r_neff.clamp(min=1, max=len(x)-1)\n",
    "\n",
    "    _, indices = torch.sort(x_norm, descending=True)\n",
    "    range_tensor = torch.arange(len(x), device=x.device)\n",
    "    sorted_mask = range_tensor < r_neff\n",
    "\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    mask = mask.view_as(module.weight)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "def model_block(model, renormalize=False, beta=1.0, method='magnitude'):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask, neff = mask_block(module, beta=beta, method=method)\n",
    "            mask = mask.to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                pre = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    post = module.weight.abs().sum(dim=0, keepdim=True)\n",
    "                    module.weight.mul_(pre / post)\n",
    "    return model, neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3751fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "def per_layer_neff(model):\n",
    "    \"\"\"Calculate the effective parameters (Neff) per layer\"\"\"\n",
    "    neff = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            layer_neff = torch.sum(param != 0).item()\n",
    "            neff[name] = layer_neff\n",
    "    return neff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b7aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Dataset setup\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model class with optional dropout\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=[512, 512, 512], dropout_rate=0.0):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}'\n",
    "                  f'accuracy: {100. * correct / len(train_loader.dataset):.2f}%')\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Testing function\n",
    "def test(model, device, test_loader, times=1):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    accuracy_list = []\n",
    "    loss_list = []\n",
    "    for _ in range(times):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        loss_list.append(test_loss)\n",
    "\n",
    "    if times == 1:\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, '\n",
    "              f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "        return test_loss, accuracy\n",
    "    \n",
    "    else:\n",
    "        return loss_list, accuracy_list, sum(accuracy_list)/times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab23c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
