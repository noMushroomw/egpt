{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8dc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "folder = \"track_structure\"\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775bb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask_per_column(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_linear_mask_per_row(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=1, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=1, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=1, descending=True)\n",
    "    range_tensor = torch.arange(input_size, device=x.device).unsqueeze(0).expand(output_size, -1)\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(1, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "847849d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_per_column_structure(model):\n",
    "    model = copy.deepcopy(model)\n",
    "    layer_masks = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask_per_column(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                layer_masks[name] = module.weight.clone()\n",
    "    return model, layer_masks\n",
    "\n",
    "def prune_model_neff_per_row_structure(model):\n",
    "    model = copy.deepcopy(model)\n",
    "    layer_masks = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask_per_row(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                layer_masks[name] = module.weight.clone()\n",
    "    return model, layer_masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932653d",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fdd5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class with optional dropout\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=[512, 512, 512], dropout_rate=0.0):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb88970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "model_configs = {\n",
    "    'Model_1_Underfit': {\n",
    "        'hidden_size': [64, 32, 16],  # Very shallow, only 1 small hidden layer\n",
    "        'lr': 1e-4,  # Lower learning rate\n",
    "        'epochs': 5,  # Fewer epochs\n",
    "        'dropout': 0.0,\n",
    "        'description': 'Underfitted: Too simple (1 layer, 32 units)'\n",
    "    },\n",
    "    'Model_2_Slight_Underfit': {\n",
    "        'hidden_size': [256, 128, 64],  # 2 small layers\n",
    "        'lr': 5e-4,\n",
    "        'epochs': 8,\n",
    "        'dropout': 0.0,\n",
    "        'description': 'Slightly underfitted: Simple architecture'\n",
    "    },\n",
    "    'Model_3_Well_Trained': {\n",
    "        'hidden_size': [512, 256, 128],  # Moderate depth and width\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 15,\n",
    "        'dropout': 0.2,  # Some regularization\n",
    "        'description': 'Well-trained: Balanced architecture with dropout'\n",
    "    },\n",
    "    'Model_4_Well_Trained_Deep': {\n",
    "        'hidden_size': [1024, 512, 256],  # Deeper but with dropout\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 20,\n",
    "        'dropout': 0.3,  # More dropout for regularization\n",
    "        'description': 'Well-trained: Deeper with good regularization'\n",
    "    },\n",
    "    'Model_5_Overfit': {\n",
    "        'hidden_size': [2048, 1024, 1024],  # Very deep and wide\n",
    "        'lr': 1e-3,  # Higher learning rate\n",
    "        'epochs': 30,  # Many epochs\n",
    "        'dropout': 0.0,  # No regularization\n",
    "        'description': 'Overfitted: Very complex without regularization'\n",
    "    },\n",
    "    'Model_6_Extra_Overfit': {\n",
    "        'hidden_size': [4096, 2048, 1024],  # Extremely deep and wide\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 50,\n",
    "        'dropout': 0.0,\n",
    "        'description': 'Extra Overfitted: Very complex without regularization'\n",
    "    },\n",
    "    'Model_7_Extra_Overfit': {\n",
    "        'hidden_size': [8192, 4096, 2048],  # Extremely deep and wide\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 100,\n",
    "        'dropout': 0.0,\n",
    "        'description': 'Extra Overfitted: Very complex without regularization'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17a6e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_masks = {}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    model = LinearModel(input_size=784, output_size=10, hidden_size=config['hidden_size'], dropout_rate=config['dropout'])\n",
    "    # load from models folder\n",
    "    model.load_state_dict(torch.load(f\"models/{model_name}.pth\"))\n",
    "    row_model, row_mask = prune_model_neff_per_row_structure(model)\n",
    "    row_masks[model_name] = row_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "261092f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = row_masks['Model_1_Underfit']['layers.1']\n",
    "test2 = row_masks['Model_2_Slight_Underfit']['layers.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0646dab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: Model_1_Underfit\n",
      "Loaded: Model_2_Slight_Underfit\n",
      "Loaded: Model_3_Well_Trained\n",
      "Loaded: Model_4_Well_Trained_Deep\n",
      "Loaded: Model_5_Overfit\n",
      "Loaded: Model_6_Extra_Overfit\n",
      "Loaded: Model_7_Extra_Overfit\n",
      "Pruning Model_1_Underfit [per_row] ...\n",
      "[Saved] PDF report: ./prune_outputs\\Model_1_Underfit\\Model_1_Underfit_row_prune_report.pdf\n",
      "Pruning Model_2_Slight_Underfit [per_row] ...\n",
      "[Saved] PDF report: ./prune_outputs\\Model_2_Slight_Underfit\\Model_2_Slight_Underfit_row_prune_report.pdf\n",
      "Pruning Model_3_Well_Trained [per_row] ...\n",
      "[Saved] PDF report: ./prune_outputs\\Model_3_Well_Trained\\Model_3_Well_Trained_row_prune_report.pdf\n",
      "Pruning Model_4_Well_Trained_Deep [per_row] ...\n",
      "[Saved] PDF report: ./prune_outputs\\Model_4_Well_Trained_Deep\\Model_4_Well_Trained_Deep_row_prune_report.pdf\n",
      "Pruning Model_5_Overfit [per_row] ...\n",
      "[Saved] PDF report: ./prune_outputs\\Model_5_Overfit\\Model_5_Overfit_row_prune_report.pdf\n",
      "Pruning Model_6_Extra_Overfit [per_row] ...\n",
      "[Saved] PDF report: ./prune_outputs\\Model_6_Extra_Overfit\\Model_6_Extra_Overfit_row_prune_report.pdf\n",
      "Pruning Model_7_Extra_Overfit [per_row] ...\n",
      "[Saved] PDF report: ./prune_outputs\\Model_7_Extra_Overfit\\Model_7_Extra_Overfit_row_prune_report.pdf\n",
      "\n",
      "[COMPARE ROW-PRUNED] Model_1_Underfit  VS  Model_2_Slight_Underfit\n",
      "Compared layer 0: layers.0 vs layers.0 -> {'cosine': 0.7862584590911865, 'ncc': 0.2715766429901123, 'mse': 0.020386314019560814, 'emd_hist': 0.034215450286865234, 'spearman': 0.27503490447998047, 'combo_score': 0.7528, 'verdict': 'SIMILAR'}\n",
      "Compared layer 1: layers.1 vs layers.1 -> {'cosine': 0.5851411819458008, 'ncc': 0.07489702850580215, 'mse': 0.07226088643074036, 'emd_hist': 0.1296095848083496, 'spearman': 0.0594063326716423, 'combo_score': 0.565, 'verdict': 'DIFFERENT'}\n",
      "Compared layer 2: layers.2 vs layers.2 -> {'cosine': 0.6095318794250488, 'ncc': 0.048978980630636215, 'mse': 0.09401950240135193, 'emd_hist': 0.10481643676757812, 'spearman': 0.0466027557849884, 'combo_score': 0.5532, 'verdict': 'DIFFERENT'}\n",
      "Compared layer 3: output vs output -> {'cosine': 0.656946063041687, 'ncc': 0.024616174399852753, 'mse': 0.12654268741607666, 'emd_hist': 0.014028549194335938, 'spearman': 0.020806938409805298, 'combo_score': 0.5732, 'verdict': 'DIFFERENT'}\n",
      "[Saved] Mask comparison → ./prune_outputs\\compare_Model_1_Underfit_VS_Model_2_Slight_Underfit_ROWPRUNE\\mask_compare_summary.json\n",
      "[Saved] ./prune_outputs\\pairwise_row_model_similarity\\row_model_similarity_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# row_prune_and_compare.py\n",
    "# ------------------------------------------------------------\n",
    "# N_eff-based ROW pruning with recording, visualization, and\n",
    "# model similarity utilities tailored for ROW-pruned analysis.\n",
    "#\n",
    "# Dependencies: torch, numpy, matplotlib\n",
    "# Place your checkpoints (.pt/.pth with state_dict) under ./models\n",
    "# Outputs under ./prune_outputs/\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import copy\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Your MLP definition (as given)\n",
    "# ----------------------------\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=[512, 512, 512], dropout_rate=0.0):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "\n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)  # Apply dropout after activation\n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utils: State dict introspection\n",
    "# ----------------------------\n",
    "def strip_module_prefix(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Remove a leading 'module.' (from DataParallel) if present.\"\"\"\n",
    "    if not state_dict:\n",
    "        return state_dict\n",
    "    if all(k.startswith('module.') for k in state_dict.keys()):\n",
    "        return {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "def infer_mlp_arch_from_state_dict(state_dict: Dict[str, torch.Tensor]) -> Tuple[int, int, List[int]]:\n",
    "    \"\"\"\n",
    "    Infer input_size, output_size, hidden_size list from a LinearModel state_dict.\n",
    "    Assumes keys like 'layers.0.weight', ..., 'output.weight'.\n",
    "    \"\"\"\n",
    "    sd = strip_module_prefix(state_dict)\n",
    "    # Collect layer weight keys\n",
    "    layer_keys = []\n",
    "    i = 0\n",
    "    while f\"layers.{i}.weight\" in sd:\n",
    "        layer_keys.append(f\"layers.{i}.weight\")\n",
    "        i += 1\n",
    "    if not layer_keys:\n",
    "        raise ValueError(\"Could not find any 'layers.i.weight' in state_dict. Is this a LinearModel?\")\n",
    "\n",
    "    first = sd[layer_keys[0]]\n",
    "    input_size = first.shape[1]   # in_features of first hidden layer\n",
    "    hidden_sizes = [sd[k].shape[0] for k in layer_keys]\n",
    "\n",
    "    if \"output.weight\" not in sd:\n",
    "        raise ValueError(\"Could not find 'output.weight' in state_dict.\")\n",
    "    output_size = sd[\"output.weight\"].shape[0]\n",
    "    return input_size, output_size, hidden_sizes\n",
    "\n",
    "\n",
    "def build_model_from_state_dict(state_dict_path: str, dropout_rate: float = 0.0, device: str = \"cpu\") -> Tuple[str, LinearModel]:\n",
    "    \"\"\"\n",
    "    Load a state_dict file, infer architecture, build a LinearModel, and load the state.\n",
    "    Returns (model_name_without_ext, model)\n",
    "    \"\"\"\n",
    "    model_name = os.path.splitext(os.path.basename(state_dict_path))[0]\n",
    "    raw = torch.load(state_dict_path, map_location=\"cpu\")\n",
    "    if isinstance(raw, dict) and \"state_dict\" in raw:\n",
    "        sd = raw[\"state_dict\"]\n",
    "    else:\n",
    "        sd = raw\n",
    "    sd = strip_module_prefix(sd)\n",
    "    in_sz, out_sz, hsz = infer_mlp_arch_from_state_dict(sd)\n",
    "    model = LinearModel(in_sz, out_sz, hidden_size=hsz, dropout_rate=dropout_rate)\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[WARN] While loading {model_name}: missing={missing}, unexpected={unexpected}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model_name, model\n",
    "\n",
    "\n",
    "def load_all_models(models_dir: str, device: str = \"cpu\") -> Dict[str, LinearModel]:\n",
    "    models = {}\n",
    "    for path in glob.glob(os.path.join(models_dir, \"*.pt\")) + glob.glob(os.path.join(models_dir, \"*.pth\")):\n",
    "        try:\n",
    "            name, model = build_model_from_state_dict(path, device=device)\n",
    "            models[name] = model\n",
    "            print(f\"Loaded: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Skip] {path}: {e}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# N_eff ROW pruning core\n",
    "# ----------------------------\n",
    "def neff_per_row(W: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    N_eff for each ROW (out_features) of W.\n",
    "    N_eff = floor( 1 / sum(p^2) ), where p is the L1-normalized row vector.\n",
    "    Returns a 1D int tensor of length out_features with keep-counts per row.\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    A = W.abs()\n",
    "    P = A / (A.sum(dim=1, keepdim=True) + eps)            # L1-normalize each row\n",
    "    neff = (1.0 / (P.pow(2).sum(dim=1) + eps)).floor()    # (out_features,)\n",
    "    neff = neff.to(torch.int64)\n",
    "    neff.clamp_(1, W.shape[1])                            # at least 1, at most in_features\n",
    "    return neff\n",
    "\n",
    "\n",
    "def mask_top_by_neff_rows(W: torch.Tensor, neff_rows: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build boolean mask of same shape as W, keeping top-N per ROW by |W|.\n",
    "    neff_rows: shape (out_features,), each entry is how many columns to keep in that row.\n",
    "    \"\"\"\n",
    "    A = W.abs()\n",
    "    out, in_ = A.shape\n",
    "    sorted_vals, sorted_idx = torch.sort(A, dim=1, descending=True)  # (out, in)\n",
    "    ranks = torch.arange(in_, device=W.device).unsqueeze(0).expand_as(sorted_idx)\n",
    "    keep_counts = neff_rows.view(-1, 1).expand_as(sorted_idx)\n",
    "    keep_sorted = ranks < keep_counts\n",
    "    mask = torch.zeros_like(W, dtype=torch.bool)\n",
    "    mask.scatter_(1, sorted_idx, keep_sorted)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LayerPruneRecord:\n",
    "    layer_name: str\n",
    "    shape: Tuple[int, int]\n",
    "    neff_1d: torch.Tensor     # 1D tensor of keep-counts (per row)\n",
    "    mask: torch.Tensor        # bool mask same shape as weight\n",
    "    weight_before: torch.Tensor\n",
    "    weight_after: torch.Tensor\n",
    "    kept_fraction: float      # fraction of weights kept (nonzero in mask)\n",
    "\n",
    "\n",
    "def prune_linear_by_neff_row(module: nn.Linear, renormalize: bool = False\n",
    "                            ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Row-prune a Linear layer using N_eff per row.\n",
    "    Returns: mask, neff, weight_before, weight_after\n",
    "    \"\"\"\n",
    "    W = module.weight.data.clone()\n",
    "    neff = neff_per_row(W)\n",
    "    mask = mask_top_by_neff_rows(W, neff)\n",
    "    W_pruned = W * mask\n",
    "    if renormalize:\n",
    "        # Preserve L1 per-row after pruning (avoid sign cancellation by using |.|)\n",
    "        row_sum = W_pruned.abs().sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "        W_pruned = W_pruned / row_sum\n",
    "    return mask, neff, W, W_pruned\n",
    "\n",
    "\n",
    "def prune_model_neff_rows(model: nn.Module, renormalize: bool = False\n",
    "                         ) -> Tuple[nn.Module, List[LayerPruneRecord]]:\n",
    "    \"\"\"\n",
    "    Copy the model, row-prune each nn.Linear by N_eff, and return (pruned_model, list_of_records).\n",
    "    \"\"\"\n",
    "    pruned = copy.deepcopy(model)\n",
    "    records: List[LayerPruneRecord] = []\n",
    "    for name, module in pruned.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask, neff, W_before, W_after = prune_linear_by_neff_row(module, renormalize=renormalize)\n",
    "            with torch.no_grad():\n",
    "                module.weight.copy_(W_after)\n",
    "            kept_fraction = float(mask.float().mean().item())\n",
    "            rec = LayerPruneRecord(\n",
    "                layer_name=name,\n",
    "                shape=tuple(W_before.shape),\n",
    "                neff_1d=neff.detach().cpu(),\n",
    "                mask=mask.detach().cpu(),\n",
    "                weight_before=W_before.detach().cpu(),\n",
    "                weight_after=W_after.detach().cpu(),\n",
    "                kept_fraction=kept_fraction,\n",
    "            )\n",
    "            records.append(rec)\n",
    "    return pruned, records\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Visualization helpers\n",
    "# ----------------------------\n",
    "def to_display_array(W: torch.Tensor, max_side: int = 512) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a 2D tensor to a 2D numpy array of magnitudes, possibly downscaled if very large.\n",
    "    \"\"\"\n",
    "    X = W.detach().cpu().abs().float()  # magnitude for visualization\n",
    "    H, Wd = X.shape\n",
    "    if max(H, Wd) <= max_side:\n",
    "        return X.numpy()\n",
    "    X_img = X.unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    scale_h = min(max_side, H)\n",
    "    scale_w = min(max_side, Wd)\n",
    "    X_small = F.interpolate(X_img, size=(scale_h, scale_w), mode=\"area\").squeeze().numpy()\n",
    "    return X_small\n",
    "\n",
    "\n",
    "def plot_heatmap(arr2d: np.ndarray, title: str, out_path: Optional[str] = None):\n",
    "    plt.figure()\n",
    "    plt.imshow(arr2d, aspect='auto')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    if out_path:\n",
    "        plt.savefig(out_path, dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def visualize_prune_records(records: List[LayerPruneRecord], out_dir: str, pdf_name: str = \"prune_report_row.pdf\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    pdf_path = os.path.join(out_dir, pdf_name)\n",
    "    with PdfPages(pdf_path) as pdf:\n",
    "        for rec in records:\n",
    "            W0 = rec.weight_before\n",
    "            M = rec.mask\n",
    "            Wp = rec.weight_after\n",
    "\n",
    "            A0 = to_display_array(W0)\n",
    "            Am = to_display_array(M.float())\n",
    "            Ap = to_display_array(Wp)\n",
    "\n",
    "            base = f\"{rec.layer_name.replace('.', '_')}_per_row\"\n",
    "            plot_heatmap(A0, f\"{rec.layer_name} (per_row) - Original |W|\", os.path.join(out_dir, base + \"_orig.png\"))\n",
    "            plot_heatmap(Am, f\"{rec.layer_name} (per_row) - Mask (1=keep)\", os.path.join(out_dir, base + \"_mask.png\"))\n",
    "            plot_heatmap(Ap, f\"{rec.layer_name} (per_row) - Pruned |W|\", os.path.join(out_dir, base + \"_pruned.png\"))\n",
    "\n",
    "            # Also add to the PDF\n",
    "            fig = plt.figure(figsize=(9, 7))\n",
    "            ax1 = fig.add_subplot(2, 2, 1)\n",
    "            im1 = ax1.imshow(A0, aspect='auto'); ax1.set_title(\"Original |W|\"); fig.colorbar(im1, ax=ax1)\n",
    "            ax2 = fig.add_subplot(2, 2, 2)\n",
    "            im2 = ax2.imshow(Am, aspect='auto'); ax2.set_title(\"Mask\"); fig.colorbar(im2, ax=ax2)\n",
    "            ax3 = fig.add_subplot(2, 1, 2)\n",
    "            im3 = ax3.imshow(Ap, aspect='auto'); ax3.set_title(f\"Pruned |W| (kept {rec.kept_fraction:.2%})\"); fig.colorbar(im3, ax=ax3)\n",
    "            fig.suptitle(f\"{rec.layer_name} - per_row - shape={rec.shape}\")\n",
    "            fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "    print(f\"[Saved] PDF report: {pdf_path}\")\n",
    "\n",
    "\n",
    "def save_prune_records(records: List[LayerPruneRecord], out_dir: str, model_name: str):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # Save a compact torch file\n",
    "    pack = []\n",
    "    for r in records:\n",
    "        pack.append({\n",
    "            \"layer_name\": r.layer_name,\n",
    "            \"shape\": r.shape,\n",
    "            \"kept_fraction\": r.kept_fraction,\n",
    "            \"neff_1d\": r.neff_1d,\n",
    "            \"mask\": r.mask,\n",
    "            \"weight_before\": r.weight_before,\n",
    "            \"weight_after\": r.weight_after,\n",
    "        })\n",
    "    torch.save({\"records\": pack}, os.path.join(out_dir, f\"{model_name}_row_prune_records.pt\"))\n",
    "    # Save a small JSON metadata (without tensors)\n",
    "    meta = [{\n",
    "        \"layer_name\": r.layer_name,\n",
    "        \"shape\": r.shape,\n",
    "        \"kept_fraction\": r.kept_fraction,\n",
    "        \"neff_1d_len\": int(r.neff_1d.numel()),\n",
    "    } for r in records]\n",
    "    with open(os.path.join(out_dir, f\"{model_name}_row_prune_meta.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Pattern / similarity utilities (row-pruning centric)\n",
    "# ----------------------------\n",
    "def canonical_reorder(W: torch.Tensor, axis: str = \"columns\", method: str = \"pc1\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Canonicalize order of columns/rows to mitigate permutations.\n",
    "    - axis: 'columns' or 'rows'\n",
    "    - method: 'pc1' (project along first singular vector), or 'l2'\n",
    "    Returns (W_reordered, order_idx)\n",
    "    \"\"\"\n",
    "    A = W.detach().cpu()\n",
    "    if axis == \"columns\":\n",
    "        if method == \"pc1\":\n",
    "            U, S, Vt = torch.linalg.svd(A.abs(), full_matrices=False)\n",
    "            v1 = Vt[0]\n",
    "            if v1.sum() < 0:\n",
    "                v1 = -v1\n",
    "            order = torch.argsort(v1)\n",
    "        elif method == \"l2\":\n",
    "            norms = torch.linalg.norm(A, dim=0)\n",
    "            order = torch.argsort(norms, descending=True)\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'pc1' or 'l2'\")\n",
    "        return A[:, order], order\n",
    "    elif axis == \"rows\":\n",
    "        if method == \"pc1\":\n",
    "            U, S, Vt = torch.linalg.svd(A.abs(), full_matrices=False)\n",
    "            u1 = U[:, 0]\n",
    "            if u1.sum() < 0:\n",
    "                u1 = -u1\n",
    "            order = torch.argsort(u1)\n",
    "        elif method == \"l2\":\n",
    "            norms = torch.linalg.norm(A, dim=1)\n",
    "            order = torch.argsort(norms, descending=True)\n",
    "        else:\n",
    "            raise ValueError(\"method must be 'pc1' or 'l2'\")\n",
    "        return A[order, :], order\n",
    "    else:\n",
    "        raise ValueError(\"axis must be 'columns' or 'rows'\")\n",
    "\n",
    "\n",
    "def canonicalize_both_axes(W: torch.Tensor, method: str = \"pc1\") -> torch.Tensor:\n",
    "    Wc, _ = canonical_reorder(W, axis=\"columns\", method=method)\n",
    "    Wc, _ = canonical_reorder(Wc, axis=\"rows\", method=method)\n",
    "    return Wc\n",
    "\n",
    "\n",
    "def resize_map_abs(W: torch.Tensor, size: Tuple[int, int] = (256, 256)) -> torch.Tensor:\n",
    "    \"\"\"Resize |W| to a fixed 2D map with area interpolation (scale-invariant comparison).\"\"\"\n",
    "    A = W.detach().cpu().abs().float().unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    R = F.interpolate(A, size=size, mode=\"area\").squeeze(0).squeeze(0)\n",
    "    return R\n",
    "\n",
    "\n",
    "def normalize_01(X: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    mn, mx = X.min(), X.max()\n",
    "    return (X - mn) / (mx - mn + eps)\n",
    "\n",
    "\n",
    "def cosine_similarity_flat(A: torch.Tensor, B: torch.Tensor, eps: float = 1e-12) -> float:\n",
    "    a = A.flatten().float()\n",
    "    b = B.flatten().float()\n",
    "    a = a / (a.norm() + eps)\n",
    "    b = b / (b.norm() + eps)\n",
    "    return float((a * b).sum().item())\n",
    "\n",
    "\n",
    "def mse_flat(A: torch.Tensor, B: torch.Tensor) -> float:\n",
    "    a = A.flatten().float()\n",
    "    b = B.flatten().float()\n",
    "    return float(F.mse_loss(a, b).item())\n",
    "\n",
    "\n",
    "def hist_emd_1d(a: torch.Tensor, b: torch.Tensor, bins: int = 64) -> float:\n",
    "    \"\"\"\n",
    "    Earth Mover's Distance between two 1D histograms (same bin edges).\n",
    "    Closed-form via L1 distance between CDFs.\n",
    "    \"\"\"\n",
    "    a = a.flatten().float()\n",
    "    b = b.flatten().float()\n",
    "    mx = max(a.max().item(), b.max().item())\n",
    "    edges_min = 0.0\n",
    "    edges_max = mx if mx > 0 else 1.0\n",
    "    ha = torch.histc(a, bins=bins, min=edges_min, max=edges_max)\n",
    "    hb = torch.histc(b, bins=bins, min=edges_min, max=edges_max)\n",
    "    ha = ha / (ha.sum() + 1e-12)\n",
    "    hb = hb / (hb.sum() + 1e-12)\n",
    "    cdfa = torch.cumsum(ha, dim=0)\n",
    "    cdfb = torch.cumsum(hb, dim=0)\n",
    "    emd = torch.sum(torch.abs(cdfa - cdfb)).item() / bins\n",
    "    return float(emd)\n",
    "\n",
    "\n",
    "def spearman_rank_corr(A: torch.Tensor, B: torch.Tensor, eps: float = 1e-12) -> float:\n",
    "    \"\"\"\n",
    "    Spearman rank correlation between flattened arrays (no SciPy).\n",
    "    \"\"\"\n",
    "    a = A.flatten()\n",
    "    b = B.flatten()\n",
    "    ra = torch.argsort(torch.argsort(a))\n",
    "    rb = torch.argsort(torch.argsort(b))\n",
    "    ra = ra.float(); rb = rb.float()\n",
    "    ra = (ra - ra.mean()) / (ra.std() + eps)\n",
    "    rb = (rb - rb.mean()) / (rb.std() + eps)\n",
    "    return float((ra * rb).mean().item())\n",
    "\n",
    "\n",
    "def greedy_row_alignment_cosine(W1: torch.Tensor, W2: torch.Tensor) -> Tuple[torch.Tensor, List[int]]:\n",
    "    \"\"\"\n",
    "    Permutation-aware greedy matching of ROWS by absolute cosine similarity.\n",
    "    Returns (W2_aligned, perm_idx) such that W2_aligned's rows match W1's row order.\n",
    "    Shapes must match on both dims.\n",
    "    \"\"\"\n",
    "    A = W1.detach().cpu()\n",
    "    B = W2.detach().cpu()\n",
    "    assert A.shape == B.shape\n",
    "    out, in_ = A.shape\n",
    "\n",
    "    def norm_rows(X):\n",
    "        return X / (X.norm(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "    An = norm_rows(A)\n",
    "    Bn = norm_rows(B)\n",
    "\n",
    "    S = torch.abs(An @ Bn.T)  # (out, out)\n",
    "\n",
    "    perm = [-1] * out\n",
    "    used_rows_A = set()\n",
    "    used_rows_B = set()\n",
    "\n",
    "    for _ in range(out):\n",
    "        S_masked = S.clone()\n",
    "        if used_rows_A:\n",
    "            S_masked[list(used_rows_A), :] = -1e9\n",
    "        if used_rows_B:\n",
    "            S_masked[:, list(used_rows_B)] = -1e9\n",
    "        i, j = torch.nonzero(S_masked == S_masked.max(), as_tuple=True)\n",
    "        if len(i) == 0:\n",
    "            break\n",
    "        i0 = int(i[0].item()); j0 = int(j[0].item())\n",
    "        perm[i0] = j0\n",
    "        used_rows_A.add(i0)\n",
    "        used_rows_B.add(j0)\n",
    "\n",
    "    perm_idx = torch.tensor(perm, dtype=torch.long)\n",
    "    Baligned = B[perm_idx, :]\n",
    "    return Baligned, perm\n",
    "\n",
    "\n",
    "def compare_weight_patterns_row_focus(\n",
    "    W1: torch.Tensor,\n",
    "    W2: torch.Tensor,\n",
    "    canonical_axis: str = \"columns\",     # columns canonicalization is most important for row-pruned analysis\n",
    "    canonical_method: str = \"pc1\",\n",
    "    resize_to: Tuple[int, int] = (256, 256),\n",
    "    try_row_alignment_if_same_shape: bool = True\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compare two layers' weight matrices for ROW-pruned analysis.\n",
    "    Steps:\n",
    "      1) Canonical reorder (usually 'columns') to mitigate feature permutations.\n",
    "      2) If shapes equal, align ROWS greedily by cosine.\n",
    "      3) Resize |W| maps to common size.\n",
    "      4) Compute metrics.\n",
    "    \"\"\"\n",
    "    A = W1.detach().cpu()\n",
    "    B = W2.detach().cpu()\n",
    "\n",
    "    # (1) canonical reorder\n",
    "    if canonical_axis == \"both\":\n",
    "        A = canonicalize_both_axes(A, method=canonical_method)\n",
    "        B = canonicalize_both_axes(B, method=canonical_method)\n",
    "    elif canonical_axis in (\"columns\", \"rows\"):\n",
    "        A, _ = canonical_reorder(A, axis=canonical_axis, method=canonical_method)\n",
    "        B, _ = canonical_reorder(B, axis=canonical_axis, method=canonical_method)\n",
    "    else:\n",
    "        raise ValueError(\"canonical_axis must be 'columns', 'rows', or 'both'.\")\n",
    "\n",
    "    # (2) optional permutation alignment on ROWS if same shape\n",
    "    if try_row_alignment_if_same_shape and A.shape == B.shape:\n",
    "        Baligned, permr = greedy_row_alignment_cosine(A, B)\n",
    "        B = Baligned\n",
    "\n",
    "    # (3) resize to common map (scale-agnostic)\n",
    "    Ra = resize_map_abs(A, size=resize_to)\n",
    "    Rb = resize_map_abs(B, size=resize_to)\n",
    "\n",
    "    # normalize to [0,1] for stable metrics\n",
    "    Ra_n = normalize_01(Ra)\n",
    "    Rb_n = normalize_01(Rb)\n",
    "\n",
    "    # (4) metrics\n",
    "    metrics = {\n",
    "        \"cosine\": cosine_similarity_flat(Ra_n, Rb_n),\n",
    "        \"ncc\": float(torch.mean((Ra_n - Ra_n.mean()) * (Rb_n - Rb_n.mean())).item() /\n",
    "                     ((Ra_n.std() + 1e-12) * (Rb_n.std() + 1e-12))),\n",
    "        \"mse\": mse_flat(Ra_n, Rb_n),\n",
    "        \"emd_hist\": hist_emd_1d(Ra_n, Rb_n, bins=64),\n",
    "        \"spearman\": spearman_rank_corr(Ra_n, Rb_n),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def visualize_two_layers_side_by_side(\n",
    "    W1: torch.Tensor, W2: torch.Tensor, title1: str, title2: str, out_path: str, resize_to=(256, 256)\n",
    "):\n",
    "    A = resize_map_abs(W1, size=resize_to).numpy()\n",
    "    B = resize_map_abs(W2, size=resize_to).numpy()\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    ax1 = plt.subplot(1, 2, 1); im1 = ax1.imshow(A, aspect='auto'); ax1.set_title(title1); plt.colorbar(im1, ax=ax1)\n",
    "    ax2 = plt.subplot(1, 2, 2); im2 = ax2.imshow(B, aspect='auto'); ax2.set_title(title2); plt.colorbar(im2, ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def verdict_from_metrics(m, w_cos=0.45, w_ncc=0.2, w_spear=0.15, w_emd=0.1, w_mse=0.1):\n",
    "    \"\"\"\n",
    "    Combine metrics into a single score in [0,1] (heuristic).\n",
    "    \"\"\"\n",
    "    cos = max(0.0, min(1.0, m[\"cosine\"]))          # [0,1]\n",
    "    ncc = (m[\"ncc\"] + 1.0) / 2.0                   # [-1,1] -> [0,1]\n",
    "    spear = (m[\"spearman\"] + 1.0) / 2.0            # [-1,1] -> [0,1]\n",
    "    emd = 1.0 - max(0.0, min(1.0, m[\"emd_hist\"]))  # lower EMD is better\n",
    "    mse = 1.0 - min(m[\"mse\"] / 0.1, 1.0)           # cap at 0.1 as “bad”\n",
    "    score = w_cos*cos + w_ncc*ncc + w_spear*spear + w_emd*emd + w_mse*mse\n",
    "    if score >= 0.75:\n",
    "        label = \"SIMILAR\"\n",
    "    elif score >= 0.6:\n",
    "        label = \"BORDERLINE\"\n",
    "    else:\n",
    "        label = \"DIFFERENT\"\n",
    "    return float(score), label\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Mask similarity for ROW-pruned models\n",
    "# ----------------------------\n",
    "def jaccard_on_resized_masks(M1: torch.Tensor, M2: torch.Tensor, size=(256, 256)) -> float:\n",
    "    \"\"\"\n",
    "    Compare binary masks of different shapes by resizing with area interpolation,\n",
    "    then threshold at 0.5 and compute Jaccard = |A∩B|/|A∪B|.\n",
    "    \"\"\"\n",
    "    A = resize_map_abs(M1.float(), size=size)  # values in [0,1] due to area interp\n",
    "    B = resize_map_abs(M2.float(), size=size)\n",
    "    Ab = (A > 0.5)\n",
    "    Bb = (B > 0.5)\n",
    "    inter = (Ab & Bb).sum().item()\n",
    "    union = (Ab | Bb).sum().item()\n",
    "    return float(inter / (union + 1e-12))\n",
    "\n",
    "\n",
    "def compare_prune_masks_row_focus(recordsA, recordsB, out_dir: str, size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Compare masks layer-by-layer (up to min depth) and write a JSON summary.\n",
    "    - Canonicalize COLUMNS (since row pruning picks columns per row).\n",
    "    - If shapes match, greedily align ROWS (using masks) before Jaccard.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    n = min(len(recordsA), len(recordsB))\n",
    "    out = {}\n",
    "    for i in range(n):\n",
    "        rA, rB = recordsA[i], recordsB[i]\n",
    "        MA = rA.mask.float()\n",
    "        MB = rB.mask.float()\n",
    "\n",
    "        # Canonicalize columns to mitigate feature permutations\n",
    "        MA_c, _ = canonical_reorder(MA, axis=\"columns\", method=\"pc1\")\n",
    "        MB_c, _ = canonical_reorder(MB, axis=\"columns\", method=\"pc1\")\n",
    "\n",
    "        # If shapes equal, align rows greedily (on mask rows)\n",
    "        if MA_c.shape == MB_c.shape:\n",
    "            # reuse greedy alignment by treating masks as weights\n",
    "            MB_aligned, _ = greedy_row_alignment_cosine(MA_c, MB_c)\n",
    "            jac = jaccard_on_resized_masks(MA_c.bool(), MB_aligned.bool(), size=size)\n",
    "        else:\n",
    "            jac = jaccard_on_resized_masks(MA_c.bool(), MB_c.bool(), size=size)\n",
    "\n",
    "        out[f\"layer_{i}:{rA.layer_name} vs {rB.layer_name}\"] = {\n",
    "            \"shapeA\": list(rA.shape), \"shapeB\": list(rB.shape),\n",
    "            \"kept_frac_A\": rA.kept_fraction, \"kept_frac_B\": rB.kept_fraction,\n",
    "            \"jaccard_mask\": jac\n",
    "        }\n",
    "        # optional side-by-side mask heatmaps\n",
    "        visualize_two_layers_side_by_side(\n",
    "            MA_c, MB_c,\n",
    "            f\"Mask {rA.layer_name}\", f\"Mask {rB.layer_name}\",\n",
    "            os.path.join(out_dir, f\"mask_compare_{i}.png\"),\n",
    "            resize_to=size\n",
    "        )\n",
    "    with open(os.path.join(out_dir, \"mask_compare_summary.json\"), \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"[Saved] Mask comparison → {os.path.join(out_dir, 'mask_compare_summary.json')}\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# End-to-end pipeline helpers (ROW focus)\n",
    "# ----------------------------\n",
    "def run_row_prune_and_save(model_name: str, model: nn.Module, out_root: str, renormalize: bool = False):\n",
    "    print(f\"Pruning {model_name} [per_row] ...\")\n",
    "    pruned_model, records = prune_model_neff_rows(model, renormalize=renormalize)\n",
    "    out_dir = os.path.join(out_root, model_name)\n",
    "    save_prune_records(records, out_dir, model_name)\n",
    "    visualize_prune_records(records, out_dir, pdf_name=f\"{model_name}_row_prune_report.pdf\")\n",
    "    return pruned_model, records\n",
    "\n",
    "\n",
    "def pick_linear_layers(model: nn.Module) -> List[Tuple[str, nn.Linear]]:\n",
    "    layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            layers.append((name, module))\n",
    "    return layers\n",
    "\n",
    "\n",
    "def compare_two_models_row_focus(\n",
    "    name_a: str, model_a: nn.Module,\n",
    "    name_b: str, model_b: nn.Module,\n",
    "    out_dir: str,\n",
    "    canonical_axis: str = \"columns\",\n",
    "    canonical_method: str = \"pc1\"\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compare the corresponding Linear layers of two models by index order (ROW-focused).\n",
    "    - Canonicalize COLUMNS (important for row-pruned analysis).\n",
    "    - If shapes equal, align ROWS greedily.\n",
    "    - Resize for cross-width comparison.\n",
    "    Saves side-by-side heatmaps and returns a metrics dict (plus verdict).\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    layers_a = pick_linear_layers(model_a)\n",
    "    layers_b = pick_linear_layers(model_b)\n",
    "    n = min(len(layers_a), len(layers_b))\n",
    "    results = {}\n",
    "    for i in range(n):\n",
    "        (name1, la), (name2, lb) = layers_a[i], layers_b[i]\n",
    "        W1 = la.weight.data\n",
    "        W2 = lb.weight.data\n",
    "        metrics = compare_weight_patterns_row_focus(\n",
    "            W1, W2,\n",
    "            canonical_axis=canonical_axis,\n",
    "            canonical_method=canonical_method,\n",
    "            resize_to=(256, 256),\n",
    "            try_row_alignment_if_same_shape=True\n",
    "        )\n",
    "        score, label = verdict_from_metrics(metrics)\n",
    "        metrics[\"combo_score\"] = round(score, 4)\n",
    "        metrics[\"verdict\"] = label\n",
    "        results[f\"layer_{i}:{name1} vs {name2}\"] = metrics\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"compare_{i}_{name1.replace('.','_')}_VS_{name2.replace('.','_')}.png\")\n",
    "        visualize_two_layers_side_by_side(W1, W2, f\"{name_a}:{name1}\", f\"{name_b}:{name2}\", out_path)\n",
    "        print(f\"Compared layer {i}: {name1} vs {name2} -> {metrics}\")\n",
    "\n",
    "    with open(os.path.join(out_dir, f\"{name_a}_VS_{name_b}_row_metrics.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    return results\n",
    "\n",
    "\n",
    "def model_level_similarity_row_focus(models: Dict[str, nn.Module], out_dir: str, canonical_axis=\"columns\"):\n",
    "    \"\"\"\n",
    "    Compute an NxN matrix of average cosine similarity (after canonicalization & resizing)\n",
    "    across corresponding layers of each pair of models (ROW-focused). Saves a heatmap.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    names = sorted(models.keys())\n",
    "    N = len(names)\n",
    "    mat = np.zeros((N, N), dtype=np.float32)\n",
    "\n",
    "    def layerwise_avg_cos(mA, mB):\n",
    "        la = pick_linear_layers(mA); lb = pick_linear_layers(mB)\n",
    "        n = min(len(la), len(lb))\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "        cs = []\n",
    "        for i in range(n):\n",
    "            W1 = la[i][1].weight.data\n",
    "            W2 = lb[i][1].weight.data\n",
    "            mets = compare_weight_patterns_row_focus(\n",
    "                W1, W2, canonical_axis=canonical_axis, canonical_method=\"pc1\",\n",
    "                resize_to=(256, 256), try_row_alignment_if_same_shape=True\n",
    "            )\n",
    "            cs.append(mets[\"cosine\"])\n",
    "        return float(np.mean(cs))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                mat[i, j] = 1.0\n",
    "            else:\n",
    "                mat[i, j] = layerwise_avg_cos(models[names[i]], models[names[j]])\n",
    "\n",
    "    # save heatmap\n",
    "    plt.figure(figsize=(1+0.4*N, 1+0.4*N))\n",
    "    plt.imshow(mat, vmin=0, vmax=1, aspect='equal')\n",
    "    plt.xticks(range(N), names, rotation=45, ha='right', fontsize=8)\n",
    "    plt.yticks(range(N), names, fontsize=8)\n",
    "    plt.colorbar(label=\"Avg. cosine over layers (row-focus)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, \"row_model_similarity_matrix.png\")\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {out_path}\")\n",
    "\n",
    "    # also dump numeric matrix\n",
    "    with open(os.path.join(out_dir, \"row_model_similarity_matrix.json\"), \"w\") as f:\n",
    "        json.dump({\"names\": names, \"matrix\": mat.tolist()}, f, indent=2)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main: end-to-end (ROW pruning only)\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = \"cuda\"  # change to \"cuda\" if desired and available\n",
    "\n",
    "    # 1) Load all models from ./models\n",
    "    models_dir = \"./models\"\n",
    "    models = load_all_models(models_dir, device=device)\n",
    "    if not models:\n",
    "        print(\"[INFO] No models found in ./models. Place your .pt/.pth state_dicts there.\")\n",
    "\n",
    "    # 2) Row-prune all models and export artifacts\n",
    "    out_root = \"./prune_outputs\"\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    pruned_models = {}\n",
    "    records_map: Dict[str, List[LayerPruneRecord]] = {}\n",
    "    for name, mdl in models.items():\n",
    "        pruned_mdl, recs = run_row_prune_and_save(name, mdl, out_root, renormalize=False)\n",
    "        pruned_models[name] = pruned_mdl\n",
    "        records_map[name] = recs\n",
    "\n",
    "    # 3) Compare two pruned models (edit names below to ones you actually have)\n",
    "    names = sorted(list(pruned_models.keys()))\n",
    "    if len(names) >= 2:\n",
    "        A_name = names[0]\n",
    "        B_name = names[1]\n",
    "        print(f\"\\n[COMPARE ROW-PRUNED] {A_name}  VS  {B_name}\")\n",
    "        cmp_out_dir = os.path.join(out_root, f\"compare_{A_name}_VS_{B_name}_ROWPRUNE\")\n",
    "\n",
    "        # Compare pruned weights (ROW-focused)\n",
    "        _ = compare_two_models_row_focus(\n",
    "            A_name, pruned_models[A_name],\n",
    "            B_name, pruned_models[B_name],\n",
    "            out_dir=cmp_out_dir,\n",
    "            canonical_axis=\"columns\",   # robust to input-feature permutations\n",
    "            canonical_method=\"pc1\"\n",
    "        )\n",
    "\n",
    "        # Also compare masks derived during row pruning\n",
    "        compare_prune_masks_row_focus(records_map[A_name], records_map[B_name], out_dir=cmp_out_dir, size=(256, 256))\n",
    "    else:\n",
    "        print(\"[INFO] Need at least two models in ./models to run comparison.\")\n",
    "\n",
    "    # 4) All-vs-all similarity (ROW-focused) across pruned models\n",
    "    if len(names) >= 2:\n",
    "        model_level_similarity_row_focus(pruned_models, out_dir=os.path.join(out_root, \"pairwise_row_model_similarity\"), canonical_axis=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ef70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
