{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c5f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset registry ---------------------------------------------------------\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_loaders(dataset_name, batch_size=128, test_batch_size=1000, data_root='./data'):\n",
    "    \"\"\"\n",
    "    Returns: train_loader, test_loader, input_size, num_classes, meta (dict)\n",
    "    \"\"\"\n",
    "    name = dataset_name.lower()\n",
    "    meta = {}\n",
    "\n",
    "    # Generic normalizations (safe defaults). If you want canonical stats, compute them once.\n",
    "    NORM_1C = transforms.Normalize((0.5,), (0.5,))\n",
    "    NORM_3C = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    if name == 'mnist':\n",
    "        # (You already have this; included for completeness.)\n",
    "        tfm = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        train = datasets.MNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.MNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'fashionmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.FashionMNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.FashionMNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'kmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.KMNIST(data_root, train=True, download=True, transform=tfm)\n",
    "        test  = datasets.KMNIST(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name in ('emnist_balanced', 'emnist'):\n",
    "        # EMNIST Balanced has 47 classes. If digits look rotated, add a Rotate(90) or permute.\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.EMNIST(data_root, split='balanced', train=True, download=True, transform=tfm)\n",
    "        test  = datasets.EMNIST(data_root, split='balanced', train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 47\n",
    "        meta['note'] = 'EMNIST images can appear rotated; for visualization add a 90-degree rotate.'\n",
    "\n",
    "    elif name == 'qmnist':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_1C])\n",
    "        train = datasets.QMNIST(data_root, what='train', download=True, transform=tfm)\n",
    "        test  = datasets.QMNIST(data_root, what='test',  download=True, transform=tfm)\n",
    "        inp, ncls = 28*28, 10\n",
    "\n",
    "    elif name == 'svhn':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.SVHN(data_root, split='train', download=True, transform=tfm)\n",
    "        test  = datasets.SVHN(data_root, split='test',  download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 10\n",
    "\n",
    "    elif name == 'cifar10':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.CIFAR10(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR10(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 10\n",
    "\n",
    "    elif name == 'cifar100':\n",
    "        tfm = transforms.Compose([transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.CIFAR100(data_root, train=True,  download=True, transform=tfm)\n",
    "        test  = datasets.CIFAR100(data_root, train=False, download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 100\n",
    "\n",
    "    elif name in ('stl10', 'stl10_32'):\n",
    "        # Downsample to 32x32 to keep input dim manageable for MLPs.\n",
    "        tfm = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                  transforms.ToTensor(), NORM_3C])\n",
    "        train = datasets.STL10(data_root, split='train', download=True, transform=tfm)\n",
    "        test  = datasets.STL10(data_root, split='test',  download=True, transform=tfm)\n",
    "        inp, ncls = 32*32*3, 10\n",
    "        meta['note'] = 'Original STL10 is 96x96; here we resize to 32x32 for MLPs.'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test,  batch_size=test_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, test_loader, inp, ncls, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615362bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fashionmnist] device: cuda\n",
      "\n",
      "=== Training Tiny_Underfit on fashionmnist ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_64000\\3823726318.py:344: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_64000\\3823726318.py:264: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fashionmnist][Tiny_Underfit] Epoch 001/010 | train_acc=0.1274 test_acc=0.1853\n",
      "[fashionmnist][Tiny_Underfit] Epoch 002/010 | train_acc=0.2533 test_acc=0.3552\n",
      "[fashionmnist][Tiny_Underfit] Epoch 003/010 | train_acc=0.3599 test_acc=0.3692\n",
      "[fashionmnist][Tiny_Underfit] Epoch 004/010 | train_acc=0.3815 test_acc=0.3941\n",
      "[fashionmnist][Tiny_Underfit] Epoch 005/010 | train_acc=0.4006 test_acc=0.4175\n",
      "[fashionmnist][Tiny_Underfit] Epoch 006/010 | train_acc=0.4188 test_acc=0.4310\n",
      "[fashionmnist][Tiny_Underfit] Epoch 007/010 | train_acc=0.4380 test_acc=0.4574\n",
      "[fashionmnist][Tiny_Underfit] Epoch 008/010 | train_acc=0.4571 test_acc=0.4776\n",
      "[fashionmnist][Tiny_Underfit] Epoch 009/010 | train_acc=0.4766 test_acc=0.4913\n",
      "[fashionmnist][Tiny_Underfit] Epoch 010/010 | train_acc=0.4924 test_acc=0.4959\n",
      "Best test acc (pre-prune): 0.4959  -> saved to checkpoints_cnn_neff\\fashionmnist\\Tiny_Underfit\\best_preprune.pt\n",
      "\n",
      "=== Training Deep_Narrow on fashionmnist ===\n",
      "[fashionmnist][Deep_Narrow] Epoch 001/015 | train_acc=0.2406 test_acc=0.5730\n",
      "[fashionmnist][Deep_Narrow] Epoch 002/015 | train_acc=0.4909 test_acc=0.6939\n",
      "[fashionmnist][Deep_Narrow] Epoch 003/015 | train_acc=0.6049 test_acc=0.7483\n",
      "[fashionmnist][Deep_Narrow] Epoch 004/015 | train_acc=0.6671 test_acc=0.7775\n",
      "[fashionmnist][Deep_Narrow] Epoch 005/015 | train_acc=0.7052 test_acc=0.7986\n",
      "[fashionmnist][Deep_Narrow] Epoch 006/015 | train_acc=0.7342 test_acc=0.8093\n",
      "[fashionmnist][Deep_Narrow] Epoch 007/015 | train_acc=0.7521 test_acc=0.8172\n",
      "[fashionmnist][Deep_Narrow] Epoch 008/015 | train_acc=0.7612 test_acc=0.8252\n",
      "[fashionmnist][Deep_Narrow] Epoch 009/015 | train_acc=0.7754 test_acc=0.8315\n",
      "[fashionmnist][Deep_Narrow] Epoch 010/015 | train_acc=0.7834 test_acc=0.8376\n",
      "[fashionmnist][Deep_Narrow] Epoch 011/015 | train_acc=0.7928 test_acc=0.8391\n",
      "[fashionmnist][Deep_Narrow] Epoch 012/015 | train_acc=0.7959 test_acc=0.8425\n",
      "[fashionmnist][Deep_Narrow] Epoch 013/015 | train_acc=0.8024 test_acc=0.8477\n",
      "[fashionmnist][Deep_Narrow] Epoch 014/015 | train_acc=0.8040 test_acc=0.8512\n",
      "[fashionmnist][Deep_Narrow] Epoch 015/015 | train_acc=0.8087 test_acc=0.8537\n",
      "Best test acc (pre-prune): 0.8537  -> saved to checkpoints_cnn_neff\\fashionmnist\\Deep_Narrow\\best_preprune.pt\n",
      "\n",
      "=== Training Balanced on fashionmnist ===\n",
      "[fashionmnist][Balanced] Epoch 001/015 | train_acc=0.6019 test_acc=0.7758\n",
      "[fashionmnist][Balanced] Epoch 002/015 | train_acc=0.7468 test_acc=0.8166\n",
      "[fashionmnist][Balanced] Epoch 003/015 | train_acc=0.7833 test_acc=0.8351\n",
      "[fashionmnist][Balanced] Epoch 004/015 | train_acc=0.8081 test_acc=0.8494\n",
      "[fashionmnist][Balanced] Epoch 005/015 | train_acc=0.8259 test_acc=0.8611\n",
      "[fashionmnist][Balanced] Epoch 006/015 | train_acc=0.8356 test_acc=0.8676\n",
      "[fashionmnist][Balanced] Epoch 007/015 | train_acc=0.8464 test_acc=0.8687\n",
      "[fashionmnist][Balanced] Epoch 008/015 | train_acc=0.8512 test_acc=0.8744\n",
      "[fashionmnist][Balanced] Epoch 009/015 | train_acc=0.8575 test_acc=0.8772\n",
      "[fashionmnist][Balanced] Epoch 010/015 | train_acc=0.8636 test_acc=0.8834\n",
      "[fashionmnist][Balanced] Epoch 011/015 | train_acc=0.8672 test_acc=0.8852\n",
      "[fashionmnist][Balanced] Epoch 012/015 | train_acc=0.8698 test_acc=0.8867\n",
      "[fashionmnist][Balanced] Epoch 013/015 | train_acc=0.8743 test_acc=0.8921\n",
      "[fashionmnist][Balanced] Epoch 014/015 | train_acc=0.8797 test_acc=0.8909\n",
      "[fashionmnist][Balanced] Epoch 015/015 | train_acc=0.8803 test_acc=0.8937\n",
      "Best test acc (pre-prune): 0.8937  -> saved to checkpoints_cnn_neff\\fashionmnist\\Balanced\\best_preprune.pt\n",
      "\n",
      "=== Training Balanced_Deep on fashionmnist ===\n",
      "[fashionmnist][Balanced_Deep] Epoch 001/020 | train_acc=0.6561 test_acc=0.8103\n",
      "[fashionmnist][Balanced_Deep] Epoch 002/020 | train_acc=0.8008 test_acc=0.8447\n",
      "[fashionmnist][Balanced_Deep] Epoch 003/020 | train_acc=0.8347 test_acc=0.8635\n",
      "[fashionmnist][Balanced_Deep] Epoch 004/020 | train_acc=0.8496 test_acc=0.8733\n",
      "[fashionmnist][Balanced_Deep] Epoch 005/020 | train_acc=0.8591 test_acc=0.8786\n",
      "[fashionmnist][Balanced_Deep] Epoch 006/020 | train_acc=0.8676 test_acc=0.8841\n",
      "[fashionmnist][Balanced_Deep] Epoch 007/020 | train_acc=0.8754 test_acc=0.8883\n",
      "[fashionmnist][Balanced_Deep] Epoch 008/020 | train_acc=0.8800 test_acc=0.8927\n",
      "[fashionmnist][Balanced_Deep] Epoch 009/020 | train_acc=0.8847 test_acc=0.8955\n",
      "[fashionmnist][Balanced_Deep] Epoch 010/020 | train_acc=0.8871 test_acc=0.8977\n",
      "[fashionmnist][Balanced_Deep] Epoch 011/020 | train_acc=0.8906 test_acc=0.8972\n",
      "[fashionmnist][Balanced_Deep] Epoch 012/020 | train_acc=0.8930 test_acc=0.9004\n",
      "[fashionmnist][Balanced_Deep] Epoch 013/020 | train_acc=0.8967 test_acc=0.9030\n",
      "[fashionmnist][Balanced_Deep] Epoch 014/020 | train_acc=0.8986 test_acc=0.9047\n",
      "[fashionmnist][Balanced_Deep] Epoch 015/020 | train_acc=0.9005 test_acc=0.9066\n",
      "[fashionmnist][Balanced_Deep] Epoch 016/020 | train_acc=0.9008 test_acc=0.9081\n",
      "[fashionmnist][Balanced_Deep] Epoch 017/020 | train_acc=0.9047 test_acc=0.9080\n",
      "[fashionmnist][Balanced_Deep] Epoch 018/020 | train_acc=0.9054 test_acc=0.9103\n",
      "[fashionmnist][Balanced_Deep] Epoch 019/020 | train_acc=0.9069 test_acc=0.9104\n",
      "[fashionmnist][Balanced_Deep] Epoch 020/020 | train_acc=0.9076 test_acc=0.9112\n",
      "Best test acc (pre-prune): 0.9112  -> saved to checkpoints_cnn_neff\\fashionmnist\\Balanced_Deep\\best_preprune.pt\n",
      "\n",
      "=== Training Wide on fashionmnist ===\n",
      "[fashionmnist][Wide] Epoch 001/030 | train_acc=0.8520 test_acc=0.8575\n",
      "[fashionmnist][Wide] Epoch 002/030 | train_acc=0.8999 test_acc=0.8504\n",
      "[fashionmnist][Wide] Epoch 003/030 | train_acc=0.9138 test_acc=0.8971\n",
      "[fashionmnist][Wide] Epoch 004/030 | train_acc=0.9233 test_acc=0.8759\n",
      "[fashionmnist][Wide] Epoch 005/030 | train_acc=0.9314 test_acc=0.9063\n",
      "[fashionmnist][Wide] Epoch 006/030 | train_acc=0.9397 test_acc=0.9040\n",
      "[fashionmnist][Wide] Epoch 007/030 | train_acc=0.9446 test_acc=0.8722\n",
      "[fashionmnist][Wide] Epoch 008/030 | train_acc=0.9513 test_acc=0.9038\n",
      "[fashionmnist][Wide] Epoch 009/030 | train_acc=0.9581 test_acc=0.9133\n",
      "[fashionmnist][Wide] Epoch 010/030 | train_acc=0.9636 test_acc=0.9004\n",
      "[fashionmnist][Wide] Epoch 011/030 | train_acc=0.9674 test_acc=0.9200\n",
      "[fashionmnist][Wide] Epoch 012/030 | train_acc=0.9723 test_acc=0.9134\n",
      "[fashionmnist][Wide] Epoch 013/030 | train_acc=0.9752 test_acc=0.9054\n",
      "[fashionmnist][Wide] Epoch 014/030 | train_acc=0.9812 test_acc=0.9029\n",
      "[fashionmnist][Wide] Epoch 015/030 | train_acc=0.9847 test_acc=0.9051\n",
      "[fashionmnist][Wide] Epoch 016/030 | train_acc=0.9867 test_acc=0.8854\n",
      "[fashionmnist][Wide] Epoch 017/030 | train_acc=0.9895 test_acc=0.9154\n",
      "[fashionmnist][Wide] Epoch 018/030 | train_acc=0.9907 test_acc=0.9160\n",
      "[fashionmnist][Wide] Epoch 019/030 | train_acc=0.9926 test_acc=0.9055\n",
      "[fashionmnist][Wide] Epoch 020/030 | train_acc=0.9942 test_acc=0.8923\n",
      "[fashionmnist][Wide] Epoch 021/030 | train_acc=0.9947 test_acc=0.9126\n",
      "[fashionmnist][Wide] Epoch 022/030 | train_acc=0.9952 test_acc=0.9087\n",
      "[fashionmnist][Wide] Epoch 023/030 | train_acc=0.9945 test_acc=0.9121\n",
      "[fashionmnist][Wide] Epoch 024/030 | train_acc=0.9944 test_acc=0.8983\n",
      "[fashionmnist][Wide] Epoch 025/030 | train_acc=0.9977 test_acc=0.9185\n",
      "[fashionmnist][Wide] Epoch 026/030 | train_acc=0.9987 test_acc=0.8866\n",
      "[fashionmnist][Wide] Epoch 027/030 | train_acc=0.9911 test_acc=0.9046\n",
      "[fashionmnist][Wide] Epoch 028/030 | train_acc=0.9978 test_acc=0.9200\n",
      "[fashionmnist][Wide] Epoch 029/030 | train_acc=0.9998 test_acc=0.9265\n",
      "[fashionmnist][Wide] Epoch 030/030 | train_acc=1.0000 test_acc=0.9242\n",
      "Best test acc (pre-prune): 0.9265  -> saved to checkpoints_cnn_neff\\fashionmnist\\Wide\\best_preprune.pt\n",
      "[cifar10] device: cuda\n",
      "\n",
      "=== Training Tiny_Underfit on cifar10 ===\n",
      "[cifar10][Tiny_Underfit] Epoch 001/010 | train_acc=0.1605 test_acc=0.2288\n",
      "[cifar10][Tiny_Underfit] Epoch 002/010 | train_acc=0.2430 test_acc=0.2638\n",
      "[cifar10][Tiny_Underfit] Epoch 003/010 | train_acc=0.2659 test_acc=0.2842\n",
      "[cifar10][Tiny_Underfit] Epoch 004/010 | train_acc=0.2830 test_acc=0.2958\n",
      "[cifar10][Tiny_Underfit] Epoch 005/010 | train_acc=0.2951 test_acc=0.3094\n",
      "[cifar10][Tiny_Underfit] Epoch 006/010 | train_acc=0.3061 test_acc=0.3186\n",
      "[cifar10][Tiny_Underfit] Epoch 007/010 | train_acc=0.3168 test_acc=0.3287\n",
      "[cifar10][Tiny_Underfit] Epoch 008/010 | train_acc=0.3237 test_acc=0.3296\n",
      "[cifar10][Tiny_Underfit] Epoch 009/010 | train_acc=0.3295 test_acc=0.3390\n",
      "[cifar10][Tiny_Underfit] Epoch 010/010 | train_acc=0.3373 test_acc=0.3492\n",
      "Best test acc (pre-prune): 0.3492  -> saved to checkpoints_cnn_neff\\cifar10\\Tiny_Underfit\\best_preprune.pt\n",
      "\n",
      "=== Training Deep_Narrow on cifar10 ===\n",
      "[cifar10][Deep_Narrow] Epoch 001/015 | train_acc=0.1166 test_acc=0.1758\n",
      "[cifar10][Deep_Narrow] Epoch 002/015 | train_acc=0.1599 test_acc=0.1991\n",
      "[cifar10][Deep_Narrow] Epoch 003/015 | train_acc=0.1888 test_acc=0.2430\n",
      "[cifar10][Deep_Narrow] Epoch 004/015 | train_acc=0.2134 test_acc=0.2870\n",
      "[cifar10][Deep_Narrow] Epoch 005/015 | train_acc=0.2358 test_acc=0.3239\n",
      "[cifar10][Deep_Narrow] Epoch 006/015 | train_acc=0.2530 test_acc=0.3461\n",
      "[cifar10][Deep_Narrow] Epoch 007/015 | train_acc=0.2654 test_acc=0.3638\n",
      "[cifar10][Deep_Narrow] Epoch 008/015 | train_acc=0.2817 test_acc=0.3783\n",
      "[cifar10][Deep_Narrow] Epoch 009/015 | train_acc=0.2873 test_acc=0.3860\n",
      "[cifar10][Deep_Narrow] Epoch 010/015 | train_acc=0.2999 test_acc=0.3984\n",
      "[cifar10][Deep_Narrow] Epoch 011/015 | train_acc=0.3091 test_acc=0.4076\n",
      "[cifar10][Deep_Narrow] Epoch 012/015 | train_acc=0.3169 test_acc=0.4117\n",
      "[cifar10][Deep_Narrow] Epoch 013/015 | train_acc=0.3262 test_acc=0.4187\n",
      "[cifar10][Deep_Narrow] Epoch 014/015 | train_acc=0.3301 test_acc=0.4261\n",
      "[cifar10][Deep_Narrow] Epoch 015/015 | train_acc=0.3385 test_acc=0.4307\n",
      "Best test acc (pre-prune): 0.4307  -> saved to checkpoints_cnn_neff\\cifar10\\Deep_Narrow\\best_preprune.pt\n",
      "\n",
      "=== Training Balanced on cifar10 ===\n",
      "[cifar10][Balanced] Epoch 001/015 | train_acc=0.2539 test_acc=0.3900\n",
      "[cifar10][Balanced] Epoch 002/015 | train_acc=0.3269 test_acc=0.4392\n",
      "[cifar10][Balanced] Epoch 003/015 | train_acc=0.3604 test_acc=0.4720\n",
      "[cifar10][Balanced] Epoch 004/015 | train_acc=0.3892 test_acc=0.4852\n",
      "[cifar10][Balanced] Epoch 005/015 | train_acc=0.4128 test_acc=0.5064\n",
      "[cifar10][Balanced] Epoch 006/015 | train_acc=0.4315 test_acc=0.5233\n",
      "[cifar10][Balanced] Epoch 007/015 | train_acc=0.4471 test_acc=0.5426\n",
      "[cifar10][Balanced] Epoch 008/015 | train_acc=0.4645 test_acc=0.5445\n",
      "[cifar10][Balanced] Epoch 009/015 | train_acc=0.4747 test_acc=0.5595\n",
      "[cifar10][Balanced] Epoch 010/015 | train_acc=0.4847 test_acc=0.5622\n",
      "[cifar10][Balanced] Epoch 011/015 | train_acc=0.4935 test_acc=0.5788\n",
      "[cifar10][Balanced] Epoch 012/015 | train_acc=0.5029 test_acc=0.5851\n",
      "[cifar10][Balanced] Epoch 013/015 | train_acc=0.5143 test_acc=0.5768\n",
      "[cifar10][Balanced] Epoch 014/015 | train_acc=0.5205 test_acc=0.5907\n",
      "[cifar10][Balanced] Epoch 015/015 | train_acc=0.5251 test_acc=0.6061\n",
      "Best test acc (pre-prune): 0.6061  -> saved to checkpoints_cnn_neff\\cifar10\\Balanced\\best_preprune.pt\n",
      "\n",
      "=== Training Balanced_Deep on cifar10 ===\n",
      "[cifar10][Balanced_Deep] Epoch 001/020 | train_acc=0.2365 test_acc=0.3851\n",
      "[cifar10][Balanced_Deep] Epoch 002/020 | train_acc=0.3280 test_acc=0.4387\n",
      "[cifar10][Balanced_Deep] Epoch 003/020 | train_acc=0.3740 test_acc=0.4909\n",
      "[cifar10][Balanced_Deep] Epoch 004/020 | train_acc=0.4076 test_acc=0.5086\n",
      "[cifar10][Balanced_Deep] Epoch 005/020 | train_acc=0.4321 test_acc=0.5253\n",
      "[cifar10][Balanced_Deep] Epoch 006/020 | train_acc=0.4560 test_acc=0.5533\n",
      "[cifar10][Balanced_Deep] Epoch 007/020 | train_acc=0.4686 test_acc=0.5668\n",
      "[cifar10][Balanced_Deep] Epoch 008/020 | train_acc=0.4908 test_acc=0.5820\n",
      "[cifar10][Balanced_Deep] Epoch 009/020 | train_acc=0.5005 test_acc=0.5917\n",
      "[cifar10][Balanced_Deep] Epoch 010/020 | train_acc=0.5160 test_acc=0.6036\n",
      "[cifar10][Balanced_Deep] Epoch 011/020 | train_acc=0.5278 test_acc=0.6052\n",
      "[cifar10][Balanced_Deep] Epoch 012/020 | train_acc=0.5356 test_acc=0.6246\n",
      "[cifar10][Balanced_Deep] Epoch 013/020 | train_acc=0.5500 test_acc=0.6303\n",
      "[cifar10][Balanced_Deep] Epoch 014/020 | train_acc=0.5611 test_acc=0.6400\n",
      "[cifar10][Balanced_Deep] Epoch 015/020 | train_acc=0.5715 test_acc=0.6412\n",
      "[cifar10][Balanced_Deep] Epoch 016/020 | train_acc=0.5762 test_acc=0.6523\n",
      "[cifar10][Balanced_Deep] Epoch 017/020 | train_acc=0.5842 test_acc=0.6672\n",
      "[cifar10][Balanced_Deep] Epoch 018/020 | train_acc=0.5927 test_acc=0.6749\n",
      "[cifar10][Balanced_Deep] Epoch 019/020 | train_acc=0.6001 test_acc=0.6771\n",
      "[cifar10][Balanced_Deep] Epoch 020/020 | train_acc=0.6047 test_acc=0.6776\n",
      "Best test acc (pre-prune): 0.6776  -> saved to checkpoints_cnn_neff\\cifar10\\Balanced_Deep\\best_preprune.pt\n",
      "\n",
      "=== Training Wide on cifar10 ===\n",
      "[cifar10][Wide] Epoch 001/030 | train_acc=0.5338 test_acc=0.5016\n",
      "[cifar10][Wide] Epoch 002/030 | train_acc=0.6464 test_acc=0.6208\n",
      "[cifar10][Wide] Epoch 003/030 | train_acc=0.6924 test_acc=0.6160\n",
      "[cifar10][Wide] Epoch 004/030 | train_acc=0.7255 test_acc=0.6419\n",
      "[cifar10][Wide] Epoch 005/030 | train_acc=0.7532 test_acc=0.6508\n",
      "[cifar10][Wide] Epoch 006/030 | train_acc=0.7729 test_acc=0.6267\n",
      "[cifar10][Wide] Epoch 007/030 | train_acc=0.7937 test_acc=0.6732\n",
      "[cifar10][Wide] Epoch 008/030 | train_acc=0.8090 test_acc=0.7394\n",
      "[cifar10][Wide] Epoch 009/030 | train_acc=0.8280 test_acc=0.6823\n",
      "[cifar10][Wide] Epoch 010/030 | train_acc=0.8428 test_acc=0.7374\n",
      "[cifar10][Wide] Epoch 011/030 | train_acc=0.8602 test_acc=0.6896\n",
      "[cifar10][Wide] Epoch 012/030 | train_acc=0.8714 test_acc=0.7158\n",
      "[cifar10][Wide] Epoch 013/030 | train_acc=0.8861 test_acc=0.7072\n",
      "[cifar10][Wide] Epoch 014/030 | train_acc=0.8986 test_acc=0.7093\n",
      "[cifar10][Wide] Epoch 015/030 | train_acc=0.9098 test_acc=0.7084\n",
      "[cifar10][Wide] Epoch 016/030 | train_acc=0.9218 test_acc=0.7386\n",
      "[cifar10][Wide] Epoch 017/030 | train_acc=0.9308 test_acc=0.7369\n",
      "[cifar10][Wide] Epoch 018/030 | train_acc=0.9410 test_acc=0.7471\n",
      "[cifar10][Wide] Epoch 019/030 | train_acc=0.9499 test_acc=0.7573\n",
      "[cifar10][Wide] Epoch 020/030 | train_acc=0.9564 test_acc=0.7459\n",
      "[cifar10][Wide] Epoch 021/030 | train_acc=0.9630 test_acc=0.7589\n",
      "[cifar10][Wide] Epoch 022/030 | train_acc=0.9702 test_acc=0.7353\n",
      "[cifar10][Wide] Epoch 023/030 | train_acc=0.9749 test_acc=0.7366\n",
      "[cifar10][Wide] Epoch 024/030 | train_acc=0.9788 test_acc=0.7872\n",
      "[cifar10][Wide] Epoch 025/030 | train_acc=0.9816 test_acc=0.7638\n",
      "[cifar10][Wide] Epoch 026/030 | train_acc=0.9852 test_acc=0.7429\n",
      "[cifar10][Wide] Epoch 027/030 | train_acc=0.9863 test_acc=0.7464\n",
      "[cifar10][Wide] Epoch 028/030 | train_acc=0.9876 test_acc=0.6765\n",
      "[cifar10][Wide] Epoch 029/030 | train_acc=0.9908 test_acc=0.7199\n",
      "[cifar10][Wide] Epoch 030/030 | train_acc=0.9937 test_acc=0.7395\n",
      "Best test acc (pre-prune): 0.7872  -> saved to checkpoints_cnn_neff\\cifar10\\Wide\\best_preprune.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Assumes you already define `get_loaders(dataset_name, batch_size=..., ...)`\n",
    "# exactly as provided in your message. We'll just import it here.\n",
    "# If it's in the same file, remove the import and place the function above.\n",
    "# --------------------------------------------------------------------------\n",
    "# from your_dataloader_module import get_loaders   # <- uncomment if needed\n",
    "\n",
    "# --------------------------\n",
    "# Repro & device utilities\n",
    "# --------------------------\n",
    "def seed_everything(seed: int = 1337):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Determinism can slow training; enable only if you need exact repeatability.\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# Model zoo: 5 CNN variants\n",
    "# --------------------------\n",
    "# We define channel layouts that mirror the spirit of your MLP configs:\n",
    "# - Underfit & brittle\n",
    "# - Deep-Narrow\n",
    "# - Balanced (baseline)\n",
    "# - Balanced-Deep\n",
    "# - Wide\n",
    "#\n",
    "# These are intentionally modest so they run on a single GPU/CPU reliably.\n",
    "\n",
    "cnn_model_configs: Dict[str, Dict] = {\n",
    "    # Underfit & brittle\n",
    "    \"Tiny_Underfit\": {\n",
    "        \"channels\": [16],          # one conv block, then GAP + FC\n",
    "        \"lr\": 3e-4,\n",
    "        \"epochs\": 10,\n",
    "        \"dropout\": 0.0,\n",
    "        \"pool_every\": 1            # pool after each block\n",
    "    },\n",
    "    # Deep-narrow (depth sensitivity)\n",
    "    \"Deep_Narrow\": {\n",
    "        \"channels\": [16, 16, 16, 16, 16, 16, 16, 16],\n",
    "        \"lr\": 3e-4,\n",
    "        \"epochs\": 15,\n",
    "        \"dropout\": 0.2,\n",
    "        \"pool_every\": 2\n",
    "    },\n",
    "    # Well-trained baseline\n",
    "    \"Balanced\": {\n",
    "        \"channels\": [32, 64, 128],\n",
    "        \"lr\": 3e-4,\n",
    "        \"epochs\": 15,\n",
    "        \"dropout\": 0.2,\n",
    "        \"pool_every\": 1\n",
    "    },\n",
    "    # Deep but still robust\n",
    "    \"Balanced_Deep\": {\n",
    "        \"channels\": [32, 64, 128, 128],\n",
    "        \"lr\": 3e-4,\n",
    "        \"epochs\": 20,\n",
    "        \"dropout\": 0.3,\n",
    "        \"pool_every\": 1\n",
    "    },\n",
    "    # Overparameterized\n",
    "    \"Wide\": {\n",
    "        \"channels\": [64, 128, 256],\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 30,\n",
    "        \"dropout\": 0.0,\n",
    "        \"pool_every\": 1\n",
    "    },\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Flexible CNN building blocks\n",
    "# --------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_ch)\n",
    "        self.do   = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.do(x)\n",
    "        return x\n",
    "\n",
    "class FlexibleCNN(nn.Module):\n",
    "    def __init__(self, in_ch: int, num_classes: int, channels: List[int],\n",
    "                 dropout: float = 0.0, pool_every: int = 1):\n",
    "        super().__init__()\n",
    "        assert len(channels) >= 1, \"Need at least one conv stage\"\n",
    "        blocks = []\n",
    "        c_prev = in_ch\n",
    "        for i, c in enumerate(channels, start=1):\n",
    "            blocks.append(ConvBlock(c_prev, c, dropout=dropout))\n",
    "            # lightweight option: insert a 3x3 depthwise-separable? Keeping simple here.\n",
    "            if pool_every > 0 and (i % pool_every == 0):\n",
    "                blocks.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            c_prev = c\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling\n",
    "        self.classifier = nn.Linear(channels[-1], num_classes)\n",
    "\n",
    "        # Kaiming init for convs, zero bias, and suitable init for BN/FC\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)  # (B, C, 1, 1) -> (B, C)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# --------------------------\n",
    "# Neff pruning utilities (plug-in; default OFF in training)\n",
    "# --------------------------\n",
    "@torch.no_grad()\n",
    "def _neff_from_abs_vector(v_abs: torch.Tensor) -> int:\n",
    "    # v_abs: 1-D absolute values\n",
    "    s = v_abs.sum()\n",
    "    if s <= 0:\n",
    "        return 0\n",
    "    p = v_abs / s\n",
    "    denom = (p * p).sum()\n",
    "    if denom <= 0:\n",
    "        return 0\n",
    "    neff = int(math.floor((1.0 / denom).item())) if isinstance(denom, torch.Tensor) else int(math.floor(1.0 / denom))\n",
    "    # Clamp to [1, N]\n",
    "    neff = max(1, min(neff, v_abs.numel()))\n",
    "    return neff\n",
    "\n",
    "@torch.no_grad()\n",
    "def _keep_topk_by_abs(original: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    if k <= 0 or original.numel() == 0:\n",
    "        return torch.zeros_like(original)\n",
    "    if k >= original.numel():\n",
    "        return original.clone()\n",
    "    v_abs = original.abs().flatten()\n",
    "    topk = torch.topk(v_abs, k, largest=True, sorted=False).indices\n",
    "    mask = torch.zeros_like(v_abs, dtype=torch.bool)\n",
    "    mask[topk] = True\n",
    "    mask = mask.view_as(original)\n",
    "    pruned = torch.where(mask, original, torch.zeros_like(original))\n",
    "    return pruned\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_neff_pruning(model: nn.Module,\n",
    "                       granularity: str = \"per_filter\",   # \"per_filter\" or \"per_tensor\"\n",
    "                       modules: Tuple = (nn.Conv2d, nn.Linear)) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Apply Neff pruning in-place to Conv/Linear weights.\n",
    "    - For Conv2d (\"per_filter\"): treat each out-channel kernel [in_c, kh, kw] as a series.\n",
    "    - For Conv2d (\"per_tensor\"): treat entire weight tensor as a single series.\n",
    "    - For Linear: treat entire weight matrix as a single series.\n",
    "\n",
    "    Returns:\n",
    "        stats dict: {param_name: {\"neff\": ..., \"N\": ..., \"sparsity\": ...}, ...}\n",
    "    \"\"\"\n",
    "    assert granularity in {\"per_filter\", \"per_tensor\"}\n",
    "    stats = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, modules):\n",
    "            W = module.weight.data\n",
    "            W_shape = tuple(W.shape)\n",
    "            before_nnz = (W != 0).sum().item()\n",
    "            N_total = W.numel()\n",
    "\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                if granularity == \"per_filter\":\n",
    "                    # loop over out channels\n",
    "                    for f in range(W.shape[0]):\n",
    "                        w = W[f]  # (in_c, kh, kw)\n",
    "                        v_abs = w.abs().flatten()\n",
    "                        neff = _neff_from_abs_vector(v_abs)\n",
    "                        W[f] = _keep_topk_by_abs(w, neff)\n",
    "                    after_nnz = (W != 0).sum().item()\n",
    "                    stats_key = f\"{name}.weight\"\n",
    "                    stats[stats_key] = {\n",
    "                        \"granularity\": \"per_filter\",\n",
    "                        \"shape\": W_shape,\n",
    "                        \"N\": N_total,\n",
    "                        \"nonzeros\": after_nnz,\n",
    "                        \"sparsity\": 1.0 - (after_nnz / N_total)\n",
    "                    }\n",
    "                else:\n",
    "                    # per_tensor\n",
    "                    v_abs = W.abs().flatten()\n",
    "                    neff = _neff_from_abs_vector(v_abs)\n",
    "                    W[:] = _keep_topk_by_abs(W, neff)\n",
    "                    after_nnz = (W != 0).sum().item()\n",
    "                    stats_key = f\"{name}.weight\"\n",
    "                    stats[stats_key] = {\n",
    "                        \"granularity\": \"per_tensor\",\n",
    "                        \"shape\": W_shape,\n",
    "                        \"N\": N_total,\n",
    "                        \"nonzeros\": after_nnz,\n",
    "                        \"sparsity\": 1.0 - (after_nnz / N_total)\n",
    "                    }\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                # treat whole matrix as one series\n",
    "                v_abs = W.abs().flatten()\n",
    "                neff = _neff_from_abs_vector(v_abs)\n",
    "                W[:] = _keep_topk_by_abs(W, neff)\n",
    "                after_nnz = (W != 0).sum().item()\n",
    "                stats_key = f\"{name}.weight\"\n",
    "                stats[stats_key] = {\n",
    "                    \"granularity\": \"per_tensor\",\n",
    "                    \"shape\": W_shape,\n",
    "                    \"N\": N_total,\n",
    "                    \"nonzeros\": after_nnz,\n",
    "                    \"sparsity\": 1.0 - (after_nnz / N_total)\n",
    "                }\n",
    "            # We do not prune BN or biases here by design.\n",
    "    return stats\n",
    "\n",
    "# --------------------------\n",
    "# Training / evaluation\n",
    "# --------------------------\n",
    "def accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, optimizer, scaler, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss, total_acc, total = 0.0, 0.0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        if device.type == \"cuda\":\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total += bs\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += accuracy(logits.detach(), y) * bs\n",
    "    return total_loss / total, total_acc / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, total = 0.0, 0.0, 0\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        bs = y.size(0)\n",
    "        total += bs\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += accuracy(logits, y) * bs\n",
    "    return total_loss / total, total_acc / total\n",
    "\n",
    "# --------------------------\n",
    "# Orchestration\n",
    "# --------------------------\n",
    "def save_json(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def append_jsonl(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"a\") as f:\n",
    "        f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "def build_model(in_ch: int, num_classes: int, cfg: Dict) -> nn.Module:\n",
    "    return FlexibleCNN(\n",
    "        in_ch=in_ch,\n",
    "        num_classes=num_classes,\n",
    "        channels=cfg[\"channels\"],\n",
    "        dropout=cfg.get(\"dropout\", 0.0),\n",
    "        pool_every=cfg.get(\"pool_every\", 1),\n",
    "    )\n",
    "\n",
    "def train_models_on_dataset(\n",
    "    dataset_name: str,\n",
    "    configs: Dict[str, Dict],\n",
    "    batch_size: int = 128,\n",
    "    test_batch_size: int = 1000,\n",
    "    data_root: str = \"./data\",\n",
    "    out_dir: str = \"./checkpoints_cnn_neff\",\n",
    "    do_prune_after_train: bool = False,          # default OFF; enable when ready\n",
    "    prune_granularity: str = \"per_filter\"        # \"per_filter\" or \"per_tensor\"\n",
    "):\n",
    "    # You already have this function defined; we call it directly.\n",
    "    train_loader, test_loader, inp, ncls, meta = get_loaders(dataset_name, batch_size, test_batch_size, data_root)\n",
    "\n",
    "    # Infer input channels safely from first sample\n",
    "    sample_x, _ = next(iter(train_loader))\n",
    "    in_ch = sample_x.shape[1]\n",
    "\n",
    "    device = get_device()\n",
    "    print(f\"[{dataset_name}] device: {device}\")\n",
    "\n",
    "    for name, cfg in configs.items():\n",
    "        print(f\"\\n=== Training {name} on {dataset_name} ===\")\n",
    "        model = build_model(in_ch, ncls, cfg).to(device)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=1e-4)\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "        # Output folders & bookkeeping\n",
    "        run_dir = Path(out_dir) / dataset_name / name\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_json({\"dataset\": dataset_name, **cfg}, run_dir / \"config.json\")\n",
    "\n",
    "        best_acc = -1.0\n",
    "        best_path_full = run_dir / \"best_preprune.pt\"\n",
    "        best_path_sd   = run_dir / \"best_preprune_state_dict.pt\"\n",
    "\n",
    "        for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "            tr_loss, tr_acc = train_one_epoch(model, opt, scaler, train_loader, device)\n",
    "            te_loss, te_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "            log_row = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": round(tr_loss, 6),\n",
    "                \"train_acc\": round(tr_acc, 6),\n",
    "                \"test_loss\": round(te_loss, 6),\n",
    "                \"test_acc\": round(te_acc, 6),\n",
    "            }\n",
    "            append_jsonl(log_row, run_dir / \"metrics.jsonl\")\n",
    "\n",
    "            print(f\"[{dataset_name}][{name}] \"\n",
    "                  f\"Epoch {epoch:03d}/{cfg['epochs']:03d} | \"\n",
    "                  f\"train_acc={tr_acc:.4f} test_acc={te_acc:.4f}\")\n",
    "\n",
    "            if te_acc > best_acc:\n",
    "                best_acc = te_acc\n",
    "                torch.save(model, best_path_full)\n",
    "                torch.save(model.state_dict(), best_path_sd)\n",
    "\n",
    "        print(f\"Best test acc (pre-prune): {best_acc:.4f}  -> saved to {best_path_full}\")\n",
    "\n",
    "        # Optional: apply Neff pruning AFTER training and save a pruned copy\n",
    "        if do_prune_after_train:\n",
    "            print(f\"Applying Neff pruning ({prune_granularity}) to {name} on {dataset_name} ...\")\n",
    "            # Load best preprune before pruning to be safe\n",
    "            model = torch.load(best_path_full, map_location=device)\n",
    "            model.eval()\n",
    "            stats = apply_neff_pruning(model, granularity=prune_granularity)\n",
    "            pruned_te_loss, pruned_te_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "            prune_report = {\n",
    "                \"granularity\": prune_granularity,\n",
    "                \"layers\": stats,\n",
    "                \"post_prune_test_acc\": pruned_te_acc,\n",
    "                \"post_prune_test_loss\": pruned_te_loss,\n",
    "            }\n",
    "            save_json(prune_report, run_dir / f\"neff_prune_{prune_granularity}.json\")\n",
    "            torch.save(model, run_dir / f\"best_postprune_neff_{prune_granularity}.pt\")\n",
    "            torch.save(model.state_dict(), run_dir / f\"best_postprune_neff_{prune_granularity}_state_dict.pt\")\n",
    "            print(f\"Post-prune acc: {pruned_te_acc:.4f}  -> pruned model saved.\")\n",
    "\n",
    "# --------------------------\n",
    "# Main\n",
    "# --------------------------\n",
    "def main():\n",
    "    seed_everything(1337)\n",
    "    OUT_DIR = \"./checkpoints_cnn_neff\"\n",
    "\n",
    "    # Datasets you requested\n",
    "    datasets_to_run = [\"fashionmnist\", \"cifar10\"]\n",
    "\n",
    "    # Toggle pruning (default: train first, no pruning)\n",
    "    DO_PRUNE_AFTER_TRAIN = False\n",
    "    PRUNE_GRANULARITY = \"per_filter\"  # or \"per_tensor\"\n",
    "\n",
    "    for dset in datasets_to_run:\n",
    "        train_models_on_dataset(\n",
    "            dataset_name=dset,\n",
    "            configs=cnn_model_configs,\n",
    "            batch_size=128,\n",
    "            test_batch_size=1000,\n",
    "            data_root=\"./data\",\n",
    "            out_dir=OUT_DIR,\n",
    "            do_prune_after_train=DO_PRUNE_AFTER_TRAIN,\n",
    "            prune_granularity=PRUNE_GRANULARITY\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fashionmnist] device: cuda\n",
      "\n",
      "=== Training Tiny_Underfit on fashionmnist ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_64000\\607686214.py:344: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_64000\\607686214.py:264: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fashionmnist][Tiny_Underfit] Epoch 001/010 | train_acc=0.1274 test_acc=0.1853\n",
      "[fashionmnist][Tiny_Underfit] Epoch 002/010 | train_acc=0.2533 test_acc=0.3552\n",
      "[fashionmnist][Tiny_Underfit] Epoch 003/010 | train_acc=0.3599 test_acc=0.3692\n",
      "[fashionmnist][Tiny_Underfit] Epoch 004/010 | train_acc=0.3815 test_acc=0.3941\n",
      "[fashionmnist][Tiny_Underfit] Epoch 005/010 | train_acc=0.4006 test_acc=0.4175\n",
      "[fashionmnist][Tiny_Underfit] Epoch 006/010 | train_acc=0.4188 test_acc=0.4310\n",
      "[fashionmnist][Tiny_Underfit] Epoch 007/010 | train_acc=0.4380 test_acc=0.4574\n",
      "[fashionmnist][Tiny_Underfit] Epoch 008/010 | train_acc=0.4571 test_acc=0.4776\n",
      "[fashionmnist][Tiny_Underfit] Epoch 009/010 | train_acc=0.4766 test_acc=0.4913\n",
      "[fashionmnist][Tiny_Underfit] Epoch 010/010 | train_acc=0.4924 test_acc=0.4959\n",
      "Best test acc (pre-prune): 0.4959  -> saved to checkpoints_cnn_neff\\fashionmnist\\Tiny_Underfit\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Tiny_Underfit on fashionmnist ...\n",
      "Post-prune acc: 0.3474  -> pruned model saved.\n",
      "\n",
      "=== Training Deep_Narrow on fashionmnist ===\n",
      "[fashionmnist][Deep_Narrow] Epoch 001/015 | train_acc=0.2543 test_acc=0.5614\n",
      "[fashionmnist][Deep_Narrow] Epoch 002/015 | train_acc=0.5202 test_acc=0.7143\n",
      "[fashionmnist][Deep_Narrow] Epoch 003/015 | train_acc=0.6317 test_acc=0.7413\n",
      "[fashionmnist][Deep_Narrow] Epoch 004/015 | train_acc=0.6803 test_acc=0.7686\n",
      "[fashionmnist][Deep_Narrow] Epoch 005/015 | train_acc=0.7141 test_acc=0.7857\n",
      "[fashionmnist][Deep_Narrow] Epoch 006/015 | train_acc=0.7330 test_acc=0.8049\n",
      "[fashionmnist][Deep_Narrow] Epoch 007/015 | train_acc=0.7527 test_acc=0.8184\n",
      "[fashionmnist][Deep_Narrow] Epoch 008/015 | train_acc=0.7643 test_acc=0.8289\n",
      "[fashionmnist][Deep_Narrow] Epoch 009/015 | train_acc=0.7775 test_acc=0.8347\n",
      "[fashionmnist][Deep_Narrow] Epoch 010/015 | train_acc=0.7849 test_acc=0.8400\n",
      "[fashionmnist][Deep_Narrow] Epoch 011/015 | train_acc=0.7901 test_acc=0.8419\n",
      "[fashionmnist][Deep_Narrow] Epoch 012/015 | train_acc=0.7975 test_acc=0.8439\n",
      "[fashionmnist][Deep_Narrow] Epoch 013/015 | train_acc=0.8018 test_acc=0.8475\n",
      "[fashionmnist][Deep_Narrow] Epoch 014/015 | train_acc=0.8044 test_acc=0.8485\n",
      "[fashionmnist][Deep_Narrow] Epoch 015/015 | train_acc=0.8079 test_acc=0.8521\n",
      "Best test acc (pre-prune): 0.8521  -> saved to checkpoints_cnn_neff\\fashionmnist\\Deep_Narrow\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Deep_Narrow on fashionmnist ...\n",
      "Post-prune acc: 0.8399  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced on fashionmnist ===\n",
      "[fashionmnist][Balanced] Epoch 001/015 | train_acc=0.5944 test_acc=0.7855\n",
      "[fashionmnist][Balanced] Epoch 002/015 | train_acc=0.7462 test_acc=0.8167\n",
      "[fashionmnist][Balanced] Epoch 003/015 | train_acc=0.7853 test_acc=0.8363\n",
      "[fashionmnist][Balanced] Epoch 004/015 | train_acc=0.8050 test_acc=0.8469\n",
      "[fashionmnist][Balanced] Epoch 005/015 | train_acc=0.8231 test_acc=0.8536\n",
      "[fashionmnist][Balanced] Epoch 006/015 | train_acc=0.8340 test_acc=0.8658\n",
      "[fashionmnist][Balanced] Epoch 007/015 | train_acc=0.8428 test_acc=0.8700\n",
      "[fashionmnist][Balanced] Epoch 008/015 | train_acc=0.8490 test_acc=0.8754\n",
      "[fashionmnist][Balanced] Epoch 009/015 | train_acc=0.8546 test_acc=0.8765\n",
      "[fashionmnist][Balanced] Epoch 010/015 | train_acc=0.8616 test_acc=0.8812\n",
      "[fashionmnist][Balanced] Epoch 011/015 | train_acc=0.8673 test_acc=0.8806\n",
      "[fashionmnist][Balanced] Epoch 012/015 | train_acc=0.8700 test_acc=0.8859\n",
      "[fashionmnist][Balanced] Epoch 013/015 | train_acc=0.8728 test_acc=0.8881\n",
      "[fashionmnist][Balanced] Epoch 014/015 | train_acc=0.8759 test_acc=0.8898\n",
      "[fashionmnist][Balanced] Epoch 015/015 | train_acc=0.8780 test_acc=0.8934\n",
      "Best test acc (pre-prune): 0.8934  -> saved to checkpoints_cnn_neff\\fashionmnist\\Balanced\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Balanced on fashionmnist ...\n",
      "Post-prune acc: 0.8794  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced_Deep on fashionmnist ===\n",
      "[fashionmnist][Balanced_Deep] Epoch 001/020 | train_acc=0.6532 test_acc=0.8132\n",
      "[fashionmnist][Balanced_Deep] Epoch 002/020 | train_acc=0.8067 test_acc=0.8489\n",
      "[fashionmnist][Balanced_Deep] Epoch 003/020 | train_acc=0.8370 test_acc=0.8647\n",
      "[fashionmnist][Balanced_Deep] Epoch 004/020 | train_acc=0.8533 test_acc=0.8746\n",
      "[fashionmnist][Balanced_Deep] Epoch 005/020 | train_acc=0.8629 test_acc=0.8795\n",
      "[fashionmnist][Balanced_Deep] Epoch 006/020 | train_acc=0.8709 test_acc=0.8865\n",
      "[fashionmnist][Balanced_Deep] Epoch 007/020 | train_acc=0.8772 test_acc=0.8904\n",
      "[fashionmnist][Balanced_Deep] Epoch 008/020 | train_acc=0.8829 test_acc=0.8933\n",
      "[fashionmnist][Balanced_Deep] Epoch 009/020 | train_acc=0.8868 test_acc=0.8983\n",
      "[fashionmnist][Balanced_Deep] Epoch 010/020 | train_acc=0.8888 test_acc=0.9002\n",
      "[fashionmnist][Balanced_Deep] Epoch 011/020 | train_acc=0.8931 test_acc=0.9007\n",
      "[fashionmnist][Balanced_Deep] Epoch 012/020 | train_acc=0.8940 test_acc=0.9035\n",
      "[fashionmnist][Balanced_Deep] Epoch 013/020 | train_acc=0.8967 test_acc=0.9034\n",
      "[fashionmnist][Balanced_Deep] Epoch 014/020 | train_acc=0.9004 test_acc=0.9083\n",
      "[fashionmnist][Balanced_Deep] Epoch 015/020 | train_acc=0.9017 test_acc=0.9071\n",
      "[fashionmnist][Balanced_Deep] Epoch 016/020 | train_acc=0.9043 test_acc=0.9087\n",
      "[fashionmnist][Balanced_Deep] Epoch 017/020 | train_acc=0.9060 test_acc=0.9099\n",
      "[fashionmnist][Balanced_Deep] Epoch 018/020 | train_acc=0.9076 test_acc=0.9110\n",
      "[fashionmnist][Balanced_Deep] Epoch 019/020 | train_acc=0.9077 test_acc=0.9122\n",
      "[fashionmnist][Balanced_Deep] Epoch 020/020 | train_acc=0.9091 test_acc=0.9135\n",
      "Best test acc (pre-prune): 0.9135  -> saved to checkpoints_cnn_neff\\fashionmnist\\Balanced_Deep\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Balanced_Deep on fashionmnist ...\n",
      "Post-prune acc: 0.9017  -> pruned model saved.\n",
      "\n",
      "=== Training Wide on fashionmnist ===\n",
      "[fashionmnist][Wide] Epoch 001/030 | train_acc=0.8504 test_acc=0.8420\n",
      "[fashionmnist][Wide] Epoch 002/030 | train_acc=0.8997 test_acc=0.8944\n",
      "[fashionmnist][Wide] Epoch 003/030 | train_acc=0.9144 test_acc=0.8825\n",
      "[fashionmnist][Wide] Epoch 004/030 | train_acc=0.9248 test_acc=0.8663\n",
      "[fashionmnist][Wide] Epoch 005/030 | train_acc=0.9325 test_acc=0.9002\n",
      "[fashionmnist][Wide] Epoch 006/030 | train_acc=0.9415 test_acc=0.8593\n",
      "[fashionmnist][Wide] Epoch 007/030 | train_acc=0.9477 test_acc=0.8988\n",
      "[fashionmnist][Wide] Epoch 008/030 | train_acc=0.9530 test_acc=0.9188\n",
      "[fashionmnist][Wide] Epoch 009/030 | train_acc=0.9586 test_acc=0.9020\n",
      "[fashionmnist][Wide] Epoch 010/030 | train_acc=0.9657 test_acc=0.9114\n",
      "[fashionmnist][Wide] Epoch 011/030 | train_acc=0.9685 test_acc=0.8997\n",
      "[fashionmnist][Wide] Epoch 012/030 | train_acc=0.9734 test_acc=0.9118\n",
      "[fashionmnist][Wide] Epoch 013/030 | train_acc=0.9790 test_acc=0.9092\n",
      "[fashionmnist][Wide] Epoch 014/030 | train_acc=0.9823 test_acc=0.9070\n",
      "[fashionmnist][Wide] Epoch 015/030 | train_acc=0.9870 test_acc=0.8662\n",
      "[fashionmnist][Wide] Epoch 016/030 | train_acc=0.9858 test_acc=0.9031\n",
      "[fashionmnist][Wide] Epoch 017/030 | train_acc=0.9907 test_acc=0.9072\n",
      "[fashionmnist][Wide] Epoch 018/030 | train_acc=0.9930 test_acc=0.9181\n",
      "[fashionmnist][Wide] Epoch 019/030 | train_acc=0.9924 test_acc=0.9122\n",
      "[fashionmnist][Wide] Epoch 020/030 | train_acc=0.9945 test_acc=0.8778\n",
      "[fashionmnist][Wide] Epoch 021/030 | train_acc=0.9954 test_acc=0.8669\n",
      "[fashionmnist][Wide] Epoch 022/030 | train_acc=0.9940 test_acc=0.9158\n",
      "[fashionmnist][Wide] Epoch 023/030 | train_acc=0.9948 test_acc=0.9171\n",
      "[fashionmnist][Wide] Epoch 024/030 | train_acc=0.9985 test_acc=0.9120\n",
      "[fashionmnist][Wide] Epoch 025/030 | train_acc=0.9972 test_acc=0.9077\n",
      "[fashionmnist][Wide] Epoch 026/030 | train_acc=0.9949 test_acc=0.9001\n",
      "[fashionmnist][Wide] Epoch 027/030 | train_acc=0.9972 test_acc=0.9105\n",
      "[fashionmnist][Wide] Epoch 028/030 | train_acc=0.9965 test_acc=0.9056\n",
      "[fashionmnist][Wide] Epoch 029/030 | train_acc=0.9968 test_acc=0.9100\n",
      "[fashionmnist][Wide] Epoch 030/030 | train_acc=0.9984 test_acc=0.9124\n",
      "Best test acc (pre-prune): 0.9188  -> saved to checkpoints_cnn_neff\\fashionmnist\\Wide\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Wide on fashionmnist ...\n",
      "Post-prune acc: 0.5434  -> pruned model saved.\n",
      "[cifar10] device: cuda\n",
      "\n",
      "=== Training Tiny_Underfit on cifar10 ===\n",
      "[cifar10][Tiny_Underfit] Epoch 001/010 | train_acc=0.1294 test_acc=0.1728\n",
      "[cifar10][Tiny_Underfit] Epoch 002/010 | train_acc=0.2197 test_acc=0.2409\n",
      "[cifar10][Tiny_Underfit] Epoch 003/010 | train_acc=0.2428 test_acc=0.2537\n",
      "[cifar10][Tiny_Underfit] Epoch 004/010 | train_acc=0.2599 test_acc=0.2753\n",
      "[cifar10][Tiny_Underfit] Epoch 005/010 | train_acc=0.2777 test_acc=0.2983\n",
      "[cifar10][Tiny_Underfit] Epoch 006/010 | train_acc=0.2975 test_acc=0.3097\n",
      "[cifar10][Tiny_Underfit] Epoch 007/010 | train_acc=0.3060 test_acc=0.3203\n",
      "[cifar10][Tiny_Underfit] Epoch 008/010 | train_acc=0.3171 test_acc=0.3289\n",
      "[cifar10][Tiny_Underfit] Epoch 009/010 | train_acc=0.3238 test_acc=0.3298\n",
      "[cifar10][Tiny_Underfit] Epoch 010/010 | train_acc=0.3296 test_acc=0.3382\n",
      "Best test acc (pre-prune): 0.3382  -> saved to checkpoints_cnn_neff\\cifar10\\Tiny_Underfit\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Tiny_Underfit on cifar10 ...\n",
      "Post-prune acc: 0.2825  -> pruned model saved.\n",
      "\n",
      "=== Training Deep_Narrow on cifar10 ===\n",
      "[cifar10][Deep_Narrow] Epoch 001/015 | train_acc=0.1223 test_acc=0.1183\n",
      "[cifar10][Deep_Narrow] Epoch 002/015 | train_acc=0.1503 test_acc=0.1775\n",
      "[cifar10][Deep_Narrow] Epoch 003/015 | train_acc=0.1778 test_acc=0.2322\n",
      "[cifar10][Deep_Narrow] Epoch 004/015 | train_acc=0.2075 test_acc=0.2543\n",
      "[cifar10][Deep_Narrow] Epoch 005/015 | train_acc=0.2237 test_acc=0.2861\n",
      "[cifar10][Deep_Narrow] Epoch 006/015 | train_acc=0.2410 test_acc=0.2884\n",
      "[cifar10][Deep_Narrow] Epoch 007/015 | train_acc=0.2507 test_acc=0.2996\n",
      "[cifar10][Deep_Narrow] Epoch 008/015 | train_acc=0.2646 test_acc=0.3207\n",
      "[cifar10][Deep_Narrow] Epoch 009/015 | train_acc=0.2768 test_acc=0.3213\n",
      "[cifar10][Deep_Narrow] Epoch 010/015 | train_acc=0.2814 test_acc=0.3377\n",
      "[cifar10][Deep_Narrow] Epoch 011/015 | train_acc=0.2853 test_acc=0.3476\n",
      "[cifar10][Deep_Narrow] Epoch 012/015 | train_acc=0.2989 test_acc=0.3499\n",
      "[cifar10][Deep_Narrow] Epoch 013/015 | train_acc=0.3044 test_acc=0.3608\n",
      "[cifar10][Deep_Narrow] Epoch 014/015 | train_acc=0.3132 test_acc=0.3603\n",
      "[cifar10][Deep_Narrow] Epoch 015/015 | train_acc=0.3153 test_acc=0.3716\n",
      "Best test acc (pre-prune): 0.3716  -> saved to checkpoints_cnn_neff\\cifar10\\Deep_Narrow\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Deep_Narrow on cifar10 ...\n",
      "Post-prune acc: 0.3236  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced on cifar10 ===\n",
      "[cifar10][Balanced] Epoch 001/015 | train_acc=0.2516 test_acc=0.3968\n",
      "[cifar10][Balanced] Epoch 002/015 | train_acc=0.3255 test_acc=0.4306\n",
      "[cifar10][Balanced] Epoch 003/015 | train_acc=0.3618 test_acc=0.4553\n",
      "[cifar10][Balanced] Epoch 004/015 | train_acc=0.3900 test_acc=0.4791\n",
      "[cifar10][Balanced] Epoch 005/015 | train_acc=0.4105 test_acc=0.5030\n",
      "[cifar10][Balanced] Epoch 006/015 | train_acc=0.4259 test_acc=0.5193\n",
      "[cifar10][Balanced] Epoch 007/015 | train_acc=0.4433 test_acc=0.5315\n",
      "[cifar10][Balanced] Epoch 008/015 | train_acc=0.4562 test_acc=0.5466\n",
      "[cifar10][Balanced] Epoch 009/015 | train_acc=0.4659 test_acc=0.5494\n",
      "[cifar10][Balanced] Epoch 010/015 | train_acc=0.4798 test_acc=0.5557\n",
      "[cifar10][Balanced] Epoch 011/015 | train_acc=0.4851 test_acc=0.5654\n",
      "[cifar10][Balanced] Epoch 012/015 | train_acc=0.4950 test_acc=0.5788\n",
      "[cifar10][Balanced] Epoch 013/015 | train_acc=0.5005 test_acc=0.5783\n",
      "[cifar10][Balanced] Epoch 014/015 | train_acc=0.5119 test_acc=0.5842\n",
      "[cifar10][Balanced] Epoch 015/015 | train_acc=0.5169 test_acc=0.5915\n",
      "Best test acc (pre-prune): 0.5915  -> saved to checkpoints_cnn_neff\\cifar10\\Balanced\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Balanced on cifar10 ...\n",
      "Post-prune acc: 0.5204  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced_Deep on cifar10 ===\n",
      "[cifar10][Balanced_Deep] Epoch 001/020 | train_acc=0.2300 test_acc=0.3817\n",
      "[cifar10][Balanced_Deep] Epoch 002/020 | train_acc=0.3269 test_acc=0.4384\n",
      "[cifar10][Balanced_Deep] Epoch 003/020 | train_acc=0.3737 test_acc=0.4793\n",
      "[cifar10][Balanced_Deep] Epoch 004/020 | train_acc=0.4046 test_acc=0.5092\n",
      "[cifar10][Balanced_Deep] Epoch 005/020 | train_acc=0.4286 test_acc=0.5295\n",
      "[cifar10][Balanced_Deep] Epoch 006/020 | train_acc=0.4492 test_acc=0.5527\n",
      "[cifar10][Balanced_Deep] Epoch 007/020 | train_acc=0.4666 test_acc=0.5586\n",
      "[cifar10][Balanced_Deep] Epoch 008/020 | train_acc=0.4848 test_acc=0.5841\n",
      "[cifar10][Balanced_Deep] Epoch 009/020 | train_acc=0.4968 test_acc=0.5901\n",
      "[cifar10][Balanced_Deep] Epoch 010/020 | train_acc=0.5135 test_acc=0.6032\n",
      "[cifar10][Balanced_Deep] Epoch 011/020 | train_acc=0.5238 test_acc=0.6091\n",
      "[cifar10][Balanced_Deep] Epoch 012/020 | train_acc=0.5355 test_acc=0.6246\n",
      "[cifar10][Balanced_Deep] Epoch 013/020 | train_acc=0.5484 test_acc=0.6324\n",
      "[cifar10][Balanced_Deep] Epoch 014/020 | train_acc=0.5573 test_acc=0.6382\n",
      "[cifar10][Balanced_Deep] Epoch 015/020 | train_acc=0.5633 test_acc=0.6494\n",
      "[cifar10][Balanced_Deep] Epoch 016/020 | train_acc=0.5720 test_acc=0.6535\n",
      "[cifar10][Balanced_Deep] Epoch 017/020 | train_acc=0.5803 test_acc=0.6596\n",
      "[cifar10][Balanced_Deep] Epoch 018/020 | train_acc=0.5875 test_acc=0.6661\n",
      "[cifar10][Balanced_Deep] Epoch 019/020 | train_acc=0.5946 test_acc=0.6703\n",
      "[cifar10][Balanced_Deep] Epoch 020/020 | train_acc=0.6013 test_acc=0.6754\n",
      "Best test acc (pre-prune): 0.6754  -> saved to checkpoints_cnn_neff\\cifar10\\Balanced_Deep\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Balanced_Deep on cifar10 ...\n",
      "Post-prune acc: 0.6498  -> pruned model saved.\n",
      "\n",
      "=== Training Wide on cifar10 ===\n",
      "[cifar10][Wide] Epoch 001/030 | train_acc=0.5311 test_acc=0.5424\n",
      "[cifar10][Wide] Epoch 002/030 | train_acc=0.6486 test_acc=0.5912\n",
      "[cifar10][Wide] Epoch 003/030 | train_acc=0.6895 test_acc=0.6255\n",
      "[cifar10][Wide] Epoch 004/030 | train_acc=0.7235 test_acc=0.6273\n",
      "[cifar10][Wide] Epoch 005/030 | train_acc=0.7496 test_acc=0.6791\n",
      "[cifar10][Wide] Epoch 006/030 | train_acc=0.7718 test_acc=0.6850\n",
      "[cifar10][Wide] Epoch 007/030 | train_acc=0.7889 test_acc=0.7358\n",
      "[cifar10][Wide] Epoch 008/030 | train_acc=0.8077 test_acc=0.6607\n",
      "[cifar10][Wide] Epoch 009/030 | train_acc=0.8267 test_acc=0.6858\n",
      "[cifar10][Wide] Epoch 010/030 | train_acc=0.8423 test_acc=0.7073\n",
      "[cifar10][Wide] Epoch 011/030 | train_acc=0.8563 test_acc=0.6833\n",
      "[cifar10][Wide] Epoch 012/030 | train_acc=0.8696 test_acc=0.7504\n",
      "[cifar10][Wide] Epoch 013/030 | train_acc=0.8845 test_acc=0.7314\n",
      "[cifar10][Wide] Epoch 014/030 | train_acc=0.8945 test_acc=0.6922\n",
      "[cifar10][Wide] Epoch 015/030 | train_acc=0.9045 test_acc=0.6994\n",
      "[cifar10][Wide] Epoch 016/030 | train_acc=0.9182 test_acc=0.7468\n",
      "[cifar10][Wide] Epoch 017/030 | train_acc=0.9297 test_acc=0.7431\n",
      "[cifar10][Wide] Epoch 018/030 | train_acc=0.9370 test_acc=0.7435\n",
      "[cifar10][Wide] Epoch 019/030 | train_acc=0.9460 test_acc=0.7717\n",
      "[cifar10][Wide] Epoch 020/030 | train_acc=0.9552 test_acc=0.7548\n",
      "[cifar10][Wide] Epoch 021/030 | train_acc=0.9608 test_acc=0.7811\n",
      "[cifar10][Wide] Epoch 022/030 | train_acc=0.9670 test_acc=0.7252\n",
      "[cifar10][Wide] Epoch 023/030 | train_acc=0.9714 test_acc=0.7538\n",
      "[cifar10][Wide] Epoch 024/030 | train_acc=0.9768 test_acc=0.7392\n",
      "[cifar10][Wide] Epoch 025/030 | train_acc=0.9809 test_acc=0.7539\n",
      "[cifar10][Wide] Epoch 026/030 | train_acc=0.9841 test_acc=0.7587\n",
      "[cifar10][Wide] Epoch 027/030 | train_acc=0.9858 test_acc=0.7617\n",
      "[cifar10][Wide] Epoch 028/030 | train_acc=0.9879 test_acc=0.7470\n",
      "[cifar10][Wide] Epoch 029/030 | train_acc=0.9900 test_acc=0.7322\n",
      "[cifar10][Wide] Epoch 030/030 | train_acc=0.9906 test_acc=0.7495\n",
      "Best test acc (pre-prune): 0.7811  -> saved to checkpoints_cnn_neff\\cifar10\\Wide\\best_preprune.pt\n",
      "Applying Neff pruning (per_filter) to Wide on cifar10 ...\n",
      "Post-prune acc: 0.5938  -> pruned model saved.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    seed_everything(1337)\n",
    "    OUT_DIR = \"./checkpoints_cnn_neff\"\n",
    "\n",
    "    # Datasets you requested\n",
    "    datasets_to_run = [\"fashionmnist\", \"cifar10\"]\n",
    "\n",
    "    # Toggle pruning (default: train first, no pruning)\n",
    "    DO_PRUNE_AFTER_TRAIN = True\n",
    "    PRUNE_GRANULARITY = \"per_filter\"  # or \"per_tensor\"\n",
    "\n",
    "    for dset in datasets_to_run:\n",
    "        train_models_on_dataset(\n",
    "            dataset_name=dset,\n",
    "            configs=cnn_model_configs,\n",
    "            batch_size=128,\n",
    "            test_batch_size=1000,\n",
    "            data_root=\"./data\",\n",
    "            out_dir=OUT_DIR,\n",
    "            do_prune_after_train=DO_PRUNE_AFTER_TRAIN,\n",
    "            prune_granularity=PRUNE_GRANULARITY\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bf039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fashionmnist] device: cuda\n",
      "\n",
      "=== Training Tiny_Underfit on fashionmnist ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_64000\\3085886193.py:344: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_64000\\3085886193.py:264: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fashionmnist][Tiny_Underfit] Epoch 001/010 | train_acc=0.1274 test_acc=0.1853\n",
      "[fashionmnist][Tiny_Underfit] Epoch 002/010 | train_acc=0.2533 test_acc=0.3552\n",
      "[fashionmnist][Tiny_Underfit] Epoch 003/010 | train_acc=0.3599 test_acc=0.3692\n",
      "[fashionmnist][Tiny_Underfit] Epoch 004/010 | train_acc=0.3815 test_acc=0.3941\n",
      "[fashionmnist][Tiny_Underfit] Epoch 005/010 | train_acc=0.4006 test_acc=0.4175\n",
      "[fashionmnist][Tiny_Underfit] Epoch 006/010 | train_acc=0.4188 test_acc=0.4310\n",
      "[fashionmnist][Tiny_Underfit] Epoch 007/010 | train_acc=0.4380 test_acc=0.4574\n",
      "[fashionmnist][Tiny_Underfit] Epoch 008/010 | train_acc=0.4571 test_acc=0.4776\n",
      "[fashionmnist][Tiny_Underfit] Epoch 009/010 | train_acc=0.4766 test_acc=0.4913\n",
      "[fashionmnist][Tiny_Underfit] Epoch 010/010 | train_acc=0.4924 test_acc=0.4959\n",
      "Best test acc (pre-prune): 0.4959  -> saved to checkpoints_cnn_neff\\fashionmnist\\Tiny_Underfit\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Tiny_Underfit on fashionmnist ...\n",
      "Post-prune acc: 0.3736  -> pruned model saved.\n",
      "\n",
      "=== Training Deep_Narrow on fashionmnist ===\n",
      "[fashionmnist][Deep_Narrow] Epoch 001/015 | train_acc=0.2543 test_acc=0.5614\n",
      "[fashionmnist][Deep_Narrow] Epoch 002/015 | train_acc=0.5202 test_acc=0.7143\n",
      "[fashionmnist][Deep_Narrow] Epoch 003/015 | train_acc=0.6317 test_acc=0.7413\n",
      "[fashionmnist][Deep_Narrow] Epoch 004/015 | train_acc=0.6803 test_acc=0.7686\n",
      "[fashionmnist][Deep_Narrow] Epoch 005/015 | train_acc=0.7141 test_acc=0.7857\n",
      "[fashionmnist][Deep_Narrow] Epoch 006/015 | train_acc=0.7330 test_acc=0.8049\n",
      "[fashionmnist][Deep_Narrow] Epoch 007/015 | train_acc=0.7527 test_acc=0.8184\n",
      "[fashionmnist][Deep_Narrow] Epoch 008/015 | train_acc=0.7643 test_acc=0.8289\n",
      "[fashionmnist][Deep_Narrow] Epoch 009/015 | train_acc=0.7775 test_acc=0.8347\n",
      "[fashionmnist][Deep_Narrow] Epoch 010/015 | train_acc=0.7849 test_acc=0.8400\n",
      "[fashionmnist][Deep_Narrow] Epoch 011/015 | train_acc=0.7901 test_acc=0.8419\n",
      "[fashionmnist][Deep_Narrow] Epoch 012/015 | train_acc=0.7975 test_acc=0.8439\n",
      "[fashionmnist][Deep_Narrow] Epoch 013/015 | train_acc=0.8018 test_acc=0.8475\n",
      "[fashionmnist][Deep_Narrow] Epoch 014/015 | train_acc=0.8044 test_acc=0.8485\n",
      "[fashionmnist][Deep_Narrow] Epoch 015/015 | train_acc=0.8079 test_acc=0.8521\n",
      "Best test acc (pre-prune): 0.8521  -> saved to checkpoints_cnn_neff\\fashionmnist\\Deep_Narrow\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Deep_Narrow on fashionmnist ...\n",
      "Post-prune acc: 0.8415  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced on fashionmnist ===\n",
      "[fashionmnist][Balanced] Epoch 001/015 | train_acc=0.5944 test_acc=0.7855\n",
      "[fashionmnist][Balanced] Epoch 002/015 | train_acc=0.7462 test_acc=0.8167\n",
      "[fashionmnist][Balanced] Epoch 003/015 | train_acc=0.7853 test_acc=0.8363\n",
      "[fashionmnist][Balanced] Epoch 004/015 | train_acc=0.8050 test_acc=0.8469\n",
      "[fashionmnist][Balanced] Epoch 005/015 | train_acc=0.8231 test_acc=0.8536\n",
      "[fashionmnist][Balanced] Epoch 006/015 | train_acc=0.8340 test_acc=0.8658\n",
      "[fashionmnist][Balanced] Epoch 007/015 | train_acc=0.8428 test_acc=0.8700\n",
      "[fashionmnist][Balanced] Epoch 008/015 | train_acc=0.8490 test_acc=0.8754\n",
      "[fashionmnist][Balanced] Epoch 009/015 | train_acc=0.8546 test_acc=0.8765\n",
      "[fashionmnist][Balanced] Epoch 010/015 | train_acc=0.8616 test_acc=0.8812\n",
      "[fashionmnist][Balanced] Epoch 011/015 | train_acc=0.8673 test_acc=0.8806\n",
      "[fashionmnist][Balanced] Epoch 012/015 | train_acc=0.8700 test_acc=0.8859\n",
      "[fashionmnist][Balanced] Epoch 013/015 | train_acc=0.8728 test_acc=0.8881\n",
      "[fashionmnist][Balanced] Epoch 014/015 | train_acc=0.8759 test_acc=0.8898\n",
      "[fashionmnist][Balanced] Epoch 015/015 | train_acc=0.8780 test_acc=0.8934\n",
      "Best test acc (pre-prune): 0.8934  -> saved to checkpoints_cnn_neff\\fashionmnist\\Balanced\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Balanced on fashionmnist ...\n",
      "Post-prune acc: 0.8783  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced_Deep on fashionmnist ===\n",
      "[fashionmnist][Balanced_Deep] Epoch 001/020 | train_acc=0.6532 test_acc=0.8132\n",
      "[fashionmnist][Balanced_Deep] Epoch 002/020 | train_acc=0.8067 test_acc=0.8489\n",
      "[fashionmnist][Balanced_Deep] Epoch 003/020 | train_acc=0.8370 test_acc=0.8647\n",
      "[fashionmnist][Balanced_Deep] Epoch 004/020 | train_acc=0.8533 test_acc=0.8746\n",
      "[fashionmnist][Balanced_Deep] Epoch 005/020 | train_acc=0.8629 test_acc=0.8795\n",
      "[fashionmnist][Balanced_Deep] Epoch 006/020 | train_acc=0.8709 test_acc=0.8865\n",
      "[fashionmnist][Balanced_Deep] Epoch 007/020 | train_acc=0.8772 test_acc=0.8904\n",
      "[fashionmnist][Balanced_Deep] Epoch 008/020 | train_acc=0.8829 test_acc=0.8933\n",
      "[fashionmnist][Balanced_Deep] Epoch 009/020 | train_acc=0.8868 test_acc=0.8983\n",
      "[fashionmnist][Balanced_Deep] Epoch 010/020 | train_acc=0.8888 test_acc=0.9002\n",
      "[fashionmnist][Balanced_Deep] Epoch 011/020 | train_acc=0.8931 test_acc=0.9007\n",
      "[fashionmnist][Balanced_Deep] Epoch 012/020 | train_acc=0.8940 test_acc=0.9035\n",
      "[fashionmnist][Balanced_Deep] Epoch 013/020 | train_acc=0.8967 test_acc=0.9034\n",
      "[fashionmnist][Balanced_Deep] Epoch 014/020 | train_acc=0.9004 test_acc=0.9083\n",
      "[fashionmnist][Balanced_Deep] Epoch 015/020 | train_acc=0.9017 test_acc=0.9071\n",
      "[fashionmnist][Balanced_Deep] Epoch 016/020 | train_acc=0.9043 test_acc=0.9087\n",
      "[fashionmnist][Balanced_Deep] Epoch 017/020 | train_acc=0.9060 test_acc=0.9099\n",
      "[fashionmnist][Balanced_Deep] Epoch 018/020 | train_acc=0.9076 test_acc=0.9110\n",
      "[fashionmnist][Balanced_Deep] Epoch 019/020 | train_acc=0.9077 test_acc=0.9122\n",
      "[fashionmnist][Balanced_Deep] Epoch 020/020 | train_acc=0.9091 test_acc=0.9135\n",
      "Best test acc (pre-prune): 0.9135  -> saved to checkpoints_cnn_neff\\fashionmnist\\Balanced_Deep\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Balanced_Deep on fashionmnist ...\n",
      "Post-prune acc: 0.9076  -> pruned model saved.\n",
      "\n",
      "=== Training Wide on fashionmnist ===\n",
      "[fashionmnist][Wide] Epoch 001/030 | train_acc=0.8504 test_acc=0.8420\n",
      "[fashionmnist][Wide] Epoch 002/030 | train_acc=0.8997 test_acc=0.8944\n",
      "[fashionmnist][Wide] Epoch 003/030 | train_acc=0.9144 test_acc=0.8825\n",
      "[fashionmnist][Wide] Epoch 004/030 | train_acc=0.9248 test_acc=0.8663\n",
      "[fashionmnist][Wide] Epoch 005/030 | train_acc=0.9325 test_acc=0.9002\n",
      "[fashionmnist][Wide] Epoch 006/030 | train_acc=0.9415 test_acc=0.8593\n",
      "[fashionmnist][Wide] Epoch 007/030 | train_acc=0.9477 test_acc=0.8988\n",
      "[fashionmnist][Wide] Epoch 008/030 | train_acc=0.9530 test_acc=0.9188\n",
      "[fashionmnist][Wide] Epoch 009/030 | train_acc=0.9586 test_acc=0.9020\n",
      "[fashionmnist][Wide] Epoch 010/030 | train_acc=0.9657 test_acc=0.9114\n",
      "[fashionmnist][Wide] Epoch 011/030 | train_acc=0.9685 test_acc=0.8997\n",
      "[fashionmnist][Wide] Epoch 012/030 | train_acc=0.9734 test_acc=0.9118\n",
      "[fashionmnist][Wide] Epoch 013/030 | train_acc=0.9790 test_acc=0.9092\n",
      "[fashionmnist][Wide] Epoch 014/030 | train_acc=0.9823 test_acc=0.9070\n",
      "[fashionmnist][Wide] Epoch 015/030 | train_acc=0.9870 test_acc=0.8662\n",
      "[fashionmnist][Wide] Epoch 016/030 | train_acc=0.9858 test_acc=0.9031\n",
      "[fashionmnist][Wide] Epoch 017/030 | train_acc=0.9907 test_acc=0.9072\n",
      "[fashionmnist][Wide] Epoch 018/030 | train_acc=0.9930 test_acc=0.9181\n",
      "[fashionmnist][Wide] Epoch 019/030 | train_acc=0.9924 test_acc=0.9122\n",
      "[fashionmnist][Wide] Epoch 020/030 | train_acc=0.9945 test_acc=0.8778\n",
      "[fashionmnist][Wide] Epoch 021/030 | train_acc=0.9954 test_acc=0.8669\n",
      "[fashionmnist][Wide] Epoch 022/030 | train_acc=0.9940 test_acc=0.9158\n",
      "[fashionmnist][Wide] Epoch 023/030 | train_acc=0.9948 test_acc=0.9171\n",
      "[fashionmnist][Wide] Epoch 024/030 | train_acc=0.9985 test_acc=0.9120\n",
      "[fashionmnist][Wide] Epoch 025/030 | train_acc=0.9972 test_acc=0.9077\n",
      "[fashionmnist][Wide] Epoch 026/030 | train_acc=0.9949 test_acc=0.9001\n",
      "[fashionmnist][Wide] Epoch 027/030 | train_acc=0.9972 test_acc=0.9105\n",
      "[fashionmnist][Wide] Epoch 028/030 | train_acc=0.9965 test_acc=0.9056\n",
      "[fashionmnist][Wide] Epoch 029/030 | train_acc=0.9968 test_acc=0.9100\n",
      "[fashionmnist][Wide] Epoch 030/030 | train_acc=0.9984 test_acc=0.9124\n",
      "Best test acc (pre-prune): 0.9188  -> saved to checkpoints_cnn_neff\\fashionmnist\\Wide\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Wide on fashionmnist ...\n",
      "Post-prune acc: 0.5962  -> pruned model saved.\n",
      "[cifar10] device: cuda\n",
      "\n",
      "=== Training Tiny_Underfit on cifar10 ===\n",
      "[cifar10][Tiny_Underfit] Epoch 001/010 | train_acc=0.1294 test_acc=0.1728\n",
      "[cifar10][Tiny_Underfit] Epoch 002/010 | train_acc=0.2197 test_acc=0.2409\n",
      "[cifar10][Tiny_Underfit] Epoch 003/010 | train_acc=0.2428 test_acc=0.2537\n",
      "[cifar10][Tiny_Underfit] Epoch 004/010 | train_acc=0.2599 test_acc=0.2753\n",
      "[cifar10][Tiny_Underfit] Epoch 005/010 | train_acc=0.2777 test_acc=0.2983\n",
      "[cifar10][Tiny_Underfit] Epoch 006/010 | train_acc=0.2975 test_acc=0.3097\n",
      "[cifar10][Tiny_Underfit] Epoch 007/010 | train_acc=0.3060 test_acc=0.3203\n",
      "[cifar10][Tiny_Underfit] Epoch 008/010 | train_acc=0.3171 test_acc=0.3289\n",
      "[cifar10][Tiny_Underfit] Epoch 009/010 | train_acc=0.3238 test_acc=0.3298\n",
      "[cifar10][Tiny_Underfit] Epoch 010/010 | train_acc=0.3296 test_acc=0.3382\n",
      "Best test acc (pre-prune): 0.3382  -> saved to checkpoints_cnn_neff\\cifar10\\Tiny_Underfit\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Tiny_Underfit on cifar10 ...\n",
      "Post-prune acc: 0.2677  -> pruned model saved.\n",
      "\n",
      "=== Training Deep_Narrow on cifar10 ===\n",
      "[cifar10][Deep_Narrow] Epoch 001/015 | train_acc=0.1223 test_acc=0.1183\n",
      "[cifar10][Deep_Narrow] Epoch 002/015 | train_acc=0.1503 test_acc=0.1775\n",
      "[cifar10][Deep_Narrow] Epoch 003/015 | train_acc=0.1778 test_acc=0.2322\n",
      "[cifar10][Deep_Narrow] Epoch 004/015 | train_acc=0.2075 test_acc=0.2543\n",
      "[cifar10][Deep_Narrow] Epoch 005/015 | train_acc=0.2237 test_acc=0.2861\n",
      "[cifar10][Deep_Narrow] Epoch 006/015 | train_acc=0.2410 test_acc=0.2884\n",
      "[cifar10][Deep_Narrow] Epoch 007/015 | train_acc=0.2507 test_acc=0.2996\n",
      "[cifar10][Deep_Narrow] Epoch 008/015 | train_acc=0.2646 test_acc=0.3207\n",
      "[cifar10][Deep_Narrow] Epoch 009/015 | train_acc=0.2768 test_acc=0.3213\n",
      "[cifar10][Deep_Narrow] Epoch 010/015 | train_acc=0.2814 test_acc=0.3377\n",
      "[cifar10][Deep_Narrow] Epoch 011/015 | train_acc=0.2853 test_acc=0.3476\n",
      "[cifar10][Deep_Narrow] Epoch 012/015 | train_acc=0.2989 test_acc=0.3499\n",
      "[cifar10][Deep_Narrow] Epoch 013/015 | train_acc=0.3044 test_acc=0.3608\n",
      "[cifar10][Deep_Narrow] Epoch 014/015 | train_acc=0.3132 test_acc=0.3603\n",
      "[cifar10][Deep_Narrow] Epoch 015/015 | train_acc=0.3153 test_acc=0.3716\n",
      "Best test acc (pre-prune): 0.3716  -> saved to checkpoints_cnn_neff\\cifar10\\Deep_Narrow\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Deep_Narrow on cifar10 ...\n",
      "Post-prune acc: 0.3135  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced on cifar10 ===\n",
      "[cifar10][Balanced] Epoch 001/015 | train_acc=0.2516 test_acc=0.3968\n",
      "[cifar10][Balanced] Epoch 002/015 | train_acc=0.3255 test_acc=0.4306\n",
      "[cifar10][Balanced] Epoch 003/015 | train_acc=0.3618 test_acc=0.4553\n",
      "[cifar10][Balanced] Epoch 004/015 | train_acc=0.3900 test_acc=0.4791\n",
      "[cifar10][Balanced] Epoch 005/015 | train_acc=0.4105 test_acc=0.5030\n",
      "[cifar10][Balanced] Epoch 006/015 | train_acc=0.4259 test_acc=0.5193\n",
      "[cifar10][Balanced] Epoch 007/015 | train_acc=0.4433 test_acc=0.5315\n",
      "[cifar10][Balanced] Epoch 008/015 | train_acc=0.4562 test_acc=0.5466\n",
      "[cifar10][Balanced] Epoch 009/015 | train_acc=0.4659 test_acc=0.5494\n",
      "[cifar10][Balanced] Epoch 010/015 | train_acc=0.4798 test_acc=0.5557\n",
      "[cifar10][Balanced] Epoch 011/015 | train_acc=0.4851 test_acc=0.5654\n",
      "[cifar10][Balanced] Epoch 012/015 | train_acc=0.4950 test_acc=0.5788\n",
      "[cifar10][Balanced] Epoch 013/015 | train_acc=0.5005 test_acc=0.5783\n",
      "[cifar10][Balanced] Epoch 014/015 | train_acc=0.5119 test_acc=0.5842\n",
      "[cifar10][Balanced] Epoch 015/015 | train_acc=0.5169 test_acc=0.5915\n",
      "Best test acc (pre-prune): 0.5915  -> saved to checkpoints_cnn_neff\\cifar10\\Balanced\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Balanced on cifar10 ...\n",
      "Post-prune acc: 0.5211  -> pruned model saved.\n",
      "\n",
      "=== Training Balanced_Deep on cifar10 ===\n",
      "[cifar10][Balanced_Deep] Epoch 001/020 | train_acc=0.2300 test_acc=0.3817\n",
      "[cifar10][Balanced_Deep] Epoch 002/020 | train_acc=0.3269 test_acc=0.4384\n",
      "[cifar10][Balanced_Deep] Epoch 003/020 | train_acc=0.3737 test_acc=0.4793\n",
      "[cifar10][Balanced_Deep] Epoch 004/020 | train_acc=0.4046 test_acc=0.5092\n",
      "[cifar10][Balanced_Deep] Epoch 005/020 | train_acc=0.4286 test_acc=0.5295\n",
      "[cifar10][Balanced_Deep] Epoch 006/020 | train_acc=0.4492 test_acc=0.5527\n",
      "[cifar10][Balanced_Deep] Epoch 007/020 | train_acc=0.4666 test_acc=0.5586\n",
      "[cifar10][Balanced_Deep] Epoch 008/020 | train_acc=0.4848 test_acc=0.5841\n",
      "[cifar10][Balanced_Deep] Epoch 009/020 | train_acc=0.4968 test_acc=0.5901\n",
      "[cifar10][Balanced_Deep] Epoch 010/020 | train_acc=0.5135 test_acc=0.6032\n",
      "[cifar10][Balanced_Deep] Epoch 011/020 | train_acc=0.5238 test_acc=0.6091\n",
      "[cifar10][Balanced_Deep] Epoch 012/020 | train_acc=0.5355 test_acc=0.6246\n",
      "[cifar10][Balanced_Deep] Epoch 013/020 | train_acc=0.5484 test_acc=0.6324\n",
      "[cifar10][Balanced_Deep] Epoch 014/020 | train_acc=0.5573 test_acc=0.6382\n",
      "[cifar10][Balanced_Deep] Epoch 015/020 | train_acc=0.5633 test_acc=0.6494\n",
      "[cifar10][Balanced_Deep] Epoch 016/020 | train_acc=0.5720 test_acc=0.6535\n",
      "[cifar10][Balanced_Deep] Epoch 017/020 | train_acc=0.5803 test_acc=0.6596\n",
      "[cifar10][Balanced_Deep] Epoch 018/020 | train_acc=0.5875 test_acc=0.6661\n",
      "[cifar10][Balanced_Deep] Epoch 019/020 | train_acc=0.5946 test_acc=0.6703\n",
      "[cifar10][Balanced_Deep] Epoch 020/020 | train_acc=0.6013 test_acc=0.6754\n",
      "Best test acc (pre-prune): 0.6754  -> saved to checkpoints_cnn_neff\\cifar10\\Balanced_Deep\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Balanced_Deep on cifar10 ...\n",
      "Post-prune acc: 0.6385  -> pruned model saved.\n",
      "\n",
      "=== Training Wide on cifar10 ===\n",
      "[cifar10][Wide] Epoch 001/030 | train_acc=0.5311 test_acc=0.5424\n",
      "[cifar10][Wide] Epoch 002/030 | train_acc=0.6486 test_acc=0.5912\n",
      "[cifar10][Wide] Epoch 003/030 | train_acc=0.6895 test_acc=0.6255\n",
      "[cifar10][Wide] Epoch 004/030 | train_acc=0.7235 test_acc=0.6273\n",
      "[cifar10][Wide] Epoch 005/030 | train_acc=0.7496 test_acc=0.6791\n",
      "[cifar10][Wide] Epoch 006/030 | train_acc=0.7718 test_acc=0.6850\n",
      "[cifar10][Wide] Epoch 007/030 | train_acc=0.7889 test_acc=0.7358\n",
      "[cifar10][Wide] Epoch 008/030 | train_acc=0.8077 test_acc=0.6607\n",
      "[cifar10][Wide] Epoch 009/030 | train_acc=0.8267 test_acc=0.6858\n",
      "[cifar10][Wide] Epoch 010/030 | train_acc=0.8423 test_acc=0.7073\n",
      "[cifar10][Wide] Epoch 011/030 | train_acc=0.8563 test_acc=0.6833\n",
      "[cifar10][Wide] Epoch 012/030 | train_acc=0.8696 test_acc=0.7504\n",
      "[cifar10][Wide] Epoch 013/030 | train_acc=0.8845 test_acc=0.7314\n",
      "[cifar10][Wide] Epoch 014/030 | train_acc=0.8945 test_acc=0.6922\n",
      "[cifar10][Wide] Epoch 015/030 | train_acc=0.9045 test_acc=0.6994\n",
      "[cifar10][Wide] Epoch 016/030 | train_acc=0.9182 test_acc=0.7468\n",
      "[cifar10][Wide] Epoch 017/030 | train_acc=0.9297 test_acc=0.7431\n",
      "[cifar10][Wide] Epoch 018/030 | train_acc=0.9370 test_acc=0.7435\n",
      "[cifar10][Wide] Epoch 019/030 | train_acc=0.9460 test_acc=0.7717\n",
      "[cifar10][Wide] Epoch 020/030 | train_acc=0.9552 test_acc=0.7548\n",
      "[cifar10][Wide] Epoch 021/030 | train_acc=0.9608 test_acc=0.7811\n",
      "[cifar10][Wide] Epoch 022/030 | train_acc=0.9670 test_acc=0.7252\n",
      "[cifar10][Wide] Epoch 023/030 | train_acc=0.9714 test_acc=0.7538\n",
      "[cifar10][Wide] Epoch 024/030 | train_acc=0.9768 test_acc=0.7392\n",
      "[cifar10][Wide] Epoch 025/030 | train_acc=0.9809 test_acc=0.7539\n",
      "[cifar10][Wide] Epoch 026/030 | train_acc=0.9841 test_acc=0.7587\n",
      "[cifar10][Wide] Epoch 027/030 | train_acc=0.9858 test_acc=0.7617\n",
      "[cifar10][Wide] Epoch 028/030 | train_acc=0.9879 test_acc=0.7470\n",
      "[cifar10][Wide] Epoch 029/030 | train_acc=0.9900 test_acc=0.7322\n",
      "[cifar10][Wide] Epoch 030/030 | train_acc=0.9906 test_acc=0.7495\n",
      "Best test acc (pre-prune): 0.7811  -> saved to checkpoints_cnn_neff\\cifar10\\Wide\\best_preprune.pt\n",
      "Applying Neff pruning (per_tensor) to Wide on cifar10 ...\n",
      "Post-prune acc: 0.6761  -> pruned model saved.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    seed_everything(1337)\n",
    "    OUT_DIR = \"./checkpoints_cnn_neff\"\n",
    "\n",
    "    # Datasets you requested\n",
    "    datasets_to_run = [\"fashionmnist\", \"cifar10\"]\n",
    "\n",
    "    # Toggle pruning (default: train first, no pruning)\n",
    "    DO_PRUNE_AFTER_TRAIN = True\n",
    "    PRUNE_GRANULARITY = \"per_tensor\"\n",
    "\n",
    "    for dset in datasets_to_run:\n",
    "        train_models_on_dataset(\n",
    "            dataset_name=dset,\n",
    "            configs=cnn_model_configs,\n",
    "            batch_size=128,\n",
    "            test_batch_size=1000,\n",
    "            data_root=\"./data\",\n",
    "            out_dir=OUT_DIR,\n",
    "            do_prune_after_train=DO_PRUNE_AFTER_TRAIN,\n",
    "            prune_granularity=PRUNE_GRANULARITY\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748dad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
