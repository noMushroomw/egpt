{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615362bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def masked_sparsity(model: nn.Module, masks: Dict[str, torch.Tensor]) -> Tuple[int, int, float]:\n",
    "    total, zero = 0, 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'weight' in name and name in masks:\n",
    "            m = masks[name]\n",
    "            total += m.numel()\n",
    "            zero += int((m == 0).sum().item())\n",
    "    sp = 0.0 if total == 0 else zero / total\n",
    "    return total, zero, sp\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CNN Backbone\n",
    "# -----------------------------\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.drop = nn.Dropout2d(p=dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        # He init\n",
    "        nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Channels list -> sequence of ConvBlocks. MaxPool(2) after every 2 blocks.\n",
    "    Global Average Pooling -> Linear head.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, num_classes: int, channels: List[int], dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert len(channels) >= 1, \"Provide at least one conv block.\"\n",
    "\n",
    "        blocks = []\n",
    "        c_in = in_channels\n",
    "        for idx, c_out in enumerate(channels):\n",
    "            blocks.append(ConvBlock(c_in, c_out, dropout=dropout))\n",
    "            if (idx + 1) % 2 == 0:\n",
    "                blocks.append(nn.MaxPool2d(kernel_size=2))\n",
    "            c_in = c_out\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        self.head = nn.Linear(channels[-1], num_classes)\n",
    "        nn.init.kaiming_normal_(self.head.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # GAP head\n",
    "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-2)  # (B, C)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Neff Pruning\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class NeffConfig:\n",
    "    beta: float = 1.0              # keep top floor(beta * Neff)\n",
    "    scope: str = \"tensor\"          # \"tensor\" or \"per_out_channel\"\n",
    "    prune_bias: bool = False\n",
    "    # schedule\n",
    "    prune_at_epoch: Optional[int] = None  # if None: no pruning; else apply at given epoch (1-indexed)\n",
    "    prune_every: Optional[int] = None     # if set (e.g., 1,2,5), prune every K epochs starting after warmup_epochs\n",
    "    warmup_epochs: int = 1               # epochs before periodic pruning starts\n",
    "    verbose: bool = True\n",
    "\n",
    "\n",
    "def _compute_neff_from_abs(abs_vec: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    abs_vec: flattened absolute values (>=0)\n",
    "    Returns Neff = floor(1 / sum(p_i^2)), where p = abs_vec / abs_vec.sum() (if sum>0).\n",
    "    Guarantees at least 1 when sum>0; returns 0 if all zeros.\n",
    "    \"\"\"\n",
    "    sum_abs = abs_vec.sum()\n",
    "    if sum_abs <= 0:\n",
    "        return 0\n",
    "    p = abs_vec / sum_abs\n",
    "    # numerical guard: use float32 for stability\n",
    "    neff = int(torch.floor(1.0 / torch.clamp((p * p).sum(), min=1e-12)).item())\n",
    "    neff = max(neff, 1)\n",
    "    return neff\n",
    "\n",
    "\n",
    "def _topk_mask_by_neff(weight: torch.Tensor, beta: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    One-shot mask for the entire tensor.\n",
    "    \"\"\"\n",
    "    w = weight.detach()\n",
    "    w_abs = w.abs().reshape(-1)\n",
    "    N = w_abs.numel()\n",
    "    if N == 0:\n",
    "        return torch.ones_like(weight)\n",
    "\n",
    "    neff = _compute_neff_from_abs(w_abs)\n",
    "    k = max(1, min(N, int(math.floor(beta * neff))))\n",
    "    # If abs sum is zero, neff returns 0 -> clamp k to 1 => keep one arbitrary largest (which will be 0)\n",
    "    vals, idx = torch.topk(w_abs, k, largest=True, sorted=False)\n",
    "    mask_flat = torch.zeros(N, dtype=w.dtype, device=w.device)\n",
    "    mask_flat[idx] = 1.0\n",
    "    return mask_flat.view_as(weight)\n",
    "\n",
    "\n",
    "def _topk_mask_by_neff_per_out_channel(weight: torch.Tensor, beta: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Per-output-channel Neff pruning.\n",
    "    Conv: weight shape (out, in, kH, kW)\n",
    "    Linear: weight shape (out, in)  -> \"out\" treated as per-row\n",
    "    \"\"\"\n",
    "    w = weight.detach()\n",
    "    if w.ndim == 2:\n",
    "        O, I = w.shape\n",
    "        mask = torch.zeros_like(w)\n",
    "        for o in range(O):\n",
    "            vec = w[o].reshape(-1).abs()\n",
    "            N = vec.numel()\n",
    "            neff = _compute_neff_from_abs(vec)\n",
    "            k = max(1, min(N, int(math.floor(beta * neff)))) if N > 0 else 0\n",
    "            if k > 0:\n",
    "                idx = torch.topk(vec, k, largest=True, sorted=False).indices\n",
    "                mask[o].view(-1)[idx] = 1.0\n",
    "        return mask\n",
    "    elif w.ndim == 4:\n",
    "        O, I, KH, KW = w.shape\n",
    "        mask = torch.zeros_like(w)\n",
    "        for o in range(O):\n",
    "            vec = w[o].reshape(-1).abs()\n",
    "            N = vec.numel()\n",
    "            neff = _compute_neff_from_abs(vec)\n",
    "            k = max(1, min(N, int(math.floor(beta * neff)))) if N > 0 else 0\n",
    "            if k > 0:\n",
    "                idx = torch.topk(vec, k, largest=True, sorted=False).indices\n",
    "                mask[o].view(-1)[idx] = 1.0\n",
    "        return mask\n",
    "    else:\n",
    "        # Fallback to whole-tensor if unexpected shape\n",
    "        return _topk_mask_by_neff(w, beta)\n",
    "\n",
    "\n",
    "def build_neff_masks(model: nn.Module, cfg: NeffConfig) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Build masks for Conv2d/Linear weights according to Neff (β·Neff).\n",
    "    \"\"\"\n",
    "    masks = {}\n",
    "    with torch.no_grad():\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                if hasattr(module, \"weight\") and module.weight is not None:\n",
    "                    wname = f\"{name}.weight\"\n",
    "                    if cfg.scope == \"per_out_channel\":\n",
    "                        mask = _topk_mask_by_neff_per_out_channel(module.weight, cfg.beta)\n",
    "                    else:\n",
    "                        mask = _topk_mask_by_neff(module.weight, cfg.beta)\n",
    "                    masks[wname] = mask.to(module.weight.dtype)\n",
    "                if cfg.prune_bias and hasattr(module, \"bias\") and module.bias is not None:\n",
    "                    bname = f\"{name}.bias\"\n",
    "                    # Bias magnitudes are generally tiny; pruning them rarely helps. Still, we support it.\n",
    "                    mask_b = _topk_mask_by_neff(module.bias, cfg.beta)\n",
    "                    masks[bname] = mask_b.to(module.bias.dtype)\n",
    "    return masks\n",
    "\n",
    "\n",
    "def apply_masks_inplace(model: nn.Module, masks: Dict[str, torch.Tensor]):\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in masks:\n",
    "                param.mul_(masks[name])\n",
    "\n",
    "\n",
    "def attach_gradient_mask_hooks(model: nn.Module, masks: Dict[str, torch.Tensor]):\n",
    "    \"\"\"\n",
    "    Ensure pruned weights stay zero by zeroing their gradients.\n",
    "    \"\"\"\n",
    "    for name, p in model.named_parameters():\n",
    "        if name in masks:\n",
    "            m = masks[name]\n",
    "            p.register_hook(lambda g, m=m: g * m)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training / Evaluation\n",
    "# -----------------------------\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return (loss_sum / max(1, total)), (correct / max(1, total))\n",
    "\n",
    "\n",
    "def train_one_model(\n",
    "    name: str,\n",
    "    cfg: Dict,\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    neff_cfg: Optional[NeffConfig] = None,\n",
    "    weight_decay: float = 0.0,\n",
    "    grad_clip: Optional[float] = None,\n",
    ") -> Dict[str, float]:\n",
    "    model = model.to(device)\n",
    "    epochs = int(cfg.get(\"epochs\", 10))\n",
    "    lr = float(cfg.get(\"lr\", 3e-4))\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Placeholders for masks (applied if/when pruning happens)\n",
    "    masks: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "    # Initial eval\n",
    "    val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "    print(f\"[{name}] Init  | val_loss={val_loss:.4f}  val_acc={val_acc:.4f}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            if grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            # re-apply masks post-step to keep sparsity enforced\n",
    "            if masks:\n",
    "                apply_masks_inplace(model, masks)\n",
    "\n",
    "        # Scheduling: when to prune\n",
    "        did_prune = False\n",
    "        if neff_cfg is not None:\n",
    "            if neff_cfg.prune_at_epoch is not None and epoch == int(neff_cfg.prune_at_epoch):\n",
    "                masks = build_neff_masks(model, neff_cfg)\n",
    "                attach_gradient_mask_hooks(model, masks)\n",
    "                apply_masks_inplace(model, masks)\n",
    "                did_prune = True\n",
    "\n",
    "            if neff_cfg.prune_every is not None:\n",
    "                if epoch > neff_cfg.warmup_epochs and ((epoch - neff_cfg.warmup_epochs) % neff_cfg.prune_every == 0):\n",
    "                    masks = build_neff_masks(model, neff_cfg)\n",
    "                    attach_gradient_mask_hooks(model, masks)\n",
    "                    apply_masks_inplace(model, masks)\n",
    "                    did_prune = True\n",
    "\n",
    "        vloss, vacc = evaluate(model, val_loader, device)\n",
    "        if neff_cfg and did_prune and neff_cfg.verbose:\n",
    "            total, zero, sp = masked_sparsity(model, masks)\n",
    "            print(f\"[{name}] Epoch {epoch:02d}  val_loss={vloss:.4f}  val_acc={vacc:.4f}  \"\n",
    "                  f\"PRUNE: β={neff_cfg.beta:.3f}, scope={neff_cfg.scope}, sparsity={sp:.3%} ({zero}/{total})\")\n",
    "        else:\n",
    "            print(f\"[{name}] Epoch {epoch:02d}  val_loss={vloss:.4f}  val_acc={vacc:.4f}\")\n",
    "\n",
    "    total_params = count_params(model)\n",
    "    total, zero, sp = masked_sparsity(model, masks) if masks else (0, 0, 0.0)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"[{name}] Done in {elapsed:.1f}s | params={total_params/1e6:.2f}M | sparsity={sp:.3%}\")\n",
    "\n",
    "    return {\n",
    "        \"val_acc\": vacc,\n",
    "        \"val_loss\": vloss,\n",
    "        \"params\": float(total_params),\n",
    "        \"sparsity\": float(sp),\n",
    "        \"elapsed_sec\": float(elapsed),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# High-level training harness\n",
    "# -----------------------------\n",
    "\n",
    "def build_cnn_from_config(\n",
    "    in_channels: int,\n",
    "    num_classes: int,\n",
    "    cfg: Dict,\n",
    ") -> nn.Module:\n",
    "    channels = cfg.get(\"hidden_size\", None) or cfg.get(\"channels\", None)\n",
    "    if channels is None:\n",
    "        raise ValueError(\"Config must include 'hidden_size' (interpreted as conv channels list) or 'channels'.\")\n",
    "    dropout = float(cfg.get(\"dropout\", 0.0))\n",
    "    return SimpleCNN(in_channels, num_classes, channels=channels, dropout=dropout)\n",
    "\n",
    "\n",
    "def train_cnn_suite(\n",
    "    model_configs: Dict[str, Dict],\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    in_channels: int,\n",
    "    num_classes: int,\n",
    "    device: Optional[torch.device] = None,\n",
    "    use_models: Optional[List[str]] = None,\n",
    "    neff_cfg: Optional[NeffConfig] = None,\n",
    "    weight_decay: float = 0.0,\n",
    "    grad_clip: Optional[float] = None,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Trains a selection of models in model_configs and returns a dict of final metrics per model.\n",
    "    \"\"\"\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = {}\n",
    "    names = use_models or list(model_configs.keys())\n",
    "\n",
    "    for name in names:\n",
    "        cfg = model_configs[name]\n",
    "        model = build_cnn_from_config(in_channels, num_classes, cfg)\n",
    "        print(f\"\\n=== Training {name} ===\")\n",
    "        print(model)\n",
    "        stats = train_one_model(\n",
    "            name=name,\n",
    "            cfg=cfg,\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            neff_cfg=neff_cfg,\n",
    "            weight_decay=weight_decay,\n",
    "            grad_clip=grad_clip,\n",
    "        )\n",
    "        results[name] = stats\n",
    "    return results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example config (copy/paste yours or modify)\n",
    "# -----------------------------\n",
    "\n",
    "model_configs = {\n",
    "    # Underfit & brittle\n",
    "    'Tiny_Underfit': {\n",
    "        'hidden_size': [64],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 10,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    # Deep-narrow (depth sensitivity)\n",
    "    'Deep_Narrow': {\n",
    "        'hidden_size': [128, 128, 128, 128, 128, 128, 128, 128],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 15,\n",
    "        'dropout': 0.2\n",
    "    },\n",
    "    # Well-trained baseline\n",
    "    'Balanced': {\n",
    "        'hidden_size': [512, 256],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 15,\n",
    "        'dropout': 0.2\n",
    "    },\n",
    "    # Deep but still robust\n",
    "    'Balanced_Deep': {\n",
    "        'hidden_size': [512, 256, 128, 64],\n",
    "        'lr': 3e-4,\n",
    "        'epochs': 20,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    # Overparameterized (note: very high channel counts may be memory-heavy)\n",
    "    'Wide': {\n",
    "        'hidden_size': [2048, 1024],\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 30,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    # Very overparameterized (optional)\n",
    "    'Very_Wide': {\n",
    "        'hidden_size': [4096, 2048, 1024, 512],\n",
    "        'lr': 1e-3,\n",
    "        'epochs': 50,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# How you'll call this (you provide loaders)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    seed_everything(0)\n",
    "\n",
    "    # You supply these:\n",
    "    # train_loader: DataLoader over (image, label)\n",
    "    # val_loader:   DataLoader over (image, label)\n",
    "    #\n",
    "    # Example:\n",
    "    # train_loader, val_loader = make_your_fashionmnist_loaders(...)\n",
    "    # OR\n",
    "    # train_loader, val_loader = make_your_cifar10_loaders(...)\n",
    "\n",
    "    # Placeholder to show interface (remove after wiring your loaders)\n",
    "    raise SystemExit(\n",
    "        \"Wire your own train_loader/val_loader, then call train_cnn_suite(...). \"\n",
    "        \"See below for the exact call signature.\"\n",
    "    )\n",
    "\n",
    "    # After you create dataloaders, infer shapes (optional)\n",
    "    # sample_x, sample_y = next(iter(train_loader))\n",
    "    # in_channels = sample_x.shape[1]\n",
    "    # num_classes = int(torch.max(sample_y).item()) + 1  # or known (10 for FMNIST/CIFAR-10)\n",
    "\n",
    "    # Or set explicitly for each dataset:\n",
    "    # Fashion-MNIST\n",
    "    # in_channels, num_classes = 1, 10\n",
    "    # CIFAR-10\n",
    "    # in_channels, num_classes = 3, 10\n",
    "\n",
    "    # Choose five (skip Very_Wide by default)\n",
    "    # use_models = ['Tiny_Underfit', 'Deep_Narrow', 'Balanced', 'Balanced_Deep', 'Wide']\n",
    "\n",
    "    # Neff pruning config examples:\n",
    "    # 1) One-shot β·Neff at epoch 1 (after warmup step)\n",
    "    # neff_cfg = NeffConfig(beta=1.0, scope=\"tensor\", prune_at_epoch=1, verbose=True)\n",
    "    #\n",
    "    # 2) Periodic β·Neff every 2 epochs after 1 warmup epoch, per-out-channel masks\n",
    "    # neff_cfg = NeffConfig(beta=1.0, scope=\"per_out_channel\", prune_every=2, warmup_epochs=1, verbose=True)\n",
    "    #\n",
    "    # Run:\n",
    "    # results = train_cnn_suite(\n",
    "    #     model_configs=model_configs,\n",
    "    #     train_loader=train_loader,\n",
    "    #     val_loader=val_loader,\n",
    "    #     in_channels=in_channels,\n",
    "    #     num_classes=num_classes,\n",
    "    #     device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    #     use_models=use_models,\n",
    "    #     neff_cfg=neff_cfg,\n",
    "    #     weight_decay=0.0,\n",
    "    #     grad_clip=None,\n",
    "    # )\n",
    "    # print(\"Final results:\", results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
