{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kan_neff_pruning.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "@torch.no_grad()\n",
    "def kan_mask_per_edge_basis(module: nn.Module,\n",
    "                            basis_scale: torch.Tensor,\n",
    "                            beta: float = 1.0,\n",
    "                            coeff_attr: str = \"coeff\",\n",
    "                            renormalize: bool = False):\n",
    "    \"\"\"\n",
    "    Neff on basis coefficients per edge (i,j).\n",
    "\n",
    "    Args:\n",
    "      module: KAN-like layer with parameter `coeff` of shape [out, in, K].\n",
    "      basis_scale: tensor [in, K] with ||phi_{j,k}(x_j)||_2 over calibration data.\n",
    "      beta: multiplicative factor for Neff.\n",
    "      coeff_attr: attribute name of coefficients.\n",
    "      renormalize: if True, preserve L1 per (i,j) over basis after masking.\n",
    "\n",
    "    Returns:\n",
    "      mask: bool tensor [out, in, K] with True=keep.\n",
    "      neff_ij: floor(Neff) per (i,j) as float tensor [out, in].\n",
    "    \"\"\"\n",
    "    C = getattr(module, coeff_attr).data      # [out, in, K]\n",
    "    out, din, K = C.shape\n",
    "    S = C.abs() * basis_scale.to(C.device).view(1, din, K)   # [out,in,K]\n",
    "\n",
    "    # Normalize across basis k per (i,j)\n",
    "    Ssum = S.sum(dim=2, keepdim=True) + 1e-12\n",
    "    P = S / Ssum\n",
    "    neff = 1.0 / (P.pow(2).sum(dim=2))       # [out, in]\n",
    "    r = torch.floor(beta * neff).clamp_(min=1, max=K).to(torch.long)\n",
    "\n",
    "    # sort within each (i,j) across K\n",
    "    P2 = P.reshape(-1, K)                    # [(out*in), K]\n",
    "    _, idx = torch.sort(P2, dim=1, descending=True)\n",
    "    range_k = torch.arange(K, device=C.device).view(1, -1).expand(P2.size(0), -1)\n",
    "    keep = range_k < r.view(-1, 1)\n",
    "    mask_flat = torch.zeros_like(P2, dtype=torch.bool)\n",
    "    mask_flat.scatter_(1, idx, keep)\n",
    "    mask = mask_flat.view(out, din, K)\n",
    "\n",
    "    # apply\n",
    "    C_masked = C * mask\n",
    "    if renormalize:\n",
    "        pre = C.abs().sum(dim=2, keepdim=True)\n",
    "        post = C_masked.abs().sum(dim=2, keepdim=True)\n",
    "        C_masked = C_masked * (pre / (post + 1e-12))\n",
    "\n",
    "    getattr(module, coeff_attr).data.copy_(C_masked)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def kan_mask_per_output_inputs(module: nn.Module,\n",
    "                               basis_scale: torch.Tensor,\n",
    "                               beta: float = 1.0,\n",
    "                               coeff_attr: str = \"coeff\",\n",
    "                               renormalize: bool = False):\n",
    "    \"\"\"\n",
    "    Neff on input edges j per output i (collapse basis first).\n",
    "\n",
    "    Args:\n",
    "      module: KAN-like layer with parameter `coeff` of shape [out, in, K].\n",
    "      basis_scale: tensor [in, K] with ||phi_{j,k}(x_j)||_2.\n",
    "      beta: multiplicative factor for Neff.\n",
    "      renormalize: if True, preserves L1 per row (i) across inputs j (over all K).\n",
    "\n",
    "    Returns:\n",
    "      mask_ijk: bool mask [out, in, K] (True=keep).\n",
    "      neff_i: floor(Neff) per output i, tensor [out].\n",
    "    \"\"\"\n",
    "    C = getattr(module, coeff_attr).data      # [out, in, K]\n",
    "    out, din, K = C.shape\n",
    "    S = C.abs() * basis_scale.to(C.device).view(1, din, K)   # [out,in,K]\n",
    "    E = S.sum(dim=2)                           # collapse basis => [out, in]\n",
    "\n",
    "    # Normalize per row i across inputs j\n",
    "    Esum = E.sum(dim=1, keepdim=True) + 1e-12\n",
    "    P = E / Esum                               # [out, in]\n",
    "    neff = 1.0 / (P.pow(2).sum(dim=1))         # [out]\n",
    "    r = torch.floor(beta * neff).clamp_(min=1, max=din).to(torch.long)\n",
    "\n",
    "    # top-r per row\n",
    "    _, idx = torch.sort(P, dim=1, descending=True)\n",
    "    range_j = torch.arange(din, device=C.device).view(1, -1).expand(out, -1)\n",
    "    keep_inputs = range_j < r.unsqueeze(1)     # [out, in]\n",
    "\n",
    "    # expand to (i,j,k)\n",
    "    mask = keep_inputs.unsqueeze(-1).expand(out, din, K)\n",
    "\n",
    "    C_masked = C * mask\n",
    "    if renormalize:\n",
    "        pre = C.abs().sum(dim=(1,2), keepdim=True)    # L1 per row across j,k\n",
    "        post = C_masked.abs().sum(dim=(1,2), keepdim=True)\n",
    "        C_masked = C_masked * (pre / (post + 1e-12))\n",
    "\n",
    "    getattr(module, coeff_attr).data.copy_(C_masked)\n",
    "    return mask, torch.floor(neff)\n",
    "\n",
    "\n",
    "def model_kan_prune(model: nn.Module,\n",
    "                    basis_scales_by_name: dict,\n",
    "                    beta: float = 1.0,\n",
    "                    mode: str = \"per_edge_basis\",\n",
    "                    coeff_attr: str = \"coeff\",\n",
    "                    layer_matcher=lambda m: hasattr(m, \"coeff\"),\n",
    "                    renormalize: bool = False):\n",
    "    \"\"\"\n",
    "    Apply KAN Neff pruning to all matched layers.\n",
    "\n",
    "    basis_scales_by_name[name] must be [in, K].\n",
    "    mode in {\"per_edge_basis\", \"per_output_inputs\"}.\n",
    "    \"\"\"\n",
    "    pruned = copy.deepcopy(model)\n",
    "    for name, m in pruned.named_modules():\n",
    "        if layer_matcher(m):\n",
    "            basis_scale = basis_scales_by_name[name]\n",
    "            if mode == \"per_edge_basis\":\n",
    "                kan_mask_per_edge_basis(m, basis_scale, beta, coeff_attr, renormalize)\n",
    "            elif mode == \"per_output_inputs\":\n",
    "                kan_mask_per_output_inputs(m, basis_scale, beta, coeff_attr, renormalize)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown mode\")\n",
    "    return pruned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9637743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "=== Train ORIGINAL KAN ===\n",
      "  epoch 01/15 | train MSE: 0.296288\n",
      "  epoch 02/15 | train MSE: 0.118026\n",
      "  epoch 03/15 | train MSE: 0.038095\n",
      "  epoch 04/15 | train MSE: 0.022506\n",
      "  epoch 05/15 | train MSE: 0.016441\n",
      "  epoch 06/15 | train MSE: 0.013015\n",
      "  epoch 07/15 | train MSE: 0.010353\n",
      "  epoch 08/15 | train MSE: 0.008936\n",
      "  epoch 09/15 | train MSE: 0.007627\n",
      "  epoch 10/15 | train MSE: 0.006760\n",
      "  epoch 11/15 | train MSE: 0.006020\n",
      "  epoch 12/15 | train MSE: 0.005554\n",
      "  epoch 13/15 | train MSE: 0.005169\n",
      "  epoch 14/15 | train MSE: 0.005272\n",
      "  epoch 15/15 | train MSE: 0.004794\n",
      "\n",
      "Evaluate ORIGINAL:\n",
      "Test MSE (original): 0.004648\n",
      "\n",
      "=== Neff PRUNE trained KAN (per-edge basis) ===\n",
      "Sparsity on KAN.coeff: 38.82% (2506/4096 nonzeros)\n",
      "\n",
      "Evaluate PRUNED (one-shot, no finetune):\n",
      "Test MSE (pruned, one-shot): 0.010057\n",
      "\n",
      "=== Finetune PRUNED KAN (masked) ===\n",
      "  epoch 01/10 | train MSE: 0.005856\n",
      "  epoch 02/10 | train MSE: 0.004529\n",
      "  epoch 03/10 | train MSE: 0.004515\n",
      "  epoch 04/10 | train MSE: 0.004306\n",
      "  epoch 05/10 | train MSE: 0.004222\n",
      "  epoch 06/10 | train MSE: 0.004205\n",
      "  epoch 07/10 | train MSE: 0.004102\n",
      "  epoch 08/10 | train MSE: 0.003972\n",
      "  epoch 09/10 | train MSE: 0.004121\n",
      "  epoch 10/10 | train MSE: 0.004050\n",
      "\n",
      "Evaluate PRUNED after finetune:\n",
      "Test MSE (pruned, finetune): 0.003915\n",
      "\n",
      "=== SUMMARY ===\n",
      "Original params in KAN.coeff: 4096\n",
      "Kept after Neff pruning:      2506  (sparsity = 38.82%)\n",
      "Test MSE original:            0.004648\n",
      "Test MSE pruned (one-shot):   0.010057\n",
      "Test MSE pruned (finetuned):  0.003915\n"
     ]
    }
   ],
   "source": [
    "# kan_neff_demo.py\n",
    "# ------------------------------------------------------------\n",
    "# KAN training + Neff pruning + evaluation + comparison\n",
    "# ------------------------------------------------------------\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Utilities\n",
    "# --------------------------\n",
    "def set_seed(seed: int = 1234):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def device_of(x):\n",
    "    return x.device if isinstance(x, torch.Tensor) else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def count_nonzero(t: torch.Tensor):\n",
    "    return (t != 0).sum().item()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Synthetic regression data\n",
    "# --------------------------\n",
    "def make_regression(n_train=20000, n_test=2000, in_dim=4, noise=0.05, seed=0):\n",
    "    gen = torch.Generator().manual_seed(seed)\n",
    "    Xtr = 2 * torch.rand(n_train, in_dim, generator=gen) - 1.0\n",
    "    Xte = 2 * torch.rand(n_test, in_dim, generator=gen) - 1.0\n",
    "\n",
    "    def f(X):\n",
    "        x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3]\n",
    "        y = torch.sin(2 * math.pi * x1) + 0.5 * x2**2 - x3 * x4 + 0.3 * x1 * x2\n",
    "        return y.unsqueeze(1)\n",
    "\n",
    "    ytr = f(Xtr) + noise * torch.randn(n_train, 1, generator=gen)\n",
    "    yte = f(Xte) + noise * torch.randn(n_test, 1, generator=gen)\n",
    "    return (Xtr, ytr), (Xte, yte)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# KAN layer (fixed RBF basis)\n",
    "# y = sum_{j=1}^in sum_{k=1}^K C[i,j,k] * phi_k(x_j) + b_i\n",
    "# --------------------------\n",
    "class KANLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, K=16, sigma=0.25):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.K = K\n",
    "        # Fixed RBF centers in [-1,1]\n",
    "        centers = torch.linspace(-1.0, 1.0, K)\n",
    "        self.register_buffer(\"centers\", centers)      # [K]\n",
    "        self.register_buffer(\"sigma\", torch.tensor(float(sigma)))\n",
    "        # Coefficients and bias\n",
    "        self.coeff = nn.Parameter(torch.empty(out_features, in_features, K))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        # Optional pruning mask (set later)\n",
    "        self.register_buffer(\"mask\", None, persistent=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Small init helps stability\n",
    "        nn.init.normal_(self.coeff, mean=0.0, std=0.1)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def phi(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, in_features]\n",
    "        returns: phi [B, in_features, K]\n",
    "        \"\"\"\n",
    "        # Broadcast over K basis\n",
    "        # (x - c)^2 / (2 sigma^2)\n",
    "        diff = x.unsqueeze(-1) - self.centers.view(1, 1, -1)\n",
    "        phi = torch.exp(-0.5 * (diff / self.sigma).pow(2))\n",
    "        return phi\n",
    "\n",
    "    def forward(self, x):\n",
    "        # phi: [B, in, K], coeff: [out, in, K]\n",
    "        phi = self.phi(x)\n",
    "        coeff = self.coeff if self.mask is None else self.coeff * self.mask\n",
    "        y = torch.einsum(\"bik,oik->bo\", phi, coeff) + self.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "class KANRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple KAN model: one KAN layer -> tanh -> linear head\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=4, hidden=64, K=16, sigma=0.25):\n",
    "        super().__init__()\n",
    "        self.kan = KANLayer(in_dim, hidden, K=K, sigma=sigma)\n",
    "        self.act = nn.Tanh()\n",
    "        self.head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.act(self.kan(x))\n",
    "        y = self.head(z)\n",
    "        return y\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Neff pruning for KAN (exactly your rule)\n",
    "# --------------------------\n",
    "@torch.no_grad()\n",
    "def neff_prune_kan_per_edge_basis(layer: KANLayer, beta: float = 1.0, renormalize: bool = False):\n",
    "    \"\"\"\n",
    "    For each (i,j), consider the series w_k = |C[i,j,k]|.\n",
    "    p_k = w_k / sum_k w_k\n",
    "    Neff_ij = floor(1 / sum_k p_k^2)\n",
    "    Keep the top floor(beta * Neff_ij) basis coefficients by magnitude and zero the rest.\n",
    "\n",
    "    This is EXACTLY your Neff rule (no activation scaling, no extra factors).\n",
    "    \"\"\"\n",
    "    C = layer.coeff.data  # [out, in, K]\n",
    "    out, din, K = C.shape\n",
    "\n",
    "    w = C.abs()\n",
    "    wsum = w.sum(dim=2, keepdim=True)                  # [out, in, 1]\n",
    "    # Normalize to simplex; if wsum==0 => p is 0; the clamp below ensures we keep at least 1\n",
    "    p = w / (wsum + 1e-12)\n",
    "    neff = 1.0 / (p.pow(2).sum(dim=2))                 # [out, in]\n",
    "    r = torch.floor(beta * neff).clamp_(min=1, max=K).long()\n",
    "\n",
    "    # Sort by |C| along k, keep top-r per (i,j)\n",
    "    w_flat = w.view(-1, K)                              # [(out*in), K]\n",
    "    _, idx = torch.sort(w_flat, dim=1, descending=True)\n",
    "    range_k = torch.arange(K, device=C.device).view(1, -1).expand(w_flat.size(0), -1)\n",
    "    keep = range_k < r.view(-1, 1)\n",
    "\n",
    "    mask_flat = torch.zeros_like(w_flat, dtype=torch.bool)\n",
    "    mask_flat.scatter_(1, idx, keep)\n",
    "    mask = mask_flat.view(out, din, K)                 # bool\n",
    "\n",
    "    # Apply mask\n",
    "    C_masked = C * mask\n",
    "    if renormalize:\n",
    "        pre = C.abs().sum(dim=2, keepdim=True)         # L1 per (i,j)\n",
    "        post = C_masked.abs().sum(dim=2, keepdim=True)\n",
    "        C_masked = C_masked * (pre / (post + 1e-12))\n",
    "\n",
    "    layer.coeff.data.copy_(C_masked)\n",
    "    # store float mask to zero grads during finetune\n",
    "    layer.mask = mask.to(C.dtype)\n",
    "\n",
    "    # return useful stats\n",
    "    kept = mask.sum().item()\n",
    "    total = mask.numel()\n",
    "    sparsity = 1.0 - kept / total\n",
    "    return {\"neff\": neff, \"mask\": mask, \"kept\": kept, \"total\": total, \"sparsity\": sparsity}\n",
    "\n",
    "\n",
    "def attach_grad_mask(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Make sure pruned parameters stay zero during finetuning.\n",
    "    \"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, KANLayer) and m.mask is not None:\n",
    "            mask = m.mask  # float tensor [out,in,K]\n",
    "            def _hook_factory(msk):\n",
    "                return lambda g: g * msk\n",
    "            m._coeff_mask_hook = m.coeff.register_hook(_hook_factory(mask))\n",
    "\n",
    "\n",
    "def remove_grad_mask(model: nn.Module):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, KANLayer) and hasattr(m, \"_coeff_mask_hook\"):\n",
    "            m._coeff_mask_hook.remove()\n",
    "            delattr(m, \"_coeff_mask_hook\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Training / evaluation\n",
    "# --------------------------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 15\n",
    "    batch_size: int = 512\n",
    "    lr: float = 3e-3\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "\n",
    "def train(model, loader, cfg: TrainConfig, device):\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    for epoch in range(cfg.epochs):\n",
    "        total = 0.0\n",
    "        n = 0\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb)\n",
    "            loss = F.mse_loss(pred, yb)\n",
    "            loss.backward()\n",
    "            # ensure zeros remain zeros if masks exist\n",
    "            for m in model.modules():\n",
    "                if isinstance(m, KANLayer) and m.mask is not None:\n",
    "                    if m.coeff.grad is not None:\n",
    "                        m.coeff.grad.mul_(m.mask)\n",
    "            opt.step()\n",
    "            # hard-enforce zeros\n",
    "            with torch.no_grad():\n",
    "                for m in model.modules():\n",
    "                    if isinstance(m, KANLayer) and m.mask is not None:\n",
    "                        m.coeff.mul_(m.mask)\n",
    "            total += loss.item() * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "        print(f\"  epoch {epoch+1:02d}/{cfg.epochs} | train MSE: {total/n:.6f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = F.mse_loss(pred, yb)\n",
    "        total += loss.item() * xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    return total / n\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Main experiment\n",
    "# --------------------------\n",
    "def main():\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Data\n",
    "    (Xtr, ytr), (Xte, yte) = make_regression(in_dim=4, n_train=20000, n_test=2000, noise=0.05, seed=2024)\n",
    "    train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=512, shuffle=True, drop_last=False)\n",
    "    test_loader = DataLoader(TensorDataset(Xte, yte), batch_size=2048, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    model_orig = KANRegressor(in_dim=4, hidden=64, K=16, sigma=0.25).to(device)\n",
    "    print(\"\\n=== Train ORIGINAL KAN ===\")\n",
    "    cfg = TrainConfig(epochs=15, batch_size=512, lr=3e-3, weight_decay=0.0)\n",
    "    model_orig = train(model_orig, train_loader, cfg, device)\n",
    "\n",
    "    print(\"\\nEvaluate ORIGINAL:\")\n",
    "    mse_orig = evaluate(model_orig, test_loader, device)\n",
    "    print(f\"Test MSE (original): {mse_orig:.6f}\")\n",
    "\n",
    "    # Clone and prune with Neff\n",
    "    print(\"\\n=== Neff PRUNE trained KAN (per-edge basis) ===\")\n",
    "    model_pruned = copy.deepcopy(model_orig).to(device)\n",
    "    pr_stats = neff_prune_kan_per_edge_basis(model_pruned.kan, beta=1.0, renormalize=False)\n",
    "    print(f\"Sparsity on KAN.coeff: {pr_stats['sparsity']*100:.2f}% \"\n",
    "          f\"({pr_stats['kept']}/{pr_stats['total']} nonzeros)\")\n",
    "\n",
    "    print(\"\\nEvaluate PRUNED (one-shot, no finetune):\")\n",
    "    mse_pruned_oneshot = evaluate(model_pruned, test_loader, device)\n",
    "    print(f\"Test MSE (pruned, one-shot): {mse_pruned_oneshot:.6f}\")\n",
    "\n",
    "    # Finetune pruned (zeros stay zeros)\n",
    "    print(\"\\n=== Finetune PRUNED KAN (masked) ===\")\n",
    "    attach_grad_mask(model_pruned)\n",
    "    cfg_ft = TrainConfig(epochs=10, batch_size=512, lr=2e-3, weight_decay=0.0)\n",
    "    model_pruned = train(model_pruned, train_loader, cfg_ft, device)\n",
    "    remove_grad_mask(model_pruned)\n",
    "\n",
    "    print(\"\\nEvaluate PRUNED after finetune:\")\n",
    "    mse_pruned_ft = evaluate(model_pruned, test_loader, device)\n",
    "    print(f\"Test MSE (pruned, finetune): {mse_pruned_ft:.6f}\")\n",
    "\n",
    "    # Summary\n",
    "    total_coeff = model_orig.kan.coeff.numel()\n",
    "    kept_coeff = count_nonzero(model_pruned.kan.coeff.data)\n",
    "    sparsity_pct = 100.0 * (1.0 - kept_coeff / total_coeff)\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Original params in KAN.coeff: {total_coeff}\")\n",
    "    print(f\"Kept after Neff pruning:      {kept_coeff}  (sparsity = {sparsity_pct:.2f}%)\")\n",
    "    print(f\"Test MSE original:            {mse_orig:.6f}\")\n",
    "    print(f\"Test MSE pruned (one-shot):   {mse_pruned_oneshot:.6f}\")\n",
    "    print(f\"Test MSE pruned (finetuned):  {mse_pruned_ft:.6f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e4f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
