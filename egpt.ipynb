{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbcbfd4",
   "metadata": {},
   "source": [
    "## start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923d9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0666b",
   "metadata": {},
   "source": [
    "# 1. get linear mask for effective weight with each weight size [output_size, input_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a772fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cfecd",
   "metadata": {},
   "source": [
    "# 2. set the edge with ineffective weight = 0 and prune the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c3e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449c769",
   "metadata": {},
   "source": [
    "# 3. train a linear model first and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e6e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size= [512, 512, 512]):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))        \n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371eb00",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba7578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "\n",
    "# MINIST-10 dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearModel(input_size=28*28, output_size=10, hidden_size=[1024, 512, 512]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce11e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3056, Accuracy: 1029/10000 (10.29%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305180\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.495303\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.278514\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.125834\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.176045\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.109889\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.107494\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.139744\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.191475\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.171283\n",
      "\n",
      "Test set: Average loss: 0.1143, Accuracy: 9639/10000 (96.39%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.065967\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.060971\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.090003\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.083971\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.042842\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.055389\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.094108\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.032641\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.144378\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.059296\n",
      "\n",
      "Test set: Average loss: 0.0814, Accuracy: 9739/10000 (97.39%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.067475\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.073596\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.062184\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.004916\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.110915\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.015800\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.098188\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014093\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.120269\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.048120\n",
      "\n",
      "Test set: Average loss: 0.0894, Accuracy: 9734/10000 (97.34%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006772\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.104749\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.035405\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.036577\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.038117\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.041802\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.086180\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.038864\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.038543\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.012724\n",
      "\n",
      "Test set: Average loss: 0.0681, Accuracy: 9796/10000 (97.96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.030332\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.003205\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.098527\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.008775\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.027595\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.016926\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.009290\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.014110\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.043761\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.031315\n",
      "\n",
      "Test set: Average loss: 0.0838, Accuracy: 9775/10000 (97.75%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.006765\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.004340\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.025968\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.040493\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.114737\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.092904\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.009017\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.006599\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.025102\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.021521\n",
      "\n",
      "Test set: Average loss: 0.0741, Accuracy: 9803/10000 (98.03%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.000882\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.084190\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.022588\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.005124\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.002672\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.033371\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.069494\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.008692\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.041024\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001298\n",
      "\n",
      "Test set: Average loss: 0.0736, Accuracy: 9815/10000 (98.15%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.017782\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.000915\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.028983\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.065813\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.023282\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.002450\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.002670\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.003050\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.033277\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.026982\n",
      "\n",
      "Test set: Average loss: 0.0725, Accuracy: 9829/10000 (98.29%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001313\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.003310\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.001040\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.004548\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001718\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.003793\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.025318\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.000227\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.000574\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000889\n",
      "\n",
      "Test set: Average loss: 0.0801, Accuracy: 9799/10000 (97.99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.010456\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.033570\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000824\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000832\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000331\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001024\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.000105\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000274\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000776\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.015663\n",
      "\n",
      "Test set: Average loss: 0.0800, Accuracy: 9809/10000 (98.09%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': []\n",
    "}\n",
    "# initial test\n",
    "test_loss, test_accuracy = test(model, device, test_loader)\n",
    "result['test_loss'].append(test_loss)\n",
    "result['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    result['train_loss'].append(train_loss)\n",
    "    result['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    # Test after each epoch\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    result['test_loss'].append(test_loss)\n",
    "    result['test_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'linear_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bef4b4",
   "metadata": {},
   "source": [
    "# prune the model and comparing the performance with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779e3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n"
     ]
    }
   ],
   "source": [
    "pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "pruned_model_renormalized.to(device)\n",
    "\n",
    "# Test the pruned model\n",
    "test_loss, test_accuracy = test(pruned_model_renormalized, device, test_loader)\n",
    "print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model\n",
    "torch.save(pruned_model_renormalized.state_dict(), 'pruned_linear_model_renormalized.pth')\n",
    "\n",
    "\n",
    "prune_model = prune_model_neff(model, renormalize=False)\n",
    "prune_model.to(device)\n",
    "# Test the pruned model without renormalization\n",
    "test_loss, test_accuracy = test(prune_model, device, test_loader)\n",
    "print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model without renormalization\n",
    "torch.save(prune_model.state_dict(), 'pruned_linear_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5985428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "\n",
      "Test set: Average loss: 12157359320308445798030016249856.0000, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Pruned Model - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n",
      "\n",
      "Test set: Average loss: 0.0744, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "Average Pruned Model - Test Loss: 0.0744, Test Accuracy: 98.10%\n",
      "Average Pruned Model with Renormalization - Test Loss: 12157359320308445798030016249856.0000, Test Accuracy: 94.69%\n"
     ]
    }
   ],
   "source": [
    "# test 10 times and show the average performance\n",
    "test_loss_acc = {'prune_loss': [], 'prune_accuracy': [], 'prune_renorm_loss': [], 'prune_renorm_accuracy': []}\n",
    "\n",
    "for i in range(10):\n",
    "    pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "    pruned_model_renormalized.to(device)\n",
    "\n",
    "    # Test the pruned model\n",
    "    test_loss, test_accuracy = test(pruned_model_renormalized, device, test_loader)\n",
    "    print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    test_loss_acc['prune_renorm_loss'].append(test_loss)\n",
    "    test_loss_acc['prune_renorm_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    pruned_model = prune_model_neff(model, renormalize=False)\n",
    "    pruned_model.to(device)\n",
    "    # Test the pruned model without renormalization\n",
    "    test_loss, test_accuracy = test(pruned_model, device, test_loader)\n",
    "    print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    test_loss_acc['prune_loss'].append(test_loss)\n",
    "    test_loss_acc['prune_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# average performance\n",
    "avg_prune_loss = np.mean(test_loss_acc['prune_loss'])\n",
    "avg_prune_accuracy = np.mean(test_loss_acc['prune_accuracy'])\n",
    "avg_prune_renorm_loss = np.mean(test_loss_acc['prune_renorm_loss'])\n",
    "avg_prune_renorm_accuracy = np.mean(test_loss_acc['prune_renorm_accuracy'])\n",
    "\n",
    "print(f'Average Pruned Model - Test Loss: {avg_prune_loss:.4f}, Test Accuracy: {avg_prune_accuracy:.2f}%')\n",
    "print(f'Average Pruned Model with Renormalization - Test Loss: {avg_prune_renorm_loss:.4f}, Test Accuracy: {avg_prune_renorm_accuracy:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c0b22",
   "metadata": {},
   "source": [
    "# measure the sparisity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac14e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d2897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the pruned model with renormalization: 0.3419\n",
      "Sparsity of the pruned model without renormalization: 0.3419\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sparsity = model_sparsity(pruned_model_renormalized)\n",
    "print(f'Sparsity of the pruned model with renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(prune_model)\n",
    "print(f'Sparsity of the pruned model without renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5bea2",
   "metadata": {},
   "source": [
    "# test in huggingface language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "860d3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, tokenized_dataset, batch_size=32):\n",
    "    from torch.utils.data import DataLoader\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35295451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_32828\\2390932059.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "raw_dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "test_dataset = raw_dataset[\"test\"]\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize and format\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# 4. Load and fine-tune BERT on train split\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "196726d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Original Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37500/37500 36:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.229300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned original model: Sparsity=0.0000, Acc=0.9439\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Training Original Model ===\")\n",
    "trainer.train()\n",
    "\n",
    "orig_acc = compute_accuracy(model, tokenized_test_dataset)\n",
    "sparsity = model_sparsity(model)\n",
    "print(f\"\\nFine-tuned original model: Sparsity={sparsity:.4f}, Acc={orig_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'bert_origin.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c621ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned model: Sparsity=0.2923, Acc=0.9451\n",
      "Pruned model with renormalization: Sparsity=0.2923, Acc=0.2500\n"
     ]
    }
   ],
   "source": [
    "pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "pruned_model_renormalized.to(device)\n",
    "pruned_model = prune_model_neff(model, renormalize=False)\n",
    "pruned_model.to(device)\n",
    "\n",
    "prune_acc = compute_accuracy(pruned_model, tokenized_test_dataset)\n",
    "prune_renorm_acc = compute_accuracy(pruned_model_renormalized, tokenized_test_dataset)\n",
    "pruned_sparsity = model_sparsity(pruned_model)\n",
    "pruned_renorm_sparsity = model_sparsity(pruned_model_renormalized)\n",
    "\n",
    "print(f\"\\nPruned model: Sparsity={pruned_sparsity:.4f}, Acc={prune_acc:.4f}\")\n",
    "print(f\"Pruned model with renormalization: Sparsity={pruned_renorm_sparsity:.4f}, Acc={prune_renorm_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfadfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model.state_dict(), 'bert_pruned.pth')\n",
    "torch.save(pruned_model_renormalized.state_dict(), 'bert_pruned_renorm.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f57e2",
   "metadata": {},
   "source": [
    "## test for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8814919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen-7B\"  # Use \"Qwen/Qwen-7B\" for smaller variant\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "DEVICE_MAP = \"auto\"  # Automatically distributes across GPUs\n",
    "BATCH_SIZE = 1  # Reduce if OOM errors occur\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(DATASET_NAME, DATASET_CONFIG, split=\"test\")\n",
    "texts = [text for text in test_dataset[\"text\"] if text.strip()]  # Remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32ccce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc91e7bfefe4daebcd1682a4905170d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 2891/2891 [03:12<00:00, 15.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 17.44\n"
     ]
    }
   ],
   "source": [
    "# Load model with quantization (4-bit) to reduce VRAM usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=DEVICE_MAP,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config={\"load_in_4bit\": True},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Calculate perplexity\n",
    "model.eval()\n",
    "total_log_likelihood = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(texts, desc=\"Calculating Perplexity\"):\n",
    "        # Tokenize text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        \n",
    "        # Forward pass to get loss\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "\n",
    "# Final perplexity calculation\n",
    "perplexity = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'qwen_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c12ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_llm(model, renormalize=False):\n",
    "    \"\"\"\n",
    "    Prune LLM model by targeting only standard Linear layers\n",
    "    Avoids quantized layers and special layer types\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(model)\n",
    "    pruned_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Only prune standard nn.Linear layers, avoid quantized layers\n",
    "        if isinstance(module, nn.Linear) and not hasattr(module, 'quant_state'):\n",
    "            try:\n",
    "                mask = get_linear_mask(module).to(module.weight.device)\n",
    "                with torch.no_grad():\n",
    "                    module.weight *= mask.float()\n",
    "                    \n",
    "                    if renormalize:\n",
    "                        # More stable renormalization\n",
    "                        row_sum = module.weight.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                        module.weight.div_(row_sum)\n",
    "                    \n",
    "                    pruned_layers.append(name)\n",
    "                    \n",
    "                    # Check sparsity of this layer\n",
    "                    sparsity = (module.weight == 0).float().mean().item()\n",
    "                    print(f\"Pruned {name}: {sparsity:.2%} sparsity\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Successfully pruned {len(pruned_layers)} layers\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c53f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned lm_head: 39.97% sparsity\n",
      "Successfully pruned 1 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity for Pruned Model: 100%|██████████| 2891/2891 [04:20<00:00, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Pruned Model: 17.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pruned_Qwen = prune_model_neff_llm(model, renormalize=False)\n",
    "pruned_Qwen.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(texts, desc=\"Calculating Perplexity for Pruned Model\"):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(pruned_Qwen.device)\n",
    "        outputs = pruned_Qwen(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "        \n",
    "perplexity_pruned = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity of Pruned Model: {perplexity_pruned:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee734096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear4bit(in_features=4096, out_features=12288, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (w2): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (c_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc215aa1",
   "metadata": {},
   "source": [
    "# test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f622de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- 2. Storage and nonzero counting ----\n",
    "def get_model_size(model, tmp_file=\"tmp_model.bin\"):\n",
    "    torch.save(model.state_dict(), tmp_file)\n",
    "    size_mb = os.path.getsize(tmp_file) / (1024 * 1024)\n",
    "    os.remove(tmp_file)\n",
    "    return size_mb\n",
    "\n",
    "def count_nonzero_params(model):\n",
    "    nonzero = 0\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "        nonzero += (p != 0).sum().item()\n",
    "    return total, nonzero\n",
    "\n",
    "def get_folder_size_mb(folder):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)\n",
    "\n",
    "# ---- 3. Sparse export/load ----\n",
    "def export_model_sparse(model, out_dir=\"sparse_export\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    meta = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            w = module.weight.data.cpu().numpy()\n",
    "            nonzero = np.nonzero(w)\n",
    "            values = w[nonzero]\n",
    "            indices = np.vstack(nonzero).T\n",
    "            np.save(os.path.join(out_dir, f\"{name}_values.npy\"), values)\n",
    "            np.save(os.path.join(out_dir, f\"{name}_indices.npy\"), indices)\n",
    "            meta[name] = {\"shape\": w.shape, \"n_nonzero\": len(values)}\n",
    "            if module.bias is not None:\n",
    "                np.save(os.path.join(out_dir, f\"{name}_bias.npy\"), module.bias.data.cpu().numpy())\n",
    "    with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "    print(f\"Exported sparse weights to {out_dir}\")\n",
    "\n",
    "def load_model_sparse(model, sparse_dir=\"sparse_export\"):\n",
    "    with open(os.path.join(sparse_dir, \"meta.json\")) as f:\n",
    "        meta = json.load(f)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and name in meta:\n",
    "            shape = tuple(meta[name][\"shape\"])\n",
    "            w = np.zeros(shape, dtype=np.float32)\n",
    "            values = np.load(os.path.join(sparse_dir, f\"{name}_values.npy\"))\n",
    "            indices = np.load(os.path.join(sparse_dir, f\"{name}_indices.npy\"))\n",
    "            w[indices[:,0], indices[:,1]] = values\n",
    "            module.weight.data = torch.tensor(w, dtype=module.weight.data.dtype, device=module.weight.data.device)\n",
    "            bias_path = os.path.join(sparse_dir, f\"{name}_bias.npy\")\n",
    "            if os.path.exists(bias_path):\n",
    "                module.bias.data = torch.tensor(np.load(bias_path), dtype=module.bias.data.dtype, device=module.bias.data.device)\n",
    "    print(f\"Loaded sparse weights from {sparse_dir}\")\n",
    "    return model\n",
    "\n",
    "# ---- 4. Accuracy function ----\n",
    "def compute_accuracy(model, tokenized_dataset, batch_size=32):\n",
    "    from torch.utils.data import DataLoader\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "# ---- 5. Tokenization and data setup ----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "raw_dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "test_dataset = raw_dataset[\"test\"]\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ---- 6. Train original model ----\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Training Original Model ===\")\n",
    "trainer.train()\n",
    "\n",
    "# ---- 7. Evaluate and analyze original ----\n",
    "orig_acc = compute_accuracy(model, tokenized_test_dataset)\n",
    "orig_size = get_model_size(model)\n",
    "orig_total, orig_nonzero = count_nonzero_params(model)\n",
    "print(f\"\\n original model: Size={orig_size:.2f} MB, Acc={orig_acc:.4f}, Nonzeros={orig_nonzero}/{orig_total}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"dense_model.pt\")\n",
    "\n",
    "# ---- 8. Prune, save, and analyze pruned model ----\n",
    "pruned_model = prune_model_neff(model, renormalize=False).to(device)\n",
    "pruned_acc = compute_accuracy(pruned_model, tokenized_test_dataset)\n",
    "pruned_size = get_model_size(pruned_model)\n",
    "pruned_total, pruned_nonzero = count_nonzero_params(pruned_model)\n",
    "print(f\"Pruned model:             Size={pruned_size:.2f} MB, Acc={pruned_acc:.4f}, Nonzeros={pruned_nonzero}/{pruned_total}\")\n",
    "\n",
    "torch.save(pruned_model.state_dict(), \"dense_pruned_model.pt\")\n",
    "export_model_sparse(pruned_model, out_dir=\"sparse_export\")\n",
    "sparse_disk_size = get_folder_size_mb(\"sparse_export\")\n",
    "print(f\"Sparse export folder size: {sparse_disk_size:.2f} MB\")\n",
    "\n",
    "# ---- 10. Load sparse weights into a new model and compare ----\n",
    "reloaded_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "reloaded_model = load_model_sparse(reloaded_model, sparse_dir=\"sparse_export\")\n",
    "reload_acc = compute_accuracy(reloaded_model, tokenized_test_dataset)\n",
    "reload_total, reload_nonzero = count_nonzero_params(reloaded_model)\n",
    "print(f\"Reloaded sparse model:    Acc={reload_acc:.4f}, Nonzeros={reload_nonzero}/{reload_total}\")\n",
    "\n",
    "finetune_args_pruned = TrainingArguments(\n",
    "    output_dir=\"./tmp_pruned_bert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "trainer_pruned = Trainer(\n",
    "    model=reloaded_model,\n",
    "    args=finetune_args_pruned,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Fine-tuning Pruned Model ===\")\n",
    "trainer_pruned.train()\n",
    "finetuned_acc = compute_accuracy(reloaded_model, tokenized_test_dataset)\n",
    "finetuned_size = get_model_size(reloaded_model)\n",
    "finetuned_total, finetuned_nonzero = count_nonzero_params(reloaded_model)\n",
    "print(f\"Fine-tuned pruned model:  Size={finetuned_size:.2f} MB, Acc={finetuned_acc:.4f}, Nonzeros={finetuned_nonzero}/{finetuned_total}\")\n",
    "\n",
    "# ---- 11. Dense vs. Sparse Storage ----\n",
    "import os\n",
    "dense_size = os.path.getsize(\"dense_pruned_model.pt\") / (1024*1024)\n",
    "print(f\"Dense .pt pruned model size: {dense_size:.2f} MB\")\n",
    "print(f\"Sparse export folder size:   {sparse_disk_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Original   : {orig_size:.2f} MB, Acc={orig_acc:.4f}, Nonzeros={orig_nonzero}\")\n",
    "print(f\"Pruned     : {pruned_size:.2f} MB, Acc={pruned_acc:.4f}, Nonzeros={pruned_nonzero}\")\n",
    "print(f\"Finetuned  : {finetuned_size:.2f} MB, Acc={finetuned_acc:.4f}, Nonzeros={finetuned_nonzero}\")\n",
    "print(f\"Sparse export folder size:   {sparse_disk_size:.2f} MB\")\n",
    "print(f\"Dense .pt pruned model size: {dense_size:.2f} MB\")\n",
    "print(f\"Reloaded   : Acc={reload_acc:.4f}, Nonzeros={reload_nonzero}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c4d55",
   "metadata": {},
   "source": [
    "## test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78151f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "\n",
    "# Custom callback to print loss\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and 'loss' in logs:\n",
    "            print(f\"\\nStep {state.global_step}: Loss = {logs['loss']:.4f}\")\n",
    "\n",
    "# ---- 1. N_eff-based mask/prune functions ----\n",
    "def get_neff_mask_linear(module):\n",
    "    w = module.weight.data\n",
    "    out_features, in_features = w.shape\n",
    "    w_abs = torch.abs(w)\n",
    "    norm_factor = w_abs.sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "    w_norm = w_abs / norm_factor\n",
    "    w_norm_sum_sq = (w_norm**2).sum(dim=1)\n",
    "    neff = torch.clamp(torch.floor(1.0 / w_norm_sum_sq), min=1).long()\n",
    "    k_max = neff.max().item()\n",
    "    topk_vals, _ = torch.topk(w_norm, k=k_max, dim=1, sorted=False)\n",
    "    thresholds = topk_vals[torch.arange(out_features), neff-1].unsqueeze(1)\n",
    "    mask = (w_norm >= thresholds).float()\n",
    "    return mask\n",
    "\n",
    "def prune_model_neff(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_neff_mask_linear(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n",
    "\n",
    "# ---- 2. Storage and nonzero counting ----\n",
    "def get_model_size(model, tmp_file=\"tmp_model.bin\"):\n",
    "    torch.save(model.state_dict(), tmp_file)\n",
    "    size_mb = os.path.getsize(tmp_file) / (1024 * 1024)\n",
    "    os.remove(tmp_file)\n",
    "    return size_mb\n",
    "\n",
    "def count_nonzero_params(model):\n",
    "    nonzero = 0\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "        nonzero += (p != 0).sum().item()\n",
    "    return total, nonzero\n",
    "\n",
    "def get_folder_size_mb(folder):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)\n",
    "\n",
    "# ---- 3. Efficient Sparse export/load ----\n",
    "def export_model_sparse_efficient(model, out_dir=\"sparse_export\"):\n",
    "    \"\"\"Export pruned model in sparse format for efficient storage\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    sparse_data = {\n",
    "        'sparse_weights': {},\n",
    "        'dense_params': {},\n",
    "        'metadata': {}\n",
    "    }\n",
    "    \n",
    "    total_params = 0\n",
    "    sparse_params = 0\n",
    "    \n",
    "    for name, param in state_dict.items():\n",
    "        param_numpy = param.cpu().numpy()\n",
    "        \n",
    "        # Only make 2D weight matrices sparse\n",
    "        if 'weight' in name and len(param.shape) == 2:\n",
    "            # Convert to CSR sparse matrix\n",
    "            sparse_matrix = sp.csr_matrix(param_numpy)\n",
    "            sparse_data['sparse_weights'][name] = {\n",
    "                'data': sparse_matrix.data.astype(np.float16),  # Use float16 for compression\n",
    "                'indices': sparse_matrix.indices.astype(np.int32),  # Use int32 instead of int64\n",
    "                'indptr': sparse_matrix.indptr.astype(np.int32),\n",
    "                'shape': sparse_matrix.shape\n",
    "            }\n",
    "            \n",
    "            nnz = sparse_matrix.nnz\n",
    "            total = param_numpy.shape[0] * param_numpy.shape[1]\n",
    "            sparsity = 1 - (nnz / total)\n",
    "            \n",
    "            sparse_data['metadata'][name] = {\n",
    "                'shape': list(param.shape),\n",
    "                'nnz': nnz,\n",
    "                'total': total,\n",
    "                'sparsity': sparsity,\n",
    "                'dtype': str(param.dtype)\n",
    "            }\n",
    "            \n",
    "            total_params += total\n",
    "            sparse_params += nnz\n",
    "            \n",
    "            print(f\"  {name}: {nnz}/{total} ({sparsity:.1%} sparse)\")\n",
    "        else:\n",
    "            # Keep other parameters dense\n",
    "            sparse_data['dense_params'][name] = param_numpy\n",
    "    \n",
    "    # Save everything in one compressed file\n",
    "    with open(os.path.join(out_dir, \"model_sparse.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(sparse_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    overall_sparsity = 1 - (sparse_params / total_params)\n",
    "    print(f\"\\nOverall sparsity: {overall_sparsity:.1%}\")\n",
    "    print(f\"Exported to {out_dir}\")\n",
    "    \n",
    "    return sparse_data['metadata']\n",
    "\n",
    "def load_model_sparse_efficient(model, sparse_dir=\"sparse_export\"):\n",
    "    \"\"\"Load sparse model back\"\"\"\n",
    "    sparse_path = os.path.join(sparse_dir, \"model_sparse.pkl\")\n",
    "    \n",
    "    with open(sparse_path, \"rb\") as f:\n",
    "        sparse_data = pickle.load(f)\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    # Reconstruct sparse weights\n",
    "    for name in state_dict:\n",
    "        if name in sparse_data['sparse_weights']:\n",
    "            sw = sparse_data['sparse_weights'][name]\n",
    "            # First convert data back to float32 before creating sparse matrix\n",
    "            data_float32 = sw['data'].astype(np.float32)\n",
    "            # Reconstruct CSR matrix with float32 data\n",
    "            sparse_matrix = sp.csr_matrix(\n",
    "                (data_float32, sw['indices'], sw['indptr']), \n",
    "                shape=sw['shape'],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            # Convert back to dense tensor\n",
    "            dense_array = sparse_matrix.toarray()\n",
    "            new_state_dict[name] = torch.from_numpy(dense_array).to(state_dict[name].dtype)\n",
    "        elif name in sparse_data['dense_params']:\n",
    "            new_state_dict[name] = torch.from_numpy(\n",
    "                sparse_data['dense_params'][name]\n",
    "            ).to(state_dict[name].dtype)\n",
    "        else:\n",
    "            # Keep original if not found\n",
    "            new_state_dict[name] = state_dict[name]\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    print(f\"Loaded sparse model from {sparse_dir}\")\n",
    "    return model\n",
    "\n",
    "# ---- 4. Accuracy function ----\n",
    "def compute_accuracy(model, tokenized_dataset, batch_size=32):\n",
    "    from torch.utils.data import DataLoader\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "# ---- Main execution ----\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    raw_dataset = load_dataset(\"ag_news\")\n",
    "    train_dataset = raw_dataset[\"train\"]\n",
    "    test_dataset = raw_dataset[\"test\"]\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128\n",
    "        )\n",
    "\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    tokenized_test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    # Train original model\n",
    "    print(\"=== Phase 1: Training Original Model ===\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "    finetune_args = TrainingArguments(\n",
    "        output_dir=\"./tmp_finetuned_bert\",\n",
    "        per_device_train_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        logging_steps=1000,  # Log every 1000 steps\n",
    "        logging_dir='./logs',  # TensorBoard logs\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[\"tensorboard\"],  # Enable TensorBoard logging\n",
    "        log_level=\"info\",  # Ensure info level logging\n",
    "        disable_tqdm=False,  # Keep progress bar\n",
    "        evaluation_strategy=\"steps\",  # Evaluate during training\n",
    "        eval_steps=1000,  # Evaluate every 1000 steps\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=finetune_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[LossLoggingCallback()],  # Add custom callback\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate original\n",
    "    orig_acc = compute_accuracy(model, tokenized_test_dataset)\n",
    "    orig_size = get_model_size(model)\n",
    "    orig_total, orig_nonzero = count_nonzero_params(model)\n",
    "    print(f\"\\n✓ Original model: {orig_size:.2f} MB, Accuracy={orig_acc:.4f}\")\n",
    "\n",
    "    # Prune model\n",
    "    print(\"\\n=== Phase 2: Pruning Model ===\")\n",
    "    pruned_model = prune_model_neff(model, renormalize=False).to(device)\n",
    "    pruned_acc = compute_accuracy(pruned_model, tokenized_test_dataset)\n",
    "    pruned_total, pruned_nonzero = count_nonzero_params(pruned_model)\n",
    "    sparsity = 1 - (pruned_nonzero / pruned_total)\n",
    "    print(f\"✓ Pruned model: Accuracy={pruned_acc:.4f} (dropped {orig_acc-pruned_acc:.4f})\")\n",
    "    print(f\"  Sparsity: {sparsity:.1%} ({pruned_total-pruned_nonzero:,} zeros)\")\n",
    "\n",
    "    # Export sparse\n",
    "    print(\"\\n=== Phase 3: Saving Sparse Model ===\")\n",
    "    metadata = export_model_sparse_efficient(pruned_model, out_dir=\"sparse_export\")\n",
    "    sparse_disk_size = get_folder_size_mb(\"sparse_export\")\n",
    "    compression_ratio = orig_size / sparse_disk_size\n",
    "    print(f\"✓ Sparse storage: {sparse_disk_size:.2f} MB (compression ratio: {compression_ratio:.1f}x)\")\n",
    "\n",
    "    # Compare with dense storage\n",
    "    torch.save(pruned_model.state_dict(), \"pruned_dense.pt\")\n",
    "    dense_pruned_size = os.path.getsize(\"pruned_dense.pt\") / (1024 * 1024)\n",
    "    print(f\"  Dense storage would be: {dense_pruned_size:.2f} MB\")\n",
    "    os.remove(\"pruned_dense.pt\")\n",
    "\n",
    "    # Load sparse model\n",
    "    print(\"\\n=== Phase 4: Loading Sparse Model ===\")\n",
    "    reloaded_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "    reloaded_model = load_model_sparse_efficient(reloaded_model, sparse_dir=\"sparse_export\")\n",
    "    reload_acc = compute_accuracy(reloaded_model, tokenized_test_dataset)\n",
    "    reload_total, reload_nonzero = count_nonzero_params(reloaded_model)\n",
    "    \n",
    "    # Check if loading worked correctly\n",
    "    acc_diff = abs(pruned_acc - reload_acc)\n",
    "    if acc_diff > 0.001:\n",
    "        print(f\"⚠️  WARNING: Accuracy changed by {acc_diff:.4f} after reload!\")\n",
    "    else:\n",
    "        print(f\"✓ Loaded successfully: Accuracy={reload_acc:.4f} (preserved)\")\n",
    "\n",
    "    # ONE-SHOT RECOVERY TRAINING\n",
    "    print(\"\\n=== Phase 5: ONE-SHOT Recovery Training ===\")\n",
    "    print(\"This is the key innovation: recovering accuracy with just 1 epoch!\")\n",
    "    \n",
    "    oneshot_args = TrainingArguments(\n",
    "        output_dir=\"./tmp_oneshot\",\n",
    "        per_device_train_batch_size=16,\n",
    "        num_train_epochs=1,  # Just ONE epoch!\n",
    "        logging_steps=1000,  # Log every 1000 steps\n",
    "        logging_dir='./logs_oneshot',\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[\"tensorboard\"],\n",
    "        log_level=\"info\",\n",
    "        disable_tqdm=False,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "    )\n",
    "    oneshot_trainer = Trainer(\n",
    "        model=reloaded_model,\n",
    "        args=oneshot_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[LossLoggingCallback()],  # Add custom callback\n",
    "    )\n",
    "    oneshot_trainer.train()\n",
    "    \n",
    "    # Evaluate after one-shot training\n",
    "    oneshot_acc = compute_accuracy(reloaded_model, tokenized_test_dataset)\n",
    "    oneshot_total, oneshot_nonzero = count_nonzero_params(reloaded_model)\n",
    "    \n",
    "    print(f\"\\n✓ After ONE-SHOT training: Accuracy={oneshot_acc:.4f}\")\n",
    "    print(f\"  Accuracy recovered: {oneshot_acc - reload_acc:.4f} → almost back to {orig_acc:.4f}!\")\n",
    "    print(f\"  Nonzero params: {oneshot_nonzero:,} (zeros filled back during training)\")\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY: Your One-Shot Recovery Technique\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"1. Original model    : {orig_size:.2f} MB, Acc={orig_acc:.4f}\")\n",
    "    print(f\"2. After pruning     : Acc={pruned_acc:.4f} (↓{orig_acc-pruned_acc:.4f}), {sparsity:.1%} sparse\")\n",
    "    print(f\"3. Sparse storage    : {sparse_disk_size:.2f} MB ({compression_ratio:.1f}x smaller)\")\n",
    "    print(f\"4. After loading     : Acc={reload_acc:.4f}\")\n",
    "    print(f\"5. ONE-SHOT recovery : Acc={oneshot_acc:.4f} (↑{oneshot_acc-reload_acc:.4f})\")\n",
    "    print(f\"\\n✨ Key insight: Just 1 epoch recovers {(oneshot_acc/orig_acc)*100:.1f}% of original accuracy!\")\n",
    "    print(f\"   Even though zeros get filled back, the sparse storage + one-shot\")\n",
    "    print(f\"   training gives you a compressed model delivery pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4ee39",
   "metadata": {},
   "source": [
    "## test3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# ---- 1. Improved N_eff-based pruning ----\n",
    "def get_neff_mask_linear(module):\n",
    "    w = module.weight.data\n",
    "    out_features, in_features = w.shape\n",
    "    w_abs = torch.abs(w)\n",
    "    norm_factor = w_abs.sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "    w_norm = w_abs / norm_factor\n",
    "    w_norm_sum_sq = (w_norm**2).sum(dim=1)\n",
    "    neff = torch.clamp(torch.floor(1.0 / w_norm_sum_sq), min=1).long()\n",
    "    k_max = neff.max().item()\n",
    "    \n",
    "    # Efficient top-k using torch.topk\n",
    "    topk_vals, _ = torch.topk(w_norm, k=k_max, dim=1, sorted=True)\n",
    "    thresholds = topk_vals[torch.arange(out_features), neff-1].unsqueeze(1)\n",
    "    mask = (w_norm >= thresholds).float()\n",
    "    return mask\n",
    "\n",
    "def prune_model_neff(model, renormalize=True):  # Changed to True for better stability\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_neff_mask_linear(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight.mul_(mask)\n",
    "                if renormalize:\n",
    "                    # L1 renormalization\n",
    "                    row_sum = module.weight.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n",
    "\n",
    "# ---- 2. Storage improvements ----\n",
    "def get_model_size(model, tmp_file=\"tmp_model.bin\"):\n",
    "    torch.save(model.state_dict(), tmp_file)\n",
    "    size_mb = os.path.getsize(tmp_file) / (1024 * 1024)\n",
    "    os.remove(tmp_file)\n",
    "    return size_mb\n",
    "\n",
    "def count_nonzero_params(model):\n",
    "    nonzero = 0\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "        nonzero += (p != 0).sum().item()\n",
    "    return total, nonzero\n",
    "\n",
    "def get_folder_size_mb(folder):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)\n",
    "\n",
    "# ---- 3. Fixed sparse export/load ----\n",
    "def export_model_sparse(model, out_dir=\"sparse_export\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    meta = {}\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    # Save all non-linear parameters normally\n",
    "    non_linear_state_dict = {}\n",
    "    linear_keys = set()\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_keys.add(f\"{module_name}.weight\")\n",
    "            if module.bias is not None:\n",
    "                linear_keys.add(f\"{module_name}.bias\")\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if k not in linear_keys:\n",
    "            non_linear_state_dict[k] = v.cpu()\n",
    "    \n",
    "    torch.save(non_linear_state_dict, os.path.join(out_dir, \"non_linear_state_dict.pt\"))\n",
    "    \n",
    "    # Save linear layers in sparse format\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            w = module.weight.data.cpu().numpy()\n",
    "            nonzero = w != 0\n",
    "            values = w[nonzero]\n",
    "            \n",
    "            # Efficient sparse storage: CSR format\n",
    "            sparse_format = {\n",
    "                \"data\": values,\n",
    "                \"indices\": np.where(nonzero)[1],  # column indices\n",
    "                \"indptr\": np.concatenate([[0], np.cumsum(nonzero.sum(axis=1))])  # row pointers\n",
    "            }\n",
    "            \n",
    "            np.savez_compressed(\n",
    "                os.path.join(out_dir, f\"{name}.npz\"),\n",
    "                **sparse_format\n",
    "            )\n",
    "            meta[name] = {\n",
    "                \"shape\": w.shape,\n",
    "                \"dtype\": str(w.dtype),\n",
    "                \"nnz\": len(values)\n",
    "            }\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                np.save(\n",
    "                    os.path.join(out_dir, f\"{name}_bias.npy\"),\n",
    "                    module.bias.data.cpu().numpy()\n",
    "                )\n",
    "    \n",
    "    with open(os.path.join(out_dir, \"meta.json\"), \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "    \n",
    "    print(f\"Exported sparse weights to {out_dir}\")\n",
    "    return get_folder_size_mb(out_dir)\n",
    "\n",
    "def load_model_sparse(model, sparse_dir=\"sparse_export\"):\n",
    "    # Load non-linear parameters\n",
    "    non_linear_path = os.path.join(sparse_dir, \"non_linear_state_dict.pt\")\n",
    "    if os.path.exists(non_linear_path):\n",
    "        non_linear_state_dict = torch.load(non_linear_path)\n",
    "        model.load_state_dict(non_linear_state_dict, strict=False)\n",
    "    \n",
    "    # Load meta data\n",
    "    with open(os.path.join(sparse_dir, \"meta.json\")) as f:\n",
    "        meta = json.load(f)\n",
    "    \n",
    "    # Load sparse linear layers\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and name in meta:\n",
    "            sparse_path = os.path.join(sparse_dir, f\"{name}.npz\")\n",
    "            if not os.path.exists(sparse_path):\n",
    "                continue\n",
    "                \n",
    "            sparse_data = np.load(sparse_path)\n",
    "            shape = tuple(meta[name][\"shape\"])\n",
    "            w = np.zeros(shape, dtype=np.float32)\n",
    "            \n",
    "            # Reconstruct from CSR format\n",
    "            rows = []\n",
    "            for i in range(len(sparse_data[\"indptr\"]) - 1):\n",
    "                start = sparse_data[\"indptr\"][i]\n",
    "                end = sparse_data[\"indptr\"][i+1]\n",
    "                cols = sparse_data[\"indices\"][start:end]\n",
    "                w[i, cols] = sparse_data[\"data\"][start:end]\n",
    "            \n",
    "            module.weight.data = torch.tensor(w, device=module.weight.data.device)\n",
    "            \n",
    "            bias_path = os.path.join(sparse_dir, f\"{name}_bias.npy\")\n",
    "            if os.path.exists(bias_path):\n",
    "                module.bias.data = torch.tensor(\n",
    "                    np.load(bias_path),\n",
    "                    device=module.bias.data.device\n",
    "                )\n",
    "    \n",
    "    print(f\"Loaded sparse weights from {sparse_dir}\")\n",
    "    return model\n",
    "\n",
    "# ---- 4. Accuracy function ----\n",
    "def compute_accuracy(model, tokenized_dataset, batch_size=32):\n",
    "    from torch.utils.data import DataLoader\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total\n",
    "\n",
    "# ---- 5. Tokenization and data setup ----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "raw_dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "test_dataset = raw_dataset[\"test\"]\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ---- 6. Train original model ----\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,  # Reduced for faster testing\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Training Original Model ===\")\n",
    "trainer.train()\n",
    "\n",
    "# ---- 7. Evaluate and analyze original ----\n",
    "orig_acc = compute_accuracy(model, tokenized_test_dataset)\n",
    "orig_size = get_model_size(model)\n",
    "orig_total, orig_nonzero = count_nonzero_params(model)\n",
    "print(f\"\\n original model: Size={orig_size:.2f} MB, Acc={orig_acc:.4f}, Nonzeros={orig_nonzero}/{orig_total}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"dense_model.pt\")\n",
    "\n",
    "# ---- 8. Prune, save, and analyze pruned model ----\n",
    "pruned_model = prune_model_neff(model, renormalize=True).to(device)\n",
    "pruned_acc = compute_accuracy(pruned_model, tokenized_test_dataset)\n",
    "pruned_size = get_model_size(pruned_model)\n",
    "pruned_total, pruned_nonzero = count_nonzero_params(pruned_model)\n",
    "print(f\"Pruned model:             Size={pruned_size:.2f} MB, Acc={pruned_acc:.4f}, Nonzeros={pruned_nonzero}/{pruned_total}\")\n",
    "\n",
    "torch.save(pruned_model.state_dict(), \"dense_pruned_model.pt\")\n",
    "sparse_disk_size = export_model_sparse(pruned_model, out_dir=\"sparse_export\")\n",
    "print(f\"Sparse export folder size: {sparse_disk_size:.2f} MB\")\n",
    "\n",
    "# ---- 9. Load sparse weights correctly ----\n",
    "reloaded_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "reloaded_model = load_model_sparse(reloaded_model, sparse_dir=\"sparse_export\")\n",
    "reload_acc = compute_accuracy(reloaded_model, tokenized_test_dataset)\n",
    "reload_total, reload_nonzero = count_nonzero_params(reloaded_model)\n",
    "print(f\"Reloaded sparse model:    Acc={reload_acc:.4f}, Nonzeros={reload_nonzero}/{reload_total}\")\n",
    "\n",
    "# ---- 10. Fine-tuning ----\n",
    "finetune_args_pruned = TrainingArguments(\n",
    "    output_dir=\"./tmp_pruned_bert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,  # Lower learning rate for fine-tuning\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "trainer_pruned = Trainer(\n",
    "    model=reloaded_model,\n",
    "    args=finetune_args_pruned,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Fine-tuning Pruned Model ===\")\n",
    "trainer_pruned.train()\n",
    "finetuned_acc = compute_accuracy(reloaded_model, tokenized_test_dataset)\n",
    "finetuned_size = get_model_size(reloaded_model)\n",
    "finetuned_total, finetuned_nonzero = count_nonzero_params(reloaded_model)\n",
    "print(f\"Fine-tuned pruned model:  Size={finetuned_size:.2f} MB, Acc={finetuned_acc:.4f}, Nonzeros={finetuned_nonzero}/{finetuned_total}\")\n",
    "\n",
    "# ---- 11. Final summary ----\n",
    "dense_size = os.path.getsize(\"dense_pruned_model.pt\") / (1024*1024)\n",
    "print(f\"Dense .pt pruned model size: {dense_size:.2f} MB\")\n",
    "print(f\"Sparse export folder size:   {sparse_disk_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Original   : {orig_size:.2f} MB, Acc={orig_acc:.4f}, Nonzeros={orig_nonzero}\")\n",
    "print(f\"Pruned     : {pruned_size:.2f} MB, Acc={pruned_acc:.4f}, Nonzeros={pruned_nonzero}\")\n",
    "print(f\"Finetuned  : {finetuned_size:.2f} MB, Acc={finetuned_acc:.4f}, Nonzeros={finetuned_nonzero}\")\n",
    "print(f\"Sparse export folder size:   {sparse_disk_size:.2f} MB\")\n",
    "print(f\"Dense .pt pruned model size: {dense_size:.2f} MB\")\n",
    "print(f\"Reloaded   : Acc={reload_acc:.4f}, Nonzeros={reload_nonzero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44570d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
