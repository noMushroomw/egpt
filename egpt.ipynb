{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbcbfd4",
   "metadata": {},
   "source": [
    "## start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923d9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0666b",
   "metadata": {},
   "source": [
    "# 1. get linear mask for effective weight with each weight size [output_size, input_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a772fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cfecd",
   "metadata": {},
   "source": [
    "# 2. set the edge with ineffective weight = 0 and prune the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c3e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n",
    "\n",
    "def model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449c769",
   "metadata": {},
   "source": [
    "# 3. train a linear model first and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e6e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size= [512, 512, 512]):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))        \n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371eb00",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba7578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "\n",
    "# MINIST-10 dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearModel(input_size=28*28, output_size=10, hidden_size=[1024, 512, 512]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "result = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce11e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1258/10000 (12.58%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.292992\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.630450\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.217274\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.250590\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.150028\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.100614\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.147552\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.104935\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.254316\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.166829\n",
      "\n",
      "Test set: Average loss: 0.0909, Accuracy: 9708/10000 (97.08%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.016918\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.015138\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.064082\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.087206\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.135824\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.188597\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.189097\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.115724\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.130663\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.058731\n",
      "\n",
      "Test set: Average loss: 0.1003, Accuracy: 9694/10000 (96.94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.092183\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.065956\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.023316\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.015216\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.041428\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.031411\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.070660\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.078625\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.042869\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.036287\n",
      "\n",
      "Test set: Average loss: 0.0657, Accuracy: 9795/10000 (97.95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019552\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.065409\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.048916\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.019781\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.041435\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000507\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.052102\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.006302\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.037098\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.044692\n",
      "\n",
      "Test set: Average loss: 0.0670, Accuracy: 9788/10000 (97.88%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.027352\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000852\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.025380\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.043197\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.018258\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.026026\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.010471\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.034256\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.010224\n",
      "\n",
      "Test set: Average loss: 0.0758, Accuracy: 9763/10000 (97.63%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.007332\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.001790\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.029685\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.009742\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.007200\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.014140\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.011165\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.069567\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.006745\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002149\n",
      "\n",
      "Test set: Average loss: 0.0820, Accuracy: 9778/10000 (97.78%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.025048\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.018639\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.069748\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001779\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.051181\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.013855\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.037232\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.002452\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.037424\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.004169\n",
      "\n",
      "Test set: Average loss: 0.0694, Accuracy: 9823/10000 (98.23%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.008480\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.006239\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.000379\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000670\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013065\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.036554\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.039588\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.028818\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.078682\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.078102\n",
      "\n",
      "Test set: Average loss: 0.0662, Accuracy: 9833/10000 (98.33%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.004003\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.002896\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.015793\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000603\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.009559\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001870\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001028\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.031454\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.010000\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.013595\n",
      "\n",
      "Test set: Average loss: 0.0877, Accuracy: 9794/10000 (97.94%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.005528\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.000267\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000976\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.074454\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001542\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.005399\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008455\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.074803\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.023799\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.023137\n",
      "\n",
      "Test set: Average loss: 0.0735, Accuracy: 9815/10000 (98.15%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial test\n",
    "test_loss, test_accuracy = test(model, device, test_loader)\n",
    "result['test_loss'].append(test_loss)\n",
    "result['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    result['train_loss'].append(train_loss)\n",
    "    result['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    # Test after each epoch\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    result['test_loss'].append(test_loss)\n",
    "    result['test_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'linear_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bef4b4",
   "metadata": {},
   "source": [
    "# prune the model and comparing the performance with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779e3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "pruned_model_renormalized.to(device)\n",
    "\n",
    "# Test the pruned model\n",
    "test_loss, test_accuracy = test(pruned_model_renormalized, device, test_loader)\n",
    "print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model\n",
    "torch.save(pruned_model_renormalized.state_dict(), 'pruned_linear_model_renormalized.pth')\n",
    "\n",
    "\n",
    "prune_model = prune_model_neff(model, renormalize=False)\n",
    "prune_model.to(device)\n",
    "# Test the pruned model without renormalization\n",
    "test_loss, test_accuracy = test(prune_model, device, test_loader)\n",
    "print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model without renormalization\n",
    "torch.save(prune_model.state_dict(), 'pruned_linear_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5985428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "Average Pruned Model - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "Average Pruned Model with Renormalization - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "# test 10 times and show the average performance\n",
    "test_loss_acc = {'prune_loss': [], 'prune_accuracy': [], 'prune_renorm_loss': [], 'prune_renorm_accuracy': []}\n",
    "\n",
    "for i in range(10):\n",
    "    pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "    pruned_model_renormalized.to(device)\n",
    "\n",
    "    # Test the pruned model\n",
    "    test_loss, test_accuracy = test(pruned_model_renormalized, device, test_loader)\n",
    "    print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    test_loss_acc['prune_renorm_loss'].append(test_loss)\n",
    "    test_loss_acc['prune_renorm_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    pruned_model = prune_model_neff(model, renormalize=False)\n",
    "    pruned_model.to(device)\n",
    "    # Test the pruned model without renormalization\n",
    "    test_loss, test_accuracy = test(pruned_model, device, test_loader)\n",
    "    print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    test_loss_acc['prune_loss'].append(test_loss)\n",
    "    test_loss_acc['prune_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# average performance\n",
    "avg_prune_loss = np.mean(test_loss_acc['prune_loss'])\n",
    "avg_prune_accuracy = np.mean(test_loss_acc['prune_accuracy'])\n",
    "avg_prune_renorm_loss = np.mean(test_loss_acc['prune_renorm_loss'])\n",
    "avg_prune_renorm_accuracy = np.mean(test_loss_acc['prune_renorm_accuracy'])\n",
    "\n",
    "print(f'Average Pruned Model - Test Loss: {avg_prune_loss:.4f}, Test Accuracy: {avg_prune_accuracy:.2f}%')\n",
    "print(f'Average Pruned Model with Renormalization - Test Loss: {avg_prune_renorm_loss:.4f}, Test Accuracy: {avg_prune_renorm_accuracy:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322e209",
   "metadata": {},
   "source": [
    "## measure the sparsity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd687cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the pruned model with renormalization: 0.3418\n",
      "Sparsity of the pruned model without renormalization: 0.3418\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sparsity = model_sparsity(pruned_model_renormalized)\n",
    "print(f'Sparsity of the pruned model with renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(prune_model)\n",
    "print(f'Sparsity of the pruned model without renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bacf6",
   "metadata": {},
   "source": [
    "## one-shot fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54eba9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model - Test Loss: 0.0667, Test Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "pruned_model_one_shot = LinearModel(input_size=28*28, output_size=10, hidden_size=[1024, 512, 512]).to(device)\n",
    "pruned_model_one_shot.load_state_dict(torch.load('pruned_linear_model.pth'))\n",
    "\n",
    "# Test the pruned model\n",
    "test_loss, test_accuracy = test(pruned_model_one_shot, device, test_loader)\n",
    "print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283cd217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.011330\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.034839\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.005182\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.004371\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.011549\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.011232\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.024064\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.002956\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.005015\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.001072\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Model Sparsity: 34.18%\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = train(pruned_model_one_shot, device, train_loader, optimizer, 1)\n",
    "result['train_loss'].append(train_loss)\n",
    "result['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    # Test after each epoch\n",
    "test_loss, test_accuracy = test(pruned_model_one_shot, device, test_loader)\n",
    "result['test_loss'].append(test_loss)\n",
    "result['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "sparsity = model_sparsity(pruned_model_one_shot)\n",
    "print(f'Model Sparsity: {sparsity:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5bea2",
   "metadata": {},
   "source": [
    "# test in huggingface language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860d3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, tokenized_dataset, batch_size=32):\n",
    "    from torch.utils.data import DataLoader\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35295451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_19303/2390932059.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "raw_dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "test_dataset = raw_dataset[\"test\"]\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize and format\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# 4. Load and fine-tune BERT on train split\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196726d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Original Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37500/37500 32:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned original model: Sparsity=0.0000, Acc=0.9428\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Training Original Model ===\")\n",
    "trainer.train()\n",
    "\n",
    "orig_acc = compute_accuracy(model, tokenized_test_dataset)\n",
    "sparsity = model_sparsity(model)\n",
    "print(f\"\\nFine-tuned original model: Sparsity={sparsity:.4f}, Acc={orig_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'bert_origin.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c621ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned model: Sparsity=0.2923, Acc=0.9447\n",
      "Pruned model with renormalization: Sparsity=0.2923, Acc=0.2500\n"
     ]
    }
   ],
   "source": [
    "pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "pruned_model_renormalized.to(device)\n",
    "pruned_model = prune_model_neff(model, renormalize=False)\n",
    "pruned_model.to(device)\n",
    "\n",
    "prune_acc = compute_accuracy(pruned_model, tokenized_test_dataset)\n",
    "prune_renorm_acc = compute_accuracy(pruned_model_renormalized, tokenized_test_dataset)\n",
    "pruned_sparsity = model_sparsity(pruned_model)\n",
    "pruned_renorm_sparsity = model_sparsity(pruned_model_renormalized)\n",
    "\n",
    "print(f\"\\nPruned model: Sparsity={pruned_sparsity:.4f}, Acc={prune_acc:.4f}\")\n",
    "print(f\"Pruned model with renormalization: Sparsity={pruned_renorm_sparsity:.4f}, Acc={prune_renorm_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfadfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model.state_dict(), 'bert_pruned.pth')\n",
    "torch.save(pruned_model_renormalized.state_dict(), 'bert_pruned_renorm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2fd80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model - Test Accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "pruned_model_one_shot = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "pruned_model_one_shot.load_state_dict(torch.load('bert_pruned.pth'))\n",
    "\n",
    "# Test the pruned model\n",
    "inital_acc = compute_accuracy(pruned_model_one_shot, tokenized_test_dataset)\n",
    "print(f'Pruned Model - Test Accuracy: {inital_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa9cc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19303/61186683.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Pruned Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 06:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model after fine-tuning: Acc=0.9421\n",
      "Sparsity of the pruned model after fine-tuning: 0.0000\n"
     ]
    }
   ],
   "source": [
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert_pruned\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pruned_model_one_shot,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Training Pruned Model ===\")\n",
    "trainer.train()\n",
    "pruned_acc = compute_accuracy(pruned_model_one_shot, tokenized_test_dataset)\n",
    "print(f\"Pruned model after fine-tuning: Acc={pruned_acc:.4f}\")\n",
    "\n",
    "sparsity = model_sparsity(pruned_model_one_shot)\n",
    "print(f'Sparsity of the pruned model after fine-tuning: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f57e2",
   "metadata": {},
   "source": [
    "## test for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8814919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fd01f2952f46d6a5f1df0264b36d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen-7B\"  # Use \"Qwen/Qwen-7B\" for smaller variant\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "DEVICE_MAP = \"auto\"  # Automatically distributes across GPUs\n",
    "BATCH_SIZE = 1  # Reduce if OOM errors occur\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(DATASET_NAME, DATASET_CONFIG, split=\"test\")\n",
    "texts = [text for text in test_dataset[\"text\"] if text.strip()]  # Remove empty strings\n",
    "\n",
    "# Load model with quantization (4-bit) to reduce VRAM usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=DEVICE_MAP,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config={\"load_in_4bit\": True},\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ccce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d310e750934922a9d8432a1e7f304c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity:   0%|          | 0/2891 [00:00<?, ?it/s]/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Calculating Perplexity: 100%|██████████| 2891/2891 [04:38<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 17.46\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity\n",
    "model.eval()\n",
    "total_log_likelihood = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(texts, desc=\"Calculating Perplexity\"):\n",
    "        # Tokenize text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        \n",
    "        # Forward pass to get loss\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "\n",
    "# Final perplexity calculation\n",
    "perplexity = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), 'qwen_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c12ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_llm(model, renormalize=False):\n",
    "    \"\"\"\n",
    "    Prune LLM model by targeting only standard Linear layers\n",
    "    Avoids quantized layers and special layer types\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(model)\n",
    "    pruned_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Only prune standard nn.Linear layers, avoid quantized layers\n",
    "        if isinstance(module, nn.Linear) and not hasattr(module, 'quant_state'):\n",
    "            try:\n",
    "                mask = get_linear_mask(module).to(module.weight.device)\n",
    "                with torch.no_grad():\n",
    "                    module.weight *= mask.float()\n",
    "                    \n",
    "                    if renormalize:\n",
    "                        # More stable renormalization\n",
    "                        row_sum = module.weight.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                        module.weight.div_(row_sum)\n",
    "                    \n",
    "                    pruned_layers.append(name)\n",
    "                    \n",
    "                    # Check sparsity of this layer\n",
    "                    sparsity = (module.weight == 0).float().mean().item()\n",
    "                    print(f\"Pruned {name}: {sparsity:.2%} sparsity\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Successfully pruned {len(pruned_layers)} layers\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c53f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping lm_head: CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacity of 31.36 GiB of which 4.09 GiB is free. Including non-PyTorch memory, this process has 26.46 GiB memory in use. Of the allocated memory 25.79 GiB is allocated by PyTorch, and 89.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Successfully pruned 0 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity for Pruned Model:   0%|          | 0/2891 [00:00<?, ?it/s]/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Calculating Perplexity for Pruned Model: 100%|██████████| 2891/2891 [04:32<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Pruned Model: 17.46\n",
      "Sparsity of the pruned model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "pruned_Qwen = prune_model_neff_llm(model, renormalize=False)\n",
    "pruned_Qwen.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# release the model from GPU memory\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Calculate perplexity\n",
    "pruned_Qwen.eval()\n",
    "total_log_likelihood = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(texts, desc=\"Calculating Perplexity for Pruned Model\"):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(pruned_Qwen.device)\n",
    "        outputs = pruned_Qwen(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "        \n",
    "perplexity_pruned = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity of Pruned Model: {perplexity_pruned:.2f}\")\n",
    "\n",
    "sparsity = model_sparsity(pruned_Qwen)\n",
    "print(f'Sparsity of the pruned model: {sparsity:.4f}')\n",
    "\n",
    "torch.save(pruned_Qwen.state_dict(), 'pruned_qwen_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee734096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c99cb41082406881315b1c9358ebfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.2.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.2.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.2.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.3.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.3.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.3.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.4.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.4.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.4.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.5.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.5.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.5.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.6.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.6.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.6.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.7.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.7.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.7.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.8.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.8.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.8.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.9.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.9.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.9.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.10.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.10.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.10.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.11.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.11.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.11.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.12.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.12.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.12.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.13.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.13.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.13.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.14.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.14.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.14.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.15.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.15.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.15.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.16.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.16.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.16.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.17.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.17.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.17.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.18.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.18.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.18.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.19.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.19.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.19.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.20.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.20.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.20.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.21.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.21.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.21.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.22.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.22.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.22.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.23.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.23.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.23.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.24.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.24.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.24.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.25.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.25.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.25.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.26.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.26.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.26.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.27.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.27.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.27.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.28.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.28.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.28.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.29.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.29.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.29.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.30.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.30.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.30.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.31.ln_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.31.attn.c_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.h.31.ln_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for transformer.ln_f.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/nomushroom/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2409: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for QWenLMHeadModel:\n\tUnexpected key(s) in state_dict: \"transformer.h.0.attn.c_attn.weight.absmax\", \"transformer.h.0.attn.c_attn.weight.quant_map\", \"transformer.h.0.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.attn.c_proj.weight.absmax\", \"transformer.h.0.attn.c_proj.weight.quant_map\", \"transformer.h.0.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.mlp.w1.weight.absmax\", \"transformer.h.0.mlp.w1.weight.quant_map\", \"transformer.h.0.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.mlp.w2.weight.absmax\", \"transformer.h.0.mlp.w2.weight.quant_map\", \"transformer.h.0.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.mlp.c_proj.weight.absmax\", \"transformer.h.0.mlp.c_proj.weight.quant_map\", \"transformer.h.0.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.attn.c_attn.weight.absmax\", \"transformer.h.1.attn.c_attn.weight.quant_map\", \"transformer.h.1.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.attn.c_proj.weight.absmax\", \"transformer.h.1.attn.c_proj.weight.quant_map\", \"transformer.h.1.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.mlp.w1.weight.absmax\", \"transformer.h.1.mlp.w1.weight.quant_map\", \"transformer.h.1.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.mlp.w2.weight.absmax\", \"transformer.h.1.mlp.w2.weight.quant_map\", \"transformer.h.1.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.mlp.c_proj.weight.absmax\", \"transformer.h.1.mlp.c_proj.weight.quant_map\", \"transformer.h.1.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.attn.c_attn.weight.absmax\", \"transformer.h.2.attn.c_attn.weight.quant_map\", \"transformer.h.2.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.attn.c_proj.weight.absmax\", \"transformer.h.2.attn.c_proj.weight.quant_map\", \"transformer.h.2.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.mlp.w1.weight.absmax\", \"transformer.h.2.mlp.w1.weight.quant_map\", \"transformer.h.2.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.mlp.w2.weight.absmax\", \"transformer.h.2.mlp.w2.weight.quant_map\", \"transformer.h.2.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.mlp.c_proj.weight.absmax\", \"transformer.h.2.mlp.c_proj.weight.quant_map\", \"transformer.h.2.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.attn.c_attn.weight.absmax\", \"transformer.h.3.attn.c_attn.weight.quant_map\", \"transformer.h.3.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.attn.c_proj.weight.absmax\", \"transformer.h.3.attn.c_proj.weight.quant_map\", \"transformer.h.3.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.mlp.w1.weight.absmax\", \"transformer.h.3.mlp.w1.weight.quant_map\", \"transformer.h.3.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.mlp.w2.weight.absmax\", \"transformer.h.3.mlp.w2.weight.quant_map\", \"transformer.h.3.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.mlp.c_proj.weight.absmax\", \"transformer.h.3.mlp.c_proj.weight.quant_map\", \"transformer.h.3.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.attn.c_attn.weight.absmax\", \"transformer.h.4.attn.c_attn.weight.quant_map\", \"transformer.h.4.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.attn.c_proj.weight.absmax\", \"transformer.h.4.attn.c_proj.weight.quant_map\", \"transformer.h.4.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.mlp.w1.weight.absmax\", \"transformer.h.4.mlp.w1.weight.quant_map\", \"transformer.h.4.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.mlp.w2.weight.absmax\", \"transformer.h.4.mlp.w2.weight.quant_map\", \"transformer.h.4.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.mlp.c_proj.weight.absmax\", \"transformer.h.4.mlp.c_proj.weight.quant_map\", \"transformer.h.4.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.attn.c_attn.weight.absmax\", \"transformer.h.5.attn.c_attn.weight.quant_map\", \"transformer.h.5.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.attn.c_proj.weight.absmax\", \"transformer.h.5.attn.c_proj.weight.quant_map\", \"transformer.h.5.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.mlp.w1.weight.absmax\", \"transformer.h.5.mlp.w1.weight.quant_map\", \"transformer.h.5.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.mlp.w2.weight.absmax\", \"transformer.h.5.mlp.w2.weight.quant_map\", \"transformer.h.5.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.mlp.c_proj.weight.absmax\", \"transformer.h.5.mlp.c_proj.weight.quant_map\", \"transformer.h.5.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.attn.c_attn.weight.absmax\", \"transformer.h.6.attn.c_attn.weight.quant_map\", \"transformer.h.6.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.attn.c_proj.weight.absmax\", \"transformer.h.6.attn.c_proj.weight.quant_map\", \"transformer.h.6.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.mlp.w1.weight.absmax\", \"transformer.h.6.mlp.w1.weight.quant_map\", \"transformer.h.6.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.mlp.w2.weight.absmax\", \"transformer.h.6.mlp.w2.weight.quant_map\", \"transformer.h.6.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.mlp.c_proj.weight.absmax\", \"transformer.h.6.mlp.c_proj.weight.quant_map\", \"transformer.h.6.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.attn.c_attn.weight.absmax\", \"transformer.h.7.attn.c_attn.weight.quant_map\", \"transformer.h.7.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.attn.c_proj.weight.absmax\", \"transformer.h.7.attn.c_proj.weight.quant_map\", \"transformer.h.7.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.mlp.w1.weight.absmax\", \"transformer.h.7.mlp.w1.weight.quant_map\", \"transformer.h.7.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.mlp.w2.weight.absmax\", \"transformer.h.7.mlp.w2.weight.quant_map\", \"transformer.h.7.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.mlp.c_proj.weight.absmax\", \"transformer.h.7.mlp.c_proj.weight.quant_map\", \"transformer.h.7.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.attn.c_attn.weight.absmax\", \"transformer.h.8.attn.c_attn.weight.quant_map\", \"transformer.h.8.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.attn.c_proj.weight.absmax\", \"transformer.h.8.attn.c_proj.weight.quant_map\", \"transformer.h.8.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.mlp.w1.weight.absmax\", \"transformer.h.8.mlp.w1.weight.quant_map\", \"transformer.h.8.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.mlp.w2.weight.absmax\", \"transformer.h.8.mlp.w2.weight.quant_map\", \"transformer.h.8.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.mlp.c_proj.weight.absmax\", \"transformer.h.8.mlp.c_proj.weight.quant_map\", \"transformer.h.8.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.attn.c_attn.weight.absmax\", \"transformer.h.9.attn.c_attn.weight.quant_map\", \"transformer.h.9.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.attn.c_proj.weight.absmax\", \"transformer.h.9.attn.c_proj.weight.quant_map\", \"transformer.h.9.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.mlp.w1.weight.absmax\", \"transformer.h.9.mlp.w1.weight.quant_map\", \"transformer.h.9.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.mlp.w2.weight.absmax\", \"transformer.h.9.mlp.w2.weight.quant_map\", \"transformer.h.9.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.mlp.c_proj.weight.absmax\", \"transformer.h.9.mlp.c_proj.weight.quant_map\", \"transformer.h.9.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.attn.c_attn.weight.absmax\", \"transformer.h.10.attn.c_attn.weight.quant_map\", \"transformer.h.10.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.attn.c_proj.weight.absmax\", \"transformer.h.10.attn.c_proj.weight.quant_map\", \"transformer.h.10.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.mlp.w1.weight.absmax\", \"transformer.h.10.mlp.w1.weight.quant_map\", \"transformer.h.10.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.mlp.w2.weight.absmax\", \"transformer.h.10.mlp.w2.weight.quant_map\", \"transformer.h.10.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.mlp.c_proj.weight.absmax\", \"transformer.h.10.mlp.c_proj.weight.quant_map\", \"transformer.h.10.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.attn.c_attn.weight.absmax\", \"transformer.h.11.attn.c_attn.weight.quant_map\", \"transformer.h.11.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.attn.c_proj.weight.absmax\", \"transformer.h.11.attn.c_proj.weight.quant_map\", \"transformer.h.11.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.mlp.w1.weight.absmax\", \"transformer.h.11.mlp.w1.weight.quant_map\", \"transformer.h.11.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.mlp.w2.weight.absmax\", \"transformer.h.11.mlp.w2.weight.quant_map\", \"transformer.h.11.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.mlp.c_proj.weight.absmax\", \"transformer.h.11.mlp.c_proj.weight.quant_map\", \"transformer.h.11.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.attn.c_attn.weight.absmax\", \"transformer.h.12.attn.c_attn.weight.quant_map\", \"transformer.h.12.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.attn.c_proj.weight.absmax\", \"transformer.h.12.attn.c_proj.weight.quant_map\", \"transformer.h.12.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.mlp.w1.weight.absmax\", \"transformer.h.12.mlp.w1.weight.quant_map\", \"transformer.h.12.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.mlp.w2.weight.absmax\", \"transformer.h.12.mlp.w2.weight.quant_map\", \"transformer.h.12.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.mlp.c_proj.weight.absmax\", \"transformer.h.12.mlp.c_proj.weight.quant_map\", \"transformer.h.12.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.attn.c_attn.weight.absmax\", \"transformer.h.13.attn.c_attn.weight.quant_map\", \"transformer.h.13.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.attn.c_proj.weight.absmax\", \"transformer.h.13.attn.c_proj.weight.quant_map\", \"transformer.h.13.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.mlp.w1.weight.absmax\", \"transformer.h.13.mlp.w1.weight.quant_map\", \"transformer.h.13.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.mlp.w2.weight.absmax\", \"transformer.h.13.mlp.w2.weight.quant_map\", \"transformer.h.13.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.mlp.c_proj.weight.absmax\", \"transformer.h.13.mlp.c_proj.weight.quant_map\", \"transformer.h.13.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.attn.c_attn.weight.absmax\", \"transformer.h.14.attn.c_attn.weight.quant_map\", \"transformer.h.14.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.attn.c_proj.weight.absmax\", \"transformer.h.14.attn.c_proj.weight.quant_map\", \"transformer.h.14.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.mlp.w1.weight.absmax\", \"transformer.h.14.mlp.w1.weight.quant_map\", \"transformer.h.14.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.mlp.w2.weight.absmax\", \"transformer.h.14.mlp.w2.weight.quant_map\", \"transformer.h.14.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.mlp.c_proj.weight.absmax\", \"transformer.h.14.mlp.c_proj.weight.quant_map\", \"transformer.h.14.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.attn.c_attn.weight.absmax\", \"transformer.h.15.attn.c_attn.weight.quant_map\", \"transformer.h.15.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.attn.c_proj.weight.absmax\", \"transformer.h.15.attn.c_proj.weight.quant_map\", \"transformer.h.15.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.mlp.w1.weight.absmax\", \"transformer.h.15.mlp.w1.weight.quant_map\", \"transformer.h.15.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.mlp.w2.weight.absmax\", \"transformer.h.15.mlp.w2.weight.quant_map\", \"transformer.h.15.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.mlp.c_proj.weight.absmax\", \"transformer.h.15.mlp.c_proj.weight.quant_map\", \"transformer.h.15.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.attn.c_attn.weight.absmax\", \"transformer.h.16.attn.c_attn.weight.quant_map\", \"transformer.h.16.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.attn.c_proj.weight.absmax\", \"transformer.h.16.attn.c_proj.weight.quant_map\", \"transformer.h.16.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.mlp.w1.weight.absmax\", \"transformer.h.16.mlp.w1.weight.quant_map\", \"transformer.h.16.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.mlp.w2.weight.absmax\", \"transformer.h.16.mlp.w2.weight.quant_map\", \"transformer.h.16.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.mlp.c_proj.weight.absmax\", \"transformer.h.16.mlp.c_proj.weight.quant_map\", \"transformer.h.16.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.attn.c_attn.weight.absmax\", \"transformer.h.17.attn.c_attn.weight.quant_map\", \"transformer.h.17.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.attn.c_proj.weight.absmax\", \"transformer.h.17.attn.c_proj.weight.quant_map\", \"transformer.h.17.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.mlp.w1.weight.absmax\", \"transformer.h.17.mlp.w1.weight.quant_map\", \"transformer.h.17.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.mlp.w2.weight.absmax\", \"transformer.h.17.mlp.w2.weight.quant_map\", \"transformer.h.17.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.mlp.c_proj.weight.absmax\", \"transformer.h.17.mlp.c_proj.weight.quant_map\", \"transformer.h.17.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.attn.c_attn.weight.absmax\", \"transformer.h.18.attn.c_attn.weight.quant_map\", \"transformer.h.18.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.attn.c_proj.weight.absmax\", \"transformer.h.18.attn.c_proj.weight.quant_map\", \"transformer.h.18.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.mlp.w1.weight.absmax\", \"transformer.h.18.mlp.w1.weight.quant_map\", \"transformer.h.18.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.mlp.w2.weight.absmax\", \"transformer.h.18.mlp.w2.weight.quant_map\", \"transformer.h.18.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.mlp.c_proj.weight.absmax\", \"transformer.h.18.mlp.c_proj.weight.quant_map\", \"transformer.h.18.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.attn.c_attn.weight.absmax\", \"transformer.h.19.attn.c_attn.weight.quant_map\", \"transformer.h.19.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.attn.c_proj.weight.absmax\", \"transformer.h.19.attn.c_proj.weight.quant_map\", \"transformer.h.19.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.mlp.w1.weight.absmax\", \"transformer.h.19.mlp.w1.weight.quant_map\", \"transformer.h.19.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.mlp.w2.weight.absmax\", \"transformer.h.19.mlp.w2.weight.quant_map\", \"transformer.h.19.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.mlp.c_proj.weight.absmax\", \"transformer.h.19.mlp.c_proj.weight.quant_map\", \"transformer.h.19.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.attn.c_attn.weight.absmax\", \"transformer.h.20.attn.c_attn.weight.quant_map\", \"transformer.h.20.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.attn.c_proj.weight.absmax\", \"transformer.h.20.attn.c_proj.weight.quant_map\", \"transformer.h.20.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.mlp.w1.weight.absmax\", \"transformer.h.20.mlp.w1.weight.quant_map\", \"transformer.h.20.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.mlp.w2.weight.absmax\", \"transformer.h.20.mlp.w2.weight.quant_map\", \"transformer.h.20.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.mlp.c_proj.weight.absmax\", \"transformer.h.20.mlp.c_proj.weight.quant_map\", \"transformer.h.20.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.attn.c_attn.weight.absmax\", \"transformer.h.21.attn.c_attn.weight.quant_map\", \"transformer.h.21.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.attn.c_proj.weight.absmax\", \"transformer.h.21.attn.c_proj.weight.quant_map\", \"transformer.h.21.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.mlp.w1.weight.absmax\", \"transformer.h.21.mlp.w1.weight.quant_map\", \"transformer.h.21.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.mlp.w2.weight.absmax\", \"transformer.h.21.mlp.w2.weight.quant_map\", \"transformer.h.21.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.mlp.c_proj.weight.absmax\", \"transformer.h.21.mlp.c_proj.weight.quant_map\", \"transformer.h.21.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.attn.c_attn.weight.absmax\", \"transformer.h.22.attn.c_attn.weight.quant_map\", \"transformer.h.22.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.attn.c_proj.weight.absmax\", \"transformer.h.22.attn.c_proj.weight.quant_map\", \"transformer.h.22.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.mlp.w1.weight.absmax\", \"transformer.h.22.mlp.w1.weight.quant_map\", \"transformer.h.22.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.mlp.w2.weight.absmax\", \"transformer.h.22.mlp.w2.weight.quant_map\", \"transformer.h.22.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.mlp.c_proj.weight.absmax\", \"transformer.h.22.mlp.c_proj.weight.quant_map\", \"transformer.h.22.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.attn.c_attn.weight.absmax\", \"transformer.h.23.attn.c_attn.weight.quant_map\", \"transformer.h.23.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.attn.c_proj.weight.absmax\", \"transformer.h.23.attn.c_proj.weight.quant_map\", \"transformer.h.23.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.mlp.w1.weight.absmax\", \"transformer.h.23.mlp.w1.weight.quant_map\", \"transformer.h.23.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.mlp.w2.weight.absmax\", \"transformer.h.23.mlp.w2.weight.quant_map\", \"transformer.h.23.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.mlp.c_proj.weight.absmax\", \"transformer.h.23.mlp.c_proj.weight.quant_map\", \"transformer.h.23.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.attn.c_attn.weight.absmax\", \"transformer.h.24.attn.c_attn.weight.quant_map\", \"transformer.h.24.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.attn.c_proj.weight.absmax\", \"transformer.h.24.attn.c_proj.weight.quant_map\", \"transformer.h.24.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.mlp.w1.weight.absmax\", \"transformer.h.24.mlp.w1.weight.quant_map\", \"transformer.h.24.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.mlp.w2.weight.absmax\", \"transformer.h.24.mlp.w2.weight.quant_map\", \"transformer.h.24.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.mlp.c_proj.weight.absmax\", \"transformer.h.24.mlp.c_proj.weight.quant_map\", \"transformer.h.24.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.attn.c_attn.weight.absmax\", \"transformer.h.25.attn.c_attn.weight.quant_map\", \"transformer.h.25.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.attn.c_proj.weight.absmax\", \"transformer.h.25.attn.c_proj.weight.quant_map\", \"transformer.h.25.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.mlp.w1.weight.absmax\", \"transformer.h.25.mlp.w1.weight.quant_map\", \"transformer.h.25.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.mlp.w2.weight.absmax\", \"transformer.h.25.mlp.w2.weight.quant_map\", \"transformer.h.25.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.mlp.c_proj.weight.absmax\", \"transformer.h.25.mlp.c_proj.weight.quant_map\", \"transformer.h.25.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.attn.c_attn.weight.absmax\", \"transformer.h.26.attn.c_attn.weight.quant_map\", \"transformer.h.26.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.attn.c_proj.weight.absmax\", \"transformer.h.26.attn.c_proj.weight.quant_map\", \"transformer.h.26.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.mlp.w1.weight.absmax\", \"transformer.h.26.mlp.w1.weight.quant_map\", \"transformer.h.26.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.mlp.w2.weight.absmax\", \"transformer.h.26.mlp.w2.weight.quant_map\", \"transformer.h.26.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.mlp.c_proj.weight.absmax\", \"transformer.h.26.mlp.c_proj.weight.quant_map\", \"transformer.h.26.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.attn.c_attn.weight.absmax\", \"transformer.h.27.attn.c_attn.weight.quant_map\", \"transformer.h.27.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.attn.c_proj.weight.absmax\", \"transformer.h.27.attn.c_proj.weight.quant_map\", \"transformer.h.27.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.mlp.w1.weight.absmax\", \"transformer.h.27.mlp.w1.weight.quant_map\", \"transformer.h.27.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.mlp.w2.weight.absmax\", \"transformer.h.27.mlp.w2.weight.quant_map\", \"transformer.h.27.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.mlp.c_proj.weight.absmax\", \"transformer.h.27.mlp.c_proj.weight.quant_map\", \"transformer.h.27.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.attn.c_attn.weight.absmax\", \"transformer.h.28.attn.c_attn.weight.quant_map\", \"transformer.h.28.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.attn.c_proj.weight.absmax\", \"transformer.h.28.attn.c_proj.weight.quant_map\", \"transformer.h.28.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.mlp.w1.weight.absmax\", \"transformer.h.28.mlp.w1.weight.quant_map\", \"transformer.h.28.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.mlp.w2.weight.absmax\", \"transformer.h.28.mlp.w2.weight.quant_map\", \"transformer.h.28.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.mlp.c_proj.weight.absmax\", \"transformer.h.28.mlp.c_proj.weight.quant_map\", \"transformer.h.28.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.attn.c_attn.weight.absmax\", \"transformer.h.29.attn.c_attn.weight.quant_map\", \"transformer.h.29.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.attn.c_proj.weight.absmax\", \"transformer.h.29.attn.c_proj.weight.quant_map\", \"transformer.h.29.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.mlp.w1.weight.absmax\", \"transformer.h.29.mlp.w1.weight.quant_map\", \"transformer.h.29.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.mlp.w2.weight.absmax\", \"transformer.h.29.mlp.w2.weight.quant_map\", \"transformer.h.29.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.mlp.c_proj.weight.absmax\", \"transformer.h.29.mlp.c_proj.weight.quant_map\", \"transformer.h.29.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.attn.c_attn.weight.absmax\", \"transformer.h.30.attn.c_attn.weight.quant_map\", \"transformer.h.30.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.attn.c_proj.weight.absmax\", \"transformer.h.30.attn.c_proj.weight.quant_map\", \"transformer.h.30.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.mlp.w1.weight.absmax\", \"transformer.h.30.mlp.w1.weight.quant_map\", \"transformer.h.30.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.mlp.w2.weight.absmax\", \"transformer.h.30.mlp.w2.weight.quant_map\", \"transformer.h.30.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.mlp.c_proj.weight.absmax\", \"transformer.h.30.mlp.c_proj.weight.quant_map\", \"transformer.h.30.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.attn.c_attn.weight.absmax\", \"transformer.h.31.attn.c_attn.weight.quant_map\", \"transformer.h.31.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.attn.c_proj.weight.absmax\", \"transformer.h.31.attn.c_proj.weight.quant_map\", \"transformer.h.31.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.mlp.w1.weight.absmax\", \"transformer.h.31.mlp.w1.weight.quant_map\", \"transformer.h.31.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.mlp.w2.weight.absmax\", \"transformer.h.31.mlp.w2.weight.quant_map\", \"transformer.h.31.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.mlp.c_proj.weight.absmax\", \"transformer.h.31.mlp.c_proj.weight.quant_map\", \"transformer.h.31.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\". \n\tsize mismatch for transformer.h.0.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.0.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.0.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.0.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.0.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.1.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.1.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.1.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.1.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.1.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.2.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.2.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.2.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.2.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.2.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.3.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.3.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.3.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.3.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.3.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.4.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.4.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.4.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.4.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.4.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.5.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.5.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.5.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.5.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.5.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.6.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.6.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.6.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.6.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.6.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.7.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.7.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.7.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.7.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.7.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.8.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.8.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.8.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.8.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.8.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.9.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.9.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.9.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.9.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.9.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.10.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.10.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.10.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.10.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.10.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.11.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.11.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.11.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.11.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.11.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.12.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.12.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.12.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.12.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.12.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.13.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.13.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.13.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.13.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.13.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.14.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.14.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.14.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.14.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.14.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.15.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.15.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.15.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.15.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.15.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.16.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.16.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.16.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.16.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.16.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.17.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.17.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.17.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.17.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.17.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.18.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.18.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.18.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.18.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.18.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.19.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.19.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.19.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.19.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.19.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.20.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.20.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.20.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.20.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.20.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.21.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.21.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.21.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.21.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.21.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.22.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.22.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.22.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.22.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.22.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.23.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.23.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.23.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.23.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.23.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.24.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.24.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.24.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.24.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.24.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.25.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.25.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.25.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.25.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.25.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.26.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.26.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.26.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.26.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.26.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.27.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.27.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.27.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.27.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.27.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.28.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.28.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.28.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.28.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.28.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.29.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.29.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.29.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.29.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.29.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.30.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.30.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.30.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.30.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.30.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.31.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.31.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.31.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.31.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.31.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m pruned_model_one_shot \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     MODEL_NAME,\n\u001b[1;32m      3\u001b[0m     device_map\u001b[38;5;241m=\u001b[39mDEVICE_MAP,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mpruned_model_one_shot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpruned_qwen_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Test the pruned model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m perplexity_inital \u001b[38;5;241m=\u001b[39m compute_accuracy(pruned_model_one_shot, tokenized_test_dataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:2593\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2585\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2589\u001b[0m             ),\n\u001b[1;32m   2590\u001b[0m         )\n\u001b[1;32m   2592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2595\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2596\u001b[0m         )\n\u001b[1;32m   2597\u001b[0m     )\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QWenLMHeadModel:\n\tUnexpected key(s) in state_dict: \"transformer.h.0.attn.c_attn.weight.absmax\", \"transformer.h.0.attn.c_attn.weight.quant_map\", \"transformer.h.0.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.attn.c_proj.weight.absmax\", \"transformer.h.0.attn.c_proj.weight.quant_map\", \"transformer.h.0.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.mlp.w1.weight.absmax\", \"transformer.h.0.mlp.w1.weight.quant_map\", \"transformer.h.0.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.mlp.w2.weight.absmax\", \"transformer.h.0.mlp.w2.weight.quant_map\", \"transformer.h.0.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.0.mlp.c_proj.weight.absmax\", \"transformer.h.0.mlp.c_proj.weight.quant_map\", \"transformer.h.0.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.attn.c_attn.weight.absmax\", \"transformer.h.1.attn.c_attn.weight.quant_map\", \"transformer.h.1.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.attn.c_proj.weight.absmax\", \"transformer.h.1.attn.c_proj.weight.quant_map\", \"transformer.h.1.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.mlp.w1.weight.absmax\", \"transformer.h.1.mlp.w1.weight.quant_map\", \"transformer.h.1.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.mlp.w2.weight.absmax\", \"transformer.h.1.mlp.w2.weight.quant_map\", \"transformer.h.1.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.1.mlp.c_proj.weight.absmax\", \"transformer.h.1.mlp.c_proj.weight.quant_map\", \"transformer.h.1.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.attn.c_attn.weight.absmax\", \"transformer.h.2.attn.c_attn.weight.quant_map\", \"transformer.h.2.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.attn.c_proj.weight.absmax\", \"transformer.h.2.attn.c_proj.weight.quant_map\", \"transformer.h.2.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.mlp.w1.weight.absmax\", \"transformer.h.2.mlp.w1.weight.quant_map\", \"transformer.h.2.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.mlp.w2.weight.absmax\", \"transformer.h.2.mlp.w2.weight.quant_map\", \"transformer.h.2.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.2.mlp.c_proj.weight.absmax\", \"transformer.h.2.mlp.c_proj.weight.quant_map\", \"transformer.h.2.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.attn.c_attn.weight.absmax\", \"transformer.h.3.attn.c_attn.weight.quant_map\", \"transformer.h.3.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.attn.c_proj.weight.absmax\", \"transformer.h.3.attn.c_proj.weight.quant_map\", \"transformer.h.3.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.mlp.w1.weight.absmax\", \"transformer.h.3.mlp.w1.weight.quant_map\", \"transformer.h.3.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.mlp.w2.weight.absmax\", \"transformer.h.3.mlp.w2.weight.quant_map\", \"transformer.h.3.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.3.mlp.c_proj.weight.absmax\", \"transformer.h.3.mlp.c_proj.weight.quant_map\", \"transformer.h.3.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.attn.c_attn.weight.absmax\", \"transformer.h.4.attn.c_attn.weight.quant_map\", \"transformer.h.4.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.attn.c_proj.weight.absmax\", \"transformer.h.4.attn.c_proj.weight.quant_map\", \"transformer.h.4.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.mlp.w1.weight.absmax\", \"transformer.h.4.mlp.w1.weight.quant_map\", \"transformer.h.4.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.mlp.w2.weight.absmax\", \"transformer.h.4.mlp.w2.weight.quant_map\", \"transformer.h.4.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.4.mlp.c_proj.weight.absmax\", \"transformer.h.4.mlp.c_proj.weight.quant_map\", \"transformer.h.4.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.attn.c_attn.weight.absmax\", \"transformer.h.5.attn.c_attn.weight.quant_map\", \"transformer.h.5.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.attn.c_proj.weight.absmax\", \"transformer.h.5.attn.c_proj.weight.quant_map\", \"transformer.h.5.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.mlp.w1.weight.absmax\", \"transformer.h.5.mlp.w1.weight.quant_map\", \"transformer.h.5.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.mlp.w2.weight.absmax\", \"transformer.h.5.mlp.w2.weight.quant_map\", \"transformer.h.5.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.5.mlp.c_proj.weight.absmax\", \"transformer.h.5.mlp.c_proj.weight.quant_map\", \"transformer.h.5.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.attn.c_attn.weight.absmax\", \"transformer.h.6.attn.c_attn.weight.quant_map\", \"transformer.h.6.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.attn.c_proj.weight.absmax\", \"transformer.h.6.attn.c_proj.weight.quant_map\", \"transformer.h.6.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.mlp.w1.weight.absmax\", \"transformer.h.6.mlp.w1.weight.quant_map\", \"transformer.h.6.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.mlp.w2.weight.absmax\", \"transformer.h.6.mlp.w2.weight.quant_map\", \"transformer.h.6.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.6.mlp.c_proj.weight.absmax\", \"transformer.h.6.mlp.c_proj.weight.quant_map\", \"transformer.h.6.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.attn.c_attn.weight.absmax\", \"transformer.h.7.attn.c_attn.weight.quant_map\", \"transformer.h.7.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.attn.c_proj.weight.absmax\", \"transformer.h.7.attn.c_proj.weight.quant_map\", \"transformer.h.7.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.mlp.w1.weight.absmax\", \"transformer.h.7.mlp.w1.weight.quant_map\", \"transformer.h.7.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.mlp.w2.weight.absmax\", \"transformer.h.7.mlp.w2.weight.quant_map\", \"transformer.h.7.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.7.mlp.c_proj.weight.absmax\", \"transformer.h.7.mlp.c_proj.weight.quant_map\", \"transformer.h.7.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.attn.c_attn.weight.absmax\", \"transformer.h.8.attn.c_attn.weight.quant_map\", \"transformer.h.8.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.attn.c_proj.weight.absmax\", \"transformer.h.8.attn.c_proj.weight.quant_map\", \"transformer.h.8.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.mlp.w1.weight.absmax\", \"transformer.h.8.mlp.w1.weight.quant_map\", \"transformer.h.8.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.mlp.w2.weight.absmax\", \"transformer.h.8.mlp.w2.weight.quant_map\", \"transformer.h.8.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.8.mlp.c_proj.weight.absmax\", \"transformer.h.8.mlp.c_proj.weight.quant_map\", \"transformer.h.8.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.attn.c_attn.weight.absmax\", \"transformer.h.9.attn.c_attn.weight.quant_map\", \"transformer.h.9.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.attn.c_proj.weight.absmax\", \"transformer.h.9.attn.c_proj.weight.quant_map\", \"transformer.h.9.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.mlp.w1.weight.absmax\", \"transformer.h.9.mlp.w1.weight.quant_map\", \"transformer.h.9.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.mlp.w2.weight.absmax\", \"transformer.h.9.mlp.w2.weight.quant_map\", \"transformer.h.9.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.9.mlp.c_proj.weight.absmax\", \"transformer.h.9.mlp.c_proj.weight.quant_map\", \"transformer.h.9.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.attn.c_attn.weight.absmax\", \"transformer.h.10.attn.c_attn.weight.quant_map\", \"transformer.h.10.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.attn.c_proj.weight.absmax\", \"transformer.h.10.attn.c_proj.weight.quant_map\", \"transformer.h.10.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.mlp.w1.weight.absmax\", \"transformer.h.10.mlp.w1.weight.quant_map\", \"transformer.h.10.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.mlp.w2.weight.absmax\", \"transformer.h.10.mlp.w2.weight.quant_map\", \"transformer.h.10.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.10.mlp.c_proj.weight.absmax\", \"transformer.h.10.mlp.c_proj.weight.quant_map\", \"transformer.h.10.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.attn.c_attn.weight.absmax\", \"transformer.h.11.attn.c_attn.weight.quant_map\", \"transformer.h.11.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.attn.c_proj.weight.absmax\", \"transformer.h.11.attn.c_proj.weight.quant_map\", \"transformer.h.11.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.mlp.w1.weight.absmax\", \"transformer.h.11.mlp.w1.weight.quant_map\", \"transformer.h.11.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.mlp.w2.weight.absmax\", \"transformer.h.11.mlp.w2.weight.quant_map\", \"transformer.h.11.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.11.mlp.c_proj.weight.absmax\", \"transformer.h.11.mlp.c_proj.weight.quant_map\", \"transformer.h.11.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.attn.c_attn.weight.absmax\", \"transformer.h.12.attn.c_attn.weight.quant_map\", \"transformer.h.12.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.attn.c_proj.weight.absmax\", \"transformer.h.12.attn.c_proj.weight.quant_map\", \"transformer.h.12.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.mlp.w1.weight.absmax\", \"transformer.h.12.mlp.w1.weight.quant_map\", \"transformer.h.12.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.mlp.w2.weight.absmax\", \"transformer.h.12.mlp.w2.weight.quant_map\", \"transformer.h.12.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.12.mlp.c_proj.weight.absmax\", \"transformer.h.12.mlp.c_proj.weight.quant_map\", \"transformer.h.12.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.attn.c_attn.weight.absmax\", \"transformer.h.13.attn.c_attn.weight.quant_map\", \"transformer.h.13.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.attn.c_proj.weight.absmax\", \"transformer.h.13.attn.c_proj.weight.quant_map\", \"transformer.h.13.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.mlp.w1.weight.absmax\", \"transformer.h.13.mlp.w1.weight.quant_map\", \"transformer.h.13.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.mlp.w2.weight.absmax\", \"transformer.h.13.mlp.w2.weight.quant_map\", \"transformer.h.13.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.13.mlp.c_proj.weight.absmax\", \"transformer.h.13.mlp.c_proj.weight.quant_map\", \"transformer.h.13.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.attn.c_attn.weight.absmax\", \"transformer.h.14.attn.c_attn.weight.quant_map\", \"transformer.h.14.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.attn.c_proj.weight.absmax\", \"transformer.h.14.attn.c_proj.weight.quant_map\", \"transformer.h.14.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.mlp.w1.weight.absmax\", \"transformer.h.14.mlp.w1.weight.quant_map\", \"transformer.h.14.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.mlp.w2.weight.absmax\", \"transformer.h.14.mlp.w2.weight.quant_map\", \"transformer.h.14.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.14.mlp.c_proj.weight.absmax\", \"transformer.h.14.mlp.c_proj.weight.quant_map\", \"transformer.h.14.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.attn.c_attn.weight.absmax\", \"transformer.h.15.attn.c_attn.weight.quant_map\", \"transformer.h.15.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.attn.c_proj.weight.absmax\", \"transformer.h.15.attn.c_proj.weight.quant_map\", \"transformer.h.15.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.mlp.w1.weight.absmax\", \"transformer.h.15.mlp.w1.weight.quant_map\", \"transformer.h.15.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.mlp.w2.weight.absmax\", \"transformer.h.15.mlp.w2.weight.quant_map\", \"transformer.h.15.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.15.mlp.c_proj.weight.absmax\", \"transformer.h.15.mlp.c_proj.weight.quant_map\", \"transformer.h.15.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.attn.c_attn.weight.absmax\", \"transformer.h.16.attn.c_attn.weight.quant_map\", \"transformer.h.16.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.attn.c_proj.weight.absmax\", \"transformer.h.16.attn.c_proj.weight.quant_map\", \"transformer.h.16.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.mlp.w1.weight.absmax\", \"transformer.h.16.mlp.w1.weight.quant_map\", \"transformer.h.16.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.mlp.w2.weight.absmax\", \"transformer.h.16.mlp.w2.weight.quant_map\", \"transformer.h.16.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.16.mlp.c_proj.weight.absmax\", \"transformer.h.16.mlp.c_proj.weight.quant_map\", \"transformer.h.16.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.attn.c_attn.weight.absmax\", \"transformer.h.17.attn.c_attn.weight.quant_map\", \"transformer.h.17.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.attn.c_proj.weight.absmax\", \"transformer.h.17.attn.c_proj.weight.quant_map\", \"transformer.h.17.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.mlp.w1.weight.absmax\", \"transformer.h.17.mlp.w1.weight.quant_map\", \"transformer.h.17.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.mlp.w2.weight.absmax\", \"transformer.h.17.mlp.w2.weight.quant_map\", \"transformer.h.17.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.17.mlp.c_proj.weight.absmax\", \"transformer.h.17.mlp.c_proj.weight.quant_map\", \"transformer.h.17.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.attn.c_attn.weight.absmax\", \"transformer.h.18.attn.c_attn.weight.quant_map\", \"transformer.h.18.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.attn.c_proj.weight.absmax\", \"transformer.h.18.attn.c_proj.weight.quant_map\", \"transformer.h.18.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.mlp.w1.weight.absmax\", \"transformer.h.18.mlp.w1.weight.quant_map\", \"transformer.h.18.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.mlp.w2.weight.absmax\", \"transformer.h.18.mlp.w2.weight.quant_map\", \"transformer.h.18.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.18.mlp.c_proj.weight.absmax\", \"transformer.h.18.mlp.c_proj.weight.quant_map\", \"transformer.h.18.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.attn.c_attn.weight.absmax\", \"transformer.h.19.attn.c_attn.weight.quant_map\", \"transformer.h.19.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.attn.c_proj.weight.absmax\", \"transformer.h.19.attn.c_proj.weight.quant_map\", \"transformer.h.19.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.mlp.w1.weight.absmax\", \"transformer.h.19.mlp.w1.weight.quant_map\", \"transformer.h.19.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.mlp.w2.weight.absmax\", \"transformer.h.19.mlp.w2.weight.quant_map\", \"transformer.h.19.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.19.mlp.c_proj.weight.absmax\", \"transformer.h.19.mlp.c_proj.weight.quant_map\", \"transformer.h.19.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.attn.c_attn.weight.absmax\", \"transformer.h.20.attn.c_attn.weight.quant_map\", \"transformer.h.20.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.attn.c_proj.weight.absmax\", \"transformer.h.20.attn.c_proj.weight.quant_map\", \"transformer.h.20.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.mlp.w1.weight.absmax\", \"transformer.h.20.mlp.w1.weight.quant_map\", \"transformer.h.20.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.mlp.w2.weight.absmax\", \"transformer.h.20.mlp.w2.weight.quant_map\", \"transformer.h.20.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.20.mlp.c_proj.weight.absmax\", \"transformer.h.20.mlp.c_proj.weight.quant_map\", \"transformer.h.20.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.attn.c_attn.weight.absmax\", \"transformer.h.21.attn.c_attn.weight.quant_map\", \"transformer.h.21.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.attn.c_proj.weight.absmax\", \"transformer.h.21.attn.c_proj.weight.quant_map\", \"transformer.h.21.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.mlp.w1.weight.absmax\", \"transformer.h.21.mlp.w1.weight.quant_map\", \"transformer.h.21.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.mlp.w2.weight.absmax\", \"transformer.h.21.mlp.w2.weight.quant_map\", \"transformer.h.21.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.21.mlp.c_proj.weight.absmax\", \"transformer.h.21.mlp.c_proj.weight.quant_map\", \"transformer.h.21.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.attn.c_attn.weight.absmax\", \"transformer.h.22.attn.c_attn.weight.quant_map\", \"transformer.h.22.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.attn.c_proj.weight.absmax\", \"transformer.h.22.attn.c_proj.weight.quant_map\", \"transformer.h.22.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.mlp.w1.weight.absmax\", \"transformer.h.22.mlp.w1.weight.quant_map\", \"transformer.h.22.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.mlp.w2.weight.absmax\", \"transformer.h.22.mlp.w2.weight.quant_map\", \"transformer.h.22.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.22.mlp.c_proj.weight.absmax\", \"transformer.h.22.mlp.c_proj.weight.quant_map\", \"transformer.h.22.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.attn.c_attn.weight.absmax\", \"transformer.h.23.attn.c_attn.weight.quant_map\", \"transformer.h.23.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.attn.c_proj.weight.absmax\", \"transformer.h.23.attn.c_proj.weight.quant_map\", \"transformer.h.23.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.mlp.w1.weight.absmax\", \"transformer.h.23.mlp.w1.weight.quant_map\", \"transformer.h.23.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.mlp.w2.weight.absmax\", \"transformer.h.23.mlp.w2.weight.quant_map\", \"transformer.h.23.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.23.mlp.c_proj.weight.absmax\", \"transformer.h.23.mlp.c_proj.weight.quant_map\", \"transformer.h.23.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.attn.c_attn.weight.absmax\", \"transformer.h.24.attn.c_attn.weight.quant_map\", \"transformer.h.24.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.attn.c_proj.weight.absmax\", \"transformer.h.24.attn.c_proj.weight.quant_map\", \"transformer.h.24.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.mlp.w1.weight.absmax\", \"transformer.h.24.mlp.w1.weight.quant_map\", \"transformer.h.24.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.mlp.w2.weight.absmax\", \"transformer.h.24.mlp.w2.weight.quant_map\", \"transformer.h.24.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.24.mlp.c_proj.weight.absmax\", \"transformer.h.24.mlp.c_proj.weight.quant_map\", \"transformer.h.24.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.attn.c_attn.weight.absmax\", \"transformer.h.25.attn.c_attn.weight.quant_map\", \"transformer.h.25.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.attn.c_proj.weight.absmax\", \"transformer.h.25.attn.c_proj.weight.quant_map\", \"transformer.h.25.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.mlp.w1.weight.absmax\", \"transformer.h.25.mlp.w1.weight.quant_map\", \"transformer.h.25.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.mlp.w2.weight.absmax\", \"transformer.h.25.mlp.w2.weight.quant_map\", \"transformer.h.25.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.25.mlp.c_proj.weight.absmax\", \"transformer.h.25.mlp.c_proj.weight.quant_map\", \"transformer.h.25.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.attn.c_attn.weight.absmax\", \"transformer.h.26.attn.c_attn.weight.quant_map\", \"transformer.h.26.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.attn.c_proj.weight.absmax\", \"transformer.h.26.attn.c_proj.weight.quant_map\", \"transformer.h.26.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.mlp.w1.weight.absmax\", \"transformer.h.26.mlp.w1.weight.quant_map\", \"transformer.h.26.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.mlp.w2.weight.absmax\", \"transformer.h.26.mlp.w2.weight.quant_map\", \"transformer.h.26.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.26.mlp.c_proj.weight.absmax\", \"transformer.h.26.mlp.c_proj.weight.quant_map\", \"transformer.h.26.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.attn.c_attn.weight.absmax\", \"transformer.h.27.attn.c_attn.weight.quant_map\", \"transformer.h.27.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.attn.c_proj.weight.absmax\", \"transformer.h.27.attn.c_proj.weight.quant_map\", \"transformer.h.27.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.mlp.w1.weight.absmax\", \"transformer.h.27.mlp.w1.weight.quant_map\", \"transformer.h.27.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.mlp.w2.weight.absmax\", \"transformer.h.27.mlp.w2.weight.quant_map\", \"transformer.h.27.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.27.mlp.c_proj.weight.absmax\", \"transformer.h.27.mlp.c_proj.weight.quant_map\", \"transformer.h.27.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.attn.c_attn.weight.absmax\", \"transformer.h.28.attn.c_attn.weight.quant_map\", \"transformer.h.28.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.attn.c_proj.weight.absmax\", \"transformer.h.28.attn.c_proj.weight.quant_map\", \"transformer.h.28.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.mlp.w1.weight.absmax\", \"transformer.h.28.mlp.w1.weight.quant_map\", \"transformer.h.28.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.mlp.w2.weight.absmax\", \"transformer.h.28.mlp.w2.weight.quant_map\", \"transformer.h.28.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.28.mlp.c_proj.weight.absmax\", \"transformer.h.28.mlp.c_proj.weight.quant_map\", \"transformer.h.28.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.attn.c_attn.weight.absmax\", \"transformer.h.29.attn.c_attn.weight.quant_map\", \"transformer.h.29.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.attn.c_proj.weight.absmax\", \"transformer.h.29.attn.c_proj.weight.quant_map\", \"transformer.h.29.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.mlp.w1.weight.absmax\", \"transformer.h.29.mlp.w1.weight.quant_map\", \"transformer.h.29.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.mlp.w2.weight.absmax\", \"transformer.h.29.mlp.w2.weight.quant_map\", \"transformer.h.29.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.29.mlp.c_proj.weight.absmax\", \"transformer.h.29.mlp.c_proj.weight.quant_map\", \"transformer.h.29.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.attn.c_attn.weight.absmax\", \"transformer.h.30.attn.c_attn.weight.quant_map\", \"transformer.h.30.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.attn.c_proj.weight.absmax\", \"transformer.h.30.attn.c_proj.weight.quant_map\", \"transformer.h.30.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.mlp.w1.weight.absmax\", \"transformer.h.30.mlp.w1.weight.quant_map\", \"transformer.h.30.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.mlp.w2.weight.absmax\", \"transformer.h.30.mlp.w2.weight.quant_map\", \"transformer.h.30.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.30.mlp.c_proj.weight.absmax\", \"transformer.h.30.mlp.c_proj.weight.quant_map\", \"transformer.h.30.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.attn.c_attn.weight.absmax\", \"transformer.h.31.attn.c_attn.weight.quant_map\", \"transformer.h.31.attn.c_attn.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.attn.c_proj.weight.absmax\", \"transformer.h.31.attn.c_proj.weight.quant_map\", \"transformer.h.31.attn.c_proj.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.mlp.w1.weight.absmax\", \"transformer.h.31.mlp.w1.weight.quant_map\", \"transformer.h.31.mlp.w1.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.mlp.w2.weight.absmax\", \"transformer.h.31.mlp.w2.weight.quant_map\", \"transformer.h.31.mlp.w2.weight.quant_state.bitsandbytes__fp4\", \"transformer.h.31.mlp.c_proj.weight.absmax\", \"transformer.h.31.mlp.c_proj.weight.quant_map\", \"transformer.h.31.mlp.c_proj.weight.quant_state.bitsandbytes__fp4\". \n\tsize mismatch for transformer.h.0.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.0.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.0.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.0.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.0.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.1.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.1.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.1.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.1.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.1.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.2.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.2.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.2.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.2.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.2.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.3.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.3.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.3.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.3.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.3.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.4.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.4.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.4.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.4.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.4.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.5.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.5.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.5.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.5.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.5.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.6.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.6.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.6.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.6.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.6.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.7.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.7.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.7.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.7.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.7.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.8.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.8.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.8.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.8.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.8.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.9.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.9.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.9.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.9.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.9.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.10.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.10.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.10.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.10.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.10.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.11.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.11.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.11.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.11.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.11.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.12.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.12.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.12.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.12.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.12.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.13.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.13.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.13.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.13.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.13.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.14.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.14.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.14.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.14.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.14.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.15.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.15.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.15.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.15.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.15.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.16.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.16.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.16.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.16.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.16.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.17.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.17.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.17.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.17.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.17.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.18.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.18.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.18.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.18.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.18.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.19.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.19.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.19.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.19.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.19.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.20.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.20.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.20.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.20.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.20.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.21.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.21.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.21.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.21.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.21.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.22.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.22.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.22.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.22.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.22.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.23.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.23.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.23.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.23.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.23.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.24.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.24.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.24.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.24.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.24.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.25.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.25.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.25.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.25.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.25.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.26.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.26.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.26.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.26.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.26.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.27.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.27.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.27.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.27.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.27.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.28.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.28.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.28.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.28.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.28.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.29.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.29.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.29.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.29.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.29.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.30.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.30.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.30.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.30.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.30.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for transformer.h.31.attn.c_attn.weight: copying a param with shape torch.Size([25165824, 1]) from checkpoint, the shape in current model is torch.Size([12288, 4096]).\n\tsize mismatch for transformer.h.31.attn.c_proj.weight: copying a param with shape torch.Size([8388608, 1]) from checkpoint, the shape in current model is torch.Size([4096, 4096]).\n\tsize mismatch for transformer.h.31.mlp.w1.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.31.mlp.w2.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for transformer.h.31.mlp.c_proj.weight: copying a param with shape torch.Size([22544384, 1]) from checkpoint, the shape in current model is torch.Size([4096, 11008])."
     ]
    }
   ],
   "source": [
    "pruned_model_one_shot = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=DEVICE_MAP,\n",
    "    torch_dtype=torch.float16,\n",
    "    #quantization_config={\"load_in_4bit\": True},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pruned_model_one_shot.load_state_dict(torch.load('pruned_qwen_model.pth'))\n",
    "\n",
    "# Test the pruned model\n",
    "perplexity_inital = compute_accuracy(pruned_model_one_shot, tokenized_test_dataset)\n",
    "print(f'Pruned Model - Test Accuracy: {perplexity_inital:.4f}')\n",
    "\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert_pruned\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pruned_Qwen,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Training Pruned Model ===\")\n",
    "trainer.train()\n",
    "\n",
    "perplexity_pruned = compute_accuracy(pruned_Qwen, tokenized_test_dataset)\n",
    "print(f\"Pruned model after fine-tuning: Acc={perplexity_pruned:.4f}\")\n",
    "\n",
    "sparsity = model_sparsity(pruned_Qwen)\n",
    "print(f'Sparsity of the pruned model after fine-tuning: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b6534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
