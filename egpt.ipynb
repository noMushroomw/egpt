{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbcbfd4",
   "metadata": {},
   "source": [
    "## start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923d9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0666b",
   "metadata": {},
   "source": [
    "# 1. get linear mask for effective weight with each weight size [output_size, input_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a772fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cfecd",
   "metadata": {},
   "source": [
    "# 2. set the edge with ineffective weight = 0 and prune the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c3e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n",
    "\n",
    "def model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449c769",
   "metadata": {},
   "source": [
    "# 3. train a linear model first and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e6e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size= [512, 512, 512]):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))        \n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371eb00",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba7578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "\n",
    "# MINIST-10 dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearModel(input_size=28*28, output_size=10, hidden_size=[1024, 512, 512]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "result = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce11e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3062, Accuracy: 1258/10000 (12.58%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.292992\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.630450\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.217274\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.250590\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.150028\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.100614\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.147552\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.104935\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.254316\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.166829\n",
      "\n",
      "Test set: Average loss: 0.0909, Accuracy: 9708/10000 (97.08%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.016918\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.015138\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.064082\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.087206\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.135824\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.188597\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.189097\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.115724\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.130663\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.058731\n",
      "\n",
      "Test set: Average loss: 0.1003, Accuracy: 9694/10000 (96.94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.092183\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.065956\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.023316\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.015216\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.041428\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.031411\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.070660\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.078625\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.042869\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.036287\n",
      "\n",
      "Test set: Average loss: 0.0657, Accuracy: 9795/10000 (97.95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019552\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.065409\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.048916\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.019781\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.041435\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000507\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.052102\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.006302\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.037098\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.044692\n",
      "\n",
      "Test set: Average loss: 0.0670, Accuracy: 9788/10000 (97.88%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.027352\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000852\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.025380\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.004889\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.043197\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.018258\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.026026\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.010471\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.034256\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.010224\n",
      "\n",
      "Test set: Average loss: 0.0758, Accuracy: 9763/10000 (97.63%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.007332\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.001790\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.029685\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.009742\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.007200\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.014140\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.011165\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.069567\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.006745\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.002149\n",
      "\n",
      "Test set: Average loss: 0.0820, Accuracy: 9778/10000 (97.78%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.025048\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.018639\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.069748\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001779\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.051181\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.013855\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.037232\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.002452\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.037424\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.004169\n",
      "\n",
      "Test set: Average loss: 0.0694, Accuracy: 9823/10000 (98.23%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.008480\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.006239\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.000379\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.000670\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013065\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.036554\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.039588\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.028818\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.078682\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.078102\n",
      "\n",
      "Test set: Average loss: 0.0662, Accuracy: 9833/10000 (98.33%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.004003\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.002896\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.015793\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000603\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.009559\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001870\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001028\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.031454\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.010000\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.013595\n",
      "\n",
      "Test set: Average loss: 0.0877, Accuracy: 9794/10000 (97.94%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.005528\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.000267\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.000976\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.074454\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001542\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.005399\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.008455\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.074803\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.023799\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.023137\n",
      "\n",
      "Test set: Average loss: 0.0735, Accuracy: 9815/10000 (98.15%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial test\n",
    "test_loss, test_accuracy = test(model, device, test_loader)\n",
    "result['test_loss'].append(test_loss)\n",
    "result['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    result['train_loss'].append(train_loss)\n",
    "    result['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    # Test after each epoch\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    result['test_loss'].append(test_loss)\n",
    "    result['test_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'linear_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bef4b4",
   "metadata": {},
   "source": [
    "# prune the model and comparing the performance with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779e3e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "pruned_model_renormalized.to(device)\n",
    "\n",
    "# Test the pruned model\n",
    "test_loss, test_accuracy = test(pruned_model_renormalized, device, test_loader)\n",
    "print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model\n",
    "torch.save(pruned_model_renormalized.state_dict(), 'pruned_linear_model_renormalized.pth')\n",
    "\n",
    "\n",
    "prune_model = prune_model_neff(model, renormalize=False)\n",
    "prune_model.to(device)\n",
    "# Test the pruned model without renormalization\n",
    "test_loss, test_accuracy = test(prune_model, device, test_loader)\n",
    "print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model without renormalization\n",
    "torch.save(prune_model.state_dict(), 'pruned_linear_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5985428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "\n",
      "Test set: Average loss: 28035697270544202374587336359936.0000, Accuracy: 8889/10000 (88.89%)\n",
      "\n",
      "Pruned Model - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "Average Pruned Model - Test Loss: 0.0667, Test Accuracy: 98.13%\n",
      "Average Pruned Model with Renormalization - Test Loss: 28035697270544202374587336359936.0000, Test Accuracy: 88.89%\n"
     ]
    }
   ],
   "source": [
    "# test 10 times and show the average performance\n",
    "test_loss_acc = {'prune_loss': [], 'prune_accuracy': [], 'prune_renorm_loss': [], 'prune_renorm_accuracy': []}\n",
    "\n",
    "for i in range(10):\n",
    "    pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "    pruned_model_renormalized.to(device)\n",
    "\n",
    "    # Test the pruned model\n",
    "    test_loss, test_accuracy = test(pruned_model_renormalized, device, test_loader)\n",
    "    print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    test_loss_acc['prune_renorm_loss'].append(test_loss)\n",
    "    test_loss_acc['prune_renorm_accuracy'].append(test_accuracy)\n",
    "    \n",
    "    pruned_model = prune_model_neff(model, renormalize=False)\n",
    "    pruned_model.to(device)\n",
    "    # Test the pruned model without renormalization\n",
    "    test_loss, test_accuracy = test(pruned_model, device, test_loader)\n",
    "    print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    test_loss_acc['prune_loss'].append(test_loss)\n",
    "    test_loss_acc['prune_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# average performance\n",
    "avg_prune_loss = np.mean(test_loss_acc['prune_loss'])\n",
    "avg_prune_accuracy = np.mean(test_loss_acc['prune_accuracy'])\n",
    "avg_prune_renorm_loss = np.mean(test_loss_acc['prune_renorm_loss'])\n",
    "avg_prune_renorm_accuracy = np.mean(test_loss_acc['prune_renorm_accuracy'])\n",
    "\n",
    "print(f'Average Pruned Model - Test Loss: {avg_prune_loss:.4f}, Test Accuracy: {avg_prune_accuracy:.2f}%')\n",
    "print(f'Average Pruned Model with Renormalization - Test Loss: {avg_prune_renorm_loss:.4f}, Test Accuracy: {avg_prune_renorm_accuracy:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322e209",
   "metadata": {},
   "source": [
    "## measure the sparsity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd687cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the pruned model with renormalization: 0.3418\n",
      "Sparsity of the pruned model without renormalization: 0.3418\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sparsity = model_sparsity(pruned_model_renormalized)\n",
    "print(f'Sparsity of the pruned model with renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(prune_model)\n",
    "print(f'Sparsity of the pruned model without renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bacf6",
   "metadata": {},
   "source": [
    "## one-shot fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54eba9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Pruned Model - Test Loss: 0.0667, Test Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "pruned_model_one_shot = LinearModel(input_size=28*28, output_size=10, hidden_size=[1024, 512, 512]).to(device)\n",
    "pruned_model_one_shot.load_state_dict(torch.load('pruned_linear_model.pth'))\n",
    "\n",
    "# Test the pruned model\n",
    "test_loss, test_accuracy = test(pruned_model_one_shot, device, test_loader)\n",
    "print(f'Pruned Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283cd217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.011330\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.034839\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.005182\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.004371\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.011549\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.011232\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.024064\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.002956\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.005015\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.001072\n",
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 9813/10000 (98.13%)\n",
      "\n",
      "Model Sparsity: 34.18%\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_accuracy = train(pruned_model_one_shot, device, train_loader, optimizer, 1)\n",
    "result['train_loss'].append(train_loss)\n",
    "result['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    # Test after each epoch\n",
    "test_loss, test_accuracy = test(pruned_model_one_shot, device, test_loader)\n",
    "result['test_loss'].append(test_loss)\n",
    "result['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "sparsity = model_sparsity(pruned_model_one_shot)\n",
    "print(f'Model Sparsity: {sparsity:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5bea2",
   "metadata": {},
   "source": [
    "# test in huggingface language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860d3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, tokenized_dataset, batch_size=32):\n",
    "    from torch.utils.data import DataLoader\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loader = DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35295451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_19303/2390932059.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "raw_dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "test_dataset = raw_dataset[\"test\"]\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenize and format\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# 4. Load and fine-tune BERT on train split\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196726d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Original Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [37500/37500 32:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.064800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned original model: Sparsity=0.0000, Acc=0.9428\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Training Original Model ===\")\n",
    "trainer.train()\n",
    "\n",
    "orig_acc = compute_accuracy(model, tokenized_test_dataset)\n",
    "sparsity = model_sparsity(model)\n",
    "print(f\"\\nFine-tuned original model: Sparsity={sparsity:.4f}, Acc={orig_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'bert_origin.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c621ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned model: Sparsity=0.2923, Acc=0.9447\n",
      "Pruned model with renormalization: Sparsity=0.2923, Acc=0.2500\n"
     ]
    }
   ],
   "source": [
    "pruned_model_renormalized = prune_model_neff(model, renormalize=True)\n",
    "pruned_model_renormalized.to(device)\n",
    "pruned_model = prune_model_neff(model, renormalize=False)\n",
    "pruned_model.to(device)\n",
    "\n",
    "prune_acc = compute_accuracy(pruned_model, tokenized_test_dataset)\n",
    "prune_renorm_acc = compute_accuracy(pruned_model_renormalized, tokenized_test_dataset)\n",
    "pruned_sparsity = model_sparsity(pruned_model)\n",
    "pruned_renorm_sparsity = model_sparsity(pruned_model_renormalized)\n",
    "\n",
    "print(f\"\\nPruned model: Sparsity={pruned_sparsity:.4f}, Acc={prune_acc:.4f}\")\n",
    "print(f\"Pruned model with renormalization: Sparsity={pruned_renorm_sparsity:.4f}, Acc={prune_renorm_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfadfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pruned_model.state_dict(), 'bert_pruned.pth')\n",
    "torch.save(pruned_model_renormalized.state_dict(), 'bert_pruned_renorm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2fd80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Model - Test Accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "pruned_model_one_shot = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4).to(device)\n",
    "pruned_model_one_shot.load_state_dict(torch.load('bert_pruned.pth'))\n",
    "\n",
    "# Test the pruned model\n",
    "inital_acc = compute_accuracy(pruned_model_one_shot, tokenized_test_dataset)\n",
    "print(f'Pruned Model - Test Accuracy: {inital_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afa9cc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19303/61186683.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Pruned Model ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 06:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.055400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model after fine-tuning: Acc=0.9421\n",
      "Sparsity of the pruned model after fine-tuning: 0.0000\n"
     ]
    }
   ],
   "source": [
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert_pruned\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pruned_model_one_shot,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Training Pruned Model ===\")\n",
    "trainer.train()\n",
    "pruned_acc = compute_accuracy(pruned_model_one_shot, tokenized_test_dataset)\n",
    "print(f\"Pruned model after fine-tuning: Acc={pruned_acc:.4f}\")\n",
    "\n",
    "sparsity = model_sparsity(pruned_model_one_shot)\n",
    "print(f'Sparsity of the pruned model after fine-tuning: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f57e2",
   "metadata": {},
   "source": [
    "## test for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8814919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen-7B\"  # Use \"Qwen/Qwen-7B\" for smaller variant\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "DEVICE_MAP = \"auto\"  # Automatically distributes across GPUs\n",
    "BATCH_SIZE = 1  # Reduce if OOM errors occur\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(DATASET_NAME, DATASET_CONFIG, split=\"test\")\n",
    "texts = [text for text in test_dataset[\"text\"] if text.strip()]  # Remove empty strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4540dea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe1b06ec202497c9456811bcdef4dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model with quantization (4-bit) to reduce VRAM usage\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=DEVICE_MAP,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config={\"load_in_4bit\": True},\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32ccce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 100/100 [00:06<00:00, 15.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 19.62\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity\n",
    "model.eval()\n",
    "total_log_likelihood = 0\n",
    "total_tokens = 0\n",
    "\n",
    "test_text = texts[:100]  # Limit to first 100 texts for testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(test_text, desc=\"Calculating Perplexity\"):\n",
    "        # Tokenize text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "        \n",
    "        # Forward pass to get loss\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "\n",
    "# Final perplexity calculation\n",
    "perplexity = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), 'qwen_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c12ecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_llm(model, renormalize=False):\n",
    "    \"\"\"\n",
    "    Prune LLM model by targeting only standard Linear layers\n",
    "    Avoids quantized layers and special layer types\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(model)\n",
    "    pruned_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Only prune standard nn.Linear layers, avoid quantized layers\n",
    "        if isinstance(module, nn.Linear) and not hasattr(module, 'quant_state'):\n",
    "            try:\n",
    "                mask = get_linear_mask(module).to(module.weight.device)\n",
    "                with torch.no_grad():\n",
    "                    module.weight *= mask.float()\n",
    "                    \n",
    "                    if renormalize:\n",
    "                        # More stable renormalization\n",
    "                        row_sum = module.weight.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                        module.weight.div_(row_sum)\n",
    "                    \n",
    "                    pruned_layers.append(name)\n",
    "                    \n",
    "                    # Check sparsity of this layer\n",
    "                    sparsity = (module.weight == 0).float().mean().item()\n",
    "                    print(f\"Pruned {name}: {sparsity:.2%} sparsity\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Successfully pruned {len(pruned_layers)} layers\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a5d110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_inplace(model, renormalize=False):\n",
    "    \"\"\"\n",
    "    Prune model in-place to avoid memory issues\n",
    "    \"\"\"\n",
    "    pruned_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Only prune standard nn.Linear layers, avoid quantized layers\n",
    "        if isinstance(module, nn.Linear) and not hasattr(module, 'quant_state'):\n",
    "            try:\n",
    "                # Create mask\n",
    "                mask = get_linear_mask(module).to(module.weight.device)\n",
    "                \n",
    "                # Apply pruning in-place\n",
    "                with torch.no_grad():\n",
    "                    module.weight.data *= mask.float()\n",
    "                    \n",
    "                    if renormalize:\n",
    "                        row_sum = module.weight.data.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                        module.weight.data.div_(row_sum)\n",
    "                    \n",
    "                    pruned_layers.append(name)\n",
    "                    sparsity = (module.weight.data == 0).float().mean().item()\n",
    "                    print(f\"Pruned {name}: {sparsity:.2%} sparsity\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Successfully pruned {len(pruned_layers)} layers\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c53f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned lm_head: 39.97% sparsity\n",
      "Successfully pruned 1 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity for Pruned Model: 100%|██████████| 100/100 [00:15<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Pruned Model: 21.05\n",
      "Sparsity of the pruned model: 0.0555\n"
     ]
    }
   ],
   "source": [
    "pruned_Qwen = prune_model_neff_inplace(model, renormalize=False)\n",
    "\n",
    "pruned_Qwen.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Calculate perplexity\n",
    "pruned_Qwen.eval()\n",
    "total_log_likelihood = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(test_text, desc=\"Calculating Perplexity for Pruned Model\"):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(pruned_Qwen.device)\n",
    "        outputs = pruned_Qwen(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "        \n",
    "perplexity_pruned = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity of Pruned Model: {perplexity_pruned:.2f}\")\n",
    "\n",
    "sparsity = model_sparsity(pruned_Qwen)\n",
    "print(f'Sparsity of the pruned model: {sparsity:.4f}')\n",
    "\n",
    "torch.save(pruned_Qwen.state_dict(), 'pruned_qwen_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c429cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_inplace(model, renormalize=False):\n",
    "    \"\"\"\n",
    "    Prune model in-place to avoid memory issues\n",
    "    \"\"\"\n",
    "    pruned_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Only prune standard nn.Linear layers\n",
    "        if isinstance(module, nn.Linear) and not hasattr(module, 'quant_state'):\n",
    "            try:\n",
    "                # Create mask\n",
    "                mask = get_linear_mask(module).to(module.weight.device)\n",
    "                \n",
    "                # Apply pruning in-place\n",
    "                with torch.no_grad():\n",
    "                    module.weight.data *= mask.float()\n",
    "                    \n",
    "                    if renormalize:\n",
    "                        row_sum = module.weight.data.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                        module.weight.data.div_(row_sum)\n",
    "                    \n",
    "                    pruned_layers.append(name)\n",
    "                    sparsity = (module.weight.data == 0).float().mean().item()\n",
    "                    print(f\"Pruned {name}: {sparsity:.2%} sparsity\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"Successfully pruned {len(pruned_layers)} layers\")\n",
    "    return model\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts, max_length=512):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a list of texts\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_log_likelihood = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=\"Calculating Perplexity\"):\n",
    "            # Tokenize text\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
    "            \n",
    "            # Forward pass to get loss\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss.item()\n",
    "            \n",
    "            # Update metrics\n",
    "            total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "            total_tokens += inputs[\"input_ids\"].size(1)\n",
    "    \n",
    "    # Final perplexity calculation\n",
    "    perplexity = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6ab0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9958c4b66e24af184b07531e2f96892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating original perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 100/100 [00:03<00:00, 29.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Perplexity: 17.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model for pruning...\")\n",
    "model_for_pruning = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=DEVICE_MAP,\n",
    "    torch_dtype=torch.float16,  # No quantization\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Step 2: Calculate original perplexity\n",
    "print(\"Calculating original perplexity...\")\n",
    "original_perplexity = calculate_perplexity(model_for_pruning, tokenizer, texts[:100])\n",
    "print(f\"Original Perplexity: {original_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for pruning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3836c9226a9d4e8396ca28c346031db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating original perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 100/100 [00:03<00:00, 30.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Perplexity: 17.18\n",
      "Pruning model...\n",
      "Pruned transformer.h.0.attn.c_attn: 44.46% sparsity\n",
      "Pruned transformer.h.0.attn.c_proj: 37.45% sparsity\n",
      "Pruned transformer.h.0.mlp.w1: 36.58% sparsity\n",
      "Pruned transformer.h.0.mlp.w2: 36.59% sparsity\n",
      "Pruned transformer.h.0.mlp.c_proj: 37.07% sparsity\n",
      "Pruned transformer.h.1.attn.c_attn: 46.15% sparsity\n",
      "Pruned transformer.h.1.attn.c_proj: 37.27% sparsity\n",
      "Pruned transformer.h.1.mlp.w1: 36.65% sparsity\n",
      "Pruned transformer.h.1.mlp.w2: 36.79% sparsity\n",
      "Pruned transformer.h.1.mlp.c_proj: 37.05% sparsity\n",
      "Pruned transformer.h.2.attn.c_attn: 45.42% sparsity\n",
      "Pruned transformer.h.2.attn.c_proj: 37.34% sparsity\n",
      "Pruned transformer.h.2.mlp.w1: 36.67% sparsity\n",
      "Pruned transformer.h.2.mlp.w2: 36.85% sparsity\n",
      "Pruned transformer.h.2.mlp.c_proj: 36.86% sparsity\n",
      "Pruned transformer.h.3.attn.c_attn: 42.56% sparsity\n",
      "Pruned transformer.h.3.attn.c_proj: 37.33% sparsity\n",
      "Pruned transformer.h.3.mlp.w1: 36.67% sparsity\n",
      "Pruned transformer.h.3.mlp.w2: 36.65% sparsity\n",
      "Pruned transformer.h.3.mlp.c_proj: 36.76% sparsity\n",
      "Pruned transformer.h.4.attn.c_attn: 41.66% sparsity\n",
      "Pruned transformer.h.4.attn.c_proj: 37.18% sparsity\n",
      "Pruned transformer.h.4.mlp.w1: 36.63% sparsity\n",
      "Pruned transformer.h.4.mlp.w2: 36.72% sparsity\n",
      "Pruned transformer.h.4.mlp.c_proj: 36.70% sparsity\n",
      "Pruned transformer.h.5.attn.c_attn: 40.83% sparsity\n",
      "Pruned transformer.h.5.attn.c_proj: 37.21% sparsity\n",
      "Pruned transformer.h.5.mlp.w1: 36.66% sparsity\n",
      "Pruned transformer.h.5.mlp.w2: 36.80% sparsity\n",
      "Pruned transformer.h.5.mlp.c_proj: 36.73% sparsity\n",
      "Pruned transformer.h.6.attn.c_attn: 40.44% sparsity\n",
      "Pruned transformer.h.6.attn.c_proj: 37.11% sparsity\n",
      "Pruned transformer.h.6.mlp.w1: 36.66% sparsity\n",
      "Pruned transformer.h.6.mlp.w2: 36.92% sparsity\n",
      "Pruned transformer.h.6.mlp.c_proj: 36.75% sparsity\n",
      "Pruned transformer.h.7.attn.c_attn: 40.08% sparsity\n",
      "Pruned transformer.h.7.attn.c_proj: 37.10% sparsity\n",
      "Pruned transformer.h.7.mlp.w1: 36.71% sparsity\n",
      "Pruned transformer.h.7.mlp.w2: 37.20% sparsity\n",
      "Pruned transformer.h.7.mlp.c_proj: 36.79% sparsity\n",
      "Pruned transformer.h.8.attn.c_attn: 40.51% sparsity\n",
      "Pruned transformer.h.8.attn.c_proj: 38.94% sparsity\n",
      "Pruned transformer.h.8.mlp.w1: 36.82% sparsity\n",
      "Pruned transformer.h.8.mlp.w2: 37.35% sparsity\n",
      "Pruned transformer.h.8.mlp.c_proj: 36.84% sparsity\n",
      "Pruned transformer.h.9.attn.c_attn: 40.86% sparsity\n",
      "Pruned transformer.h.9.attn.c_proj: 37.02% sparsity\n",
      "Pruned transformer.h.9.mlp.w1: 36.80% sparsity\n",
      "Pruned transformer.h.9.mlp.w2: 37.44% sparsity\n",
      "Pruned transformer.h.9.mlp.c_proj: 36.74% sparsity\n",
      "Pruned transformer.h.10.attn.c_attn: 41.86% sparsity\n",
      "Pruned transformer.h.10.attn.c_proj: 36.91% sparsity\n",
      "Pruned transformer.h.10.mlp.w1: 36.89% sparsity\n",
      "Pruned transformer.h.10.mlp.w2: 37.66% sparsity\n",
      "Pruned transformer.h.10.mlp.c_proj: 36.75% sparsity\n",
      "Pruned transformer.h.11.attn.c_attn: 40.12% sparsity\n",
      "Pruned transformer.h.11.attn.c_proj: 36.84% sparsity\n",
      "Pruned transformer.h.11.mlp.w1: 36.97% sparsity\n",
      "Pruned transformer.h.11.mlp.w2: 37.87% sparsity\n",
      "Pruned transformer.h.11.mlp.c_proj: 36.75% sparsity\n",
      "Pruned transformer.h.12.attn.c_attn: 40.88% sparsity\n",
      "Pruned transformer.h.12.attn.c_proj: 36.90% sparsity\n",
      "Pruned transformer.h.12.mlp.w1: 37.00% sparsity\n",
      "Pruned transformer.h.12.mlp.w2: 37.87% sparsity\n",
      "Pruned transformer.h.12.mlp.c_proj: 36.75% sparsity\n",
      "Pruned transformer.h.13.attn.c_attn: 39.91% sparsity\n",
      "Pruned transformer.h.13.attn.c_proj: 36.82% sparsity\n",
      "Pruned transformer.h.13.mlp.w1: 37.05% sparsity\n",
      "Pruned transformer.h.13.mlp.w2: 38.17% sparsity\n",
      "Pruned transformer.h.13.mlp.c_proj: 36.74% sparsity\n",
      "Pruned transformer.h.14.attn.c_attn: 40.30% sparsity\n",
      "Pruned transformer.h.14.attn.c_proj: 36.85% sparsity\n",
      "Pruned transformer.h.14.mlp.w1: 36.98% sparsity\n",
      "Pruned transformer.h.14.mlp.w2: 37.73% sparsity\n",
      "Pruned transformer.h.14.mlp.c_proj: 36.71% sparsity\n",
      "Pruned transformer.h.15.attn.c_attn: 38.77% sparsity\n",
      "Pruned transformer.h.15.attn.c_proj: 36.80% sparsity\n",
      "Pruned transformer.h.15.mlp.w1: 37.02% sparsity\n",
      "Pruned transformer.h.15.mlp.w2: 38.11% sparsity\n",
      "Pruned transformer.h.15.mlp.c_proj: 36.73% sparsity\n",
      "Pruned transformer.h.16.attn.c_attn: 39.52% sparsity\n",
      "Pruned transformer.h.16.attn.c_proj: 36.80% sparsity\n",
      "Pruned transformer.h.16.mlp.w1: 36.98% sparsity\n",
      "Pruned transformer.h.16.mlp.w2: 37.79% sparsity\n",
      "Pruned transformer.h.16.mlp.c_proj: 36.68% sparsity\n",
      "Pruned transformer.h.17.attn.c_attn: 38.58% sparsity\n",
      "Pruned transformer.h.17.attn.c_proj: 36.79% sparsity\n",
      "Pruned transformer.h.17.mlp.w1: 37.03% sparsity\n",
      "Pruned transformer.h.17.mlp.w2: 37.88% sparsity\n",
      "Pruned transformer.h.17.mlp.c_proj: 36.68% sparsity\n",
      "Pruned transformer.h.18.attn.c_attn: 38.96% sparsity\n",
      "Pruned transformer.h.18.attn.c_proj: 36.79% sparsity\n",
      "Pruned transformer.h.18.mlp.w1: 36.93% sparsity\n",
      "Pruned transformer.h.18.mlp.w2: 37.63% sparsity\n",
      "Pruned transformer.h.18.mlp.c_proj: 36.70% sparsity\n",
      "Pruned transformer.h.19.attn.c_attn: 38.32% sparsity\n",
      "Pruned transformer.h.19.attn.c_proj: 36.75% sparsity\n",
      "Pruned transformer.h.19.mlp.w1: 36.90% sparsity\n",
      "Pruned transformer.h.19.mlp.w2: 37.37% sparsity\n",
      "Pruned transformer.h.19.mlp.c_proj: 36.65% sparsity\n",
      "Pruned transformer.h.20.attn.c_attn: 38.33% sparsity\n",
      "Pruned transformer.h.20.attn.c_proj: 36.78% sparsity\n",
      "Pruned transformer.h.20.mlp.w1: 36.85% sparsity\n",
      "Pruned transformer.h.20.mlp.w2: 37.31% sparsity\n",
      "Pruned transformer.h.20.mlp.c_proj: 36.65% sparsity\n",
      "Pruned transformer.h.21.attn.c_attn: 38.37% sparsity\n",
      "Pruned transformer.h.21.attn.c_proj: 36.74% sparsity\n",
      "Pruned transformer.h.21.mlp.w1: 36.81% sparsity\n",
      "Pruned transformer.h.21.mlp.w2: 37.11% sparsity\n",
      "Pruned transformer.h.21.mlp.c_proj: 36.63% sparsity\n",
      "Pruned transformer.h.22.attn.c_attn: 38.24% sparsity\n",
      "Pruned transformer.h.22.attn.c_proj: 36.94% sparsity\n",
      "Pruned transformer.h.22.mlp.w1: 36.72% sparsity\n",
      "Pruned transformer.h.22.mlp.w2: 36.92% sparsity\n",
      "Pruned transformer.h.22.mlp.c_proj: 36.65% sparsity\n",
      "Pruned transformer.h.23.attn.c_attn: 37.82% sparsity\n",
      "Pruned transformer.h.23.attn.c_proj: 36.83% sparsity\n",
      "Pruned transformer.h.23.mlp.w1: 36.69% sparsity\n",
      "Pruned transformer.h.23.mlp.w2: 36.81% sparsity\n",
      "Pruned transformer.h.23.mlp.c_proj: 36.63% sparsity\n",
      "Pruned transformer.h.24.attn.c_attn: 38.25% sparsity\n",
      "Pruned transformer.h.24.attn.c_proj: 36.83% sparsity\n",
      "Pruned transformer.h.24.mlp.w1: 36.65% sparsity\n",
      "Pruned transformer.h.24.mlp.w2: 36.72% sparsity\n",
      "Pruned transformer.h.24.mlp.c_proj: 36.63% sparsity\n",
      "Pruned transformer.h.25.attn.c_attn: 38.37% sparsity\n",
      "Pruned transformer.h.25.attn.c_proj: 36.82% sparsity\n",
      "Pruned transformer.h.25.mlp.w1: 36.61% sparsity\n",
      "Pruned transformer.h.25.mlp.w2: 36.70% sparsity\n",
      "Pruned transformer.h.25.mlp.c_proj: 36.62% sparsity\n",
      "Pruned transformer.h.26.attn.c_attn: 38.36% sparsity\n",
      "Pruned transformer.h.26.attn.c_proj: 36.97% sparsity\n",
      "Pruned transformer.h.26.mlp.w1: 36.61% sparsity\n",
      "Pruned transformer.h.26.mlp.w2: 36.68% sparsity\n",
      "Pruned transformer.h.26.mlp.c_proj: 36.63% sparsity\n",
      "Pruned transformer.h.27.attn.c_attn: 38.19% sparsity\n",
      "Pruned transformer.h.27.attn.c_proj: 36.79% sparsity\n",
      "Pruned transformer.h.27.mlp.w1: 36.66% sparsity\n",
      "Pruned transformer.h.27.mlp.w2: 36.74% sparsity\n",
      "Pruned transformer.h.27.mlp.c_proj: 36.65% sparsity\n",
      "Pruned transformer.h.28.attn.c_attn: 38.75% sparsity\n",
      "Pruned transformer.h.28.attn.c_proj: 36.74% sparsity\n",
      "Pruned transformer.h.28.mlp.w1: 36.68% sparsity\n",
      "Pruned transformer.h.28.mlp.w2: 36.82% sparsity\n",
      "Pruned transformer.h.28.mlp.c_proj: 36.66% sparsity\n",
      "Pruned transformer.h.29.attn.c_attn: 38.75% sparsity\n",
      "Pruned transformer.h.29.attn.c_proj: 36.73% sparsity\n",
      "Pruned transformer.h.29.mlp.w1: 36.70% sparsity\n",
      "Pruned transformer.h.29.mlp.w2: 36.94% sparsity\n",
      "Pruned transformer.h.29.mlp.c_proj: 36.70% sparsity\n",
      "Pruned transformer.h.30.attn.c_attn: 38.41% sparsity\n",
      "Pruned transformer.h.30.attn.c_proj: 36.89% sparsity\n",
      "Pruned transformer.h.30.mlp.w1: 37.01% sparsity\n",
      "Pruned transformer.h.30.mlp.w2: 37.14% sparsity\n",
      "Pruned transformer.h.30.mlp.c_proj: 37.11% sparsity\n",
      "Pruned transformer.h.31.attn.c_attn: 37.49% sparsity\n",
      "Pruned transformer.h.31.attn.c_proj: 38.67% sparsity\n",
      "Pruned transformer.h.31.mlp.w1: 37.11% sparsity\n",
      "Pruned transformer.h.31.mlp.w2: 37.34% sparsity\n",
      "Pruned transformer.h.31.mlp.c_proj: 38.22% sparsity\n",
      "Pruned lm_head: 39.97% sparsity\n",
      "Successfully pruned 161 layers\n",
      "Calculating pruned perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 100/100 [00:03<00:00, 31.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Perplexity: 30.47\n",
      "Model Sparsity: 0.3484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 3: Prune the model in-place\n",
    "print(\"Pruning model...\")\n",
    "pruned_model = prune_model_neff_inplace(model_for_pruning, renormalize=False)\n",
    "\n",
    "# Step 4: Calculate pruned perplexity\n",
    "print(\"Calculating pruned perplexity...\")\n",
    "pruned_perplexity = calculate_perplexity(pruned_model, tokenizer, texts[:100])\n",
    "print(f\"Pruned Perplexity: {pruned_perplexity:.2f}\")\n",
    "\n",
    "# Step 5: Calculate sparsity\n",
    "sparsity = model_sparsity(pruned_model)\n",
    "print(f'Model Sparsity: {sparsity:.4f}')\n",
    "\n",
    "# Step 6: Save the pruned model (optional)\n",
    "torch.save(pruned_model.state_dict(), 'pruned_qwen_fp16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee734096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bd48e14ce742869dea39271b27c857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 100/100 [00:03<00:00, 29.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 30.47\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m finetune_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     35\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tmp_finetuned_bert_pruned\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     45\u001b[0m     model\u001b[38;5;241m=\u001b[39mpruned_model_one_shot,\n\u001b[0;32m     46\u001b[0m     args\u001b[38;5;241m=\u001b[39mfinetune_args,\n\u001b[1;32m---> 47\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_train_dataset\u001b[49m,\n\u001b[0;32m     48\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_dataset,\n\u001b[0;32m     49\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training Pruned Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "pruned_model_one_shot = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=DEVICE_MAP,\n",
    "    torch_dtype=torch.float16,\n",
    "    #quantization_config={\"load_in_4bit\": True},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pruned_model_one_shot.load_state_dict(torch.load('pruned_qwen_fp16.pth'))\n",
    "\n",
    "# Test the pruned model\n",
    "pruned_model_one_shot.eval()\n",
    "total_log_likelihood = 0\n",
    "total_tokens = 0\n",
    "\n",
    "test_text = texts[:100]  # Limit to first 100 texts for testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(test_text, desc=\"Calculating Perplexity\"):\n",
    "        # Tokenize text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(pruned_model_one_shot.device)\n",
    "        \n",
    "        # Forward pass to get loss\n",
    "        outputs = pruned_model_one_shot(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "\n",
    "# Final perplexity calculation\n",
    "perplexity = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "finetune_args = TrainingArguments(\n",
    "    output_dir=\"./tmp_finetuned_bert_pruned\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pruned_model_one_shot,\n",
    "    args=finetune_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(\"\\n=== Training Pruned Model ===\")\n",
    "trainer.train()\n",
    "\n",
    "# Test the pruned model\n",
    "pruned_model_one_shot.eval()\n",
    "total_log_likelihood = 0\n",
    "total_tokens = 0\n",
    "\n",
    "test_text = texts[:100]  # Limit to first 100 texts for testing\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(test_text, desc=\"Calculating Perplexity\"):\n",
    "        # Tokenize text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(pruned_model_one_shot.device)\n",
    "        \n",
    "        # Forward pass to get loss\n",
    "        outputs = pruned_model_one_shot(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss.item()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_log_likelihood += loss * inputs[\"input_ids\"].size(1)\n",
    "        total_tokens += inputs[\"input_ids\"].size(1)\n",
    "\n",
    "# Final perplexity calculation\n",
    "perplexity = torch.exp(torch.tensor(total_log_likelihood / total_tokens)).item()\n",
    "print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "sparsity = model_sparsity(pruned_Qwen)\n",
    "print(f'Sparsity of the pruned model after fine-tuning: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840b6534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QWenLMHeadModel(\n",
       "  (transformer): QWenModel(\n",
       "    (wte): Embedding(151936, 4096)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (rotary_emb): RotaryEmbedding()\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (w2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (c_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_for_pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6efd9ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_FFN(model, renormalize=False):\n",
    "    \"\"\"\n",
    "    Prune only the final lm_head linear layer\n",
    "    \"\"\"\n",
    "    pruned_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        # Only prune the lm_head layer\n",
    "        if name == 'lm_head' and isinstance(module, nn.Linear):\n",
    "            try:\n",
    "                # Create mask\n",
    "                mask = get_linear_mask(module).to(module.weight.device)\n",
    "                \n",
    "                # Apply pruning in-place\n",
    "                with torch.no_grad():\n",
    "                    module.weight.data *= mask.float()\n",
    "                    \n",
    "                    if renormalize:\n",
    "                        row_sum = module.weight.data.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                        module.weight.data.div_(row_sum)\n",
    "                    \n",
    "                    pruned_layers.append(name)\n",
    "                    sparsity = (module.weight.data == 0).float().mean().item()\n",
    "                    print(f\"Pruned {name}: {sparsity:.2%} sparsity\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        if name == 'mlp' and isinstance(module, nn.Linear):\n",
    "            try:\n",
    "                # Create mask for MLP layers\n",
    "                mask = get_linear_mask(module).to(module.weight.device)\n",
    "                \n",
    "                # Apply pruning in-place\n",
    "                with torch.no_grad():\n",
    "                    module.weight.data *= mask.float()\n",
    "                    \n",
    "                    if renormalize:\n",
    "                        row_sum = module.weight.data.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "                        module.weight.data.div_(row_sum)\n",
    "                    \n",
    "                    pruned_layers.append(name)\n",
    "                    sparsity = (module.weight.data == 0).float().mean().item()\n",
    "                    print(f\"Pruned {name}: {sparsity:.2%} sparsity\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if len(pruned_layers) == 0:\n",
    "        print(\"Warning: No lm_head layer found to prune!\")\n",
    "        # Let's check what layers are available\n",
    "        print(\"Available layers:\")\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                print(f\"  - {name}: {module.weight.shape}\")\n",
    "    else:\n",
    "        print(f\"Successfully pruned {len(pruned_layers)} layers\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd82710c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning model...\n",
      "Pruned lm_head: 39.97% sparsity\n",
      "Successfully pruned 1 layers\n",
      "Calculating pruned perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 100/100 [01:11<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Perplexity: 18.52\n",
      "Model Sparsity: 0.0322\n"
     ]
    }
   ],
   "source": [
    "print(\"Pruning model...\")\n",
    "pruned_model = prune_model_FFN(model_for_pruning, renormalize=False)\n",
    "\n",
    "# Step 4: Calculate pruned perplexity\n",
    "print(\"Calculating pruned perplexity...\")\n",
    "pruned_perplexity = calculate_perplexity(pruned_model, tokenizer, texts[:100])\n",
    "print(f\"Pruned Perplexity: {pruned_perplexity:.2f}\")\n",
    "\n",
    "# Step 5: Calculate sparsity\n",
    "sparsity = model_sparsity(pruned_model)\n",
    "print(f'Model Sparsity: {sparsity:.4f}')\n",
    "\n",
    "# Step 6: Save the pruned model (optional)\n",
    "torch.save(pruned_model.state_dict(), 'pruned_qwen_FFN.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32371f3b",
   "metadata": {},
   "source": [
    "test only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32365e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4c8f6cc42047f49f8eaadefba2f1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd6676bc8c946b98f5e7ae1ed2cd609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646d0595e7454c7487d3697b4242d4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Evaluating original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1169/3519 [07:35<15:14,  2.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 108\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# === RUN FULL PIPELINE ===\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==> Evaluating original model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m ppl_original \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ppl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[RESULT] Original PPL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl_original\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==> Pruning and correcting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m, in \u001b[0;36mevaluate_ppl\u001b[1;34m(model, input_ids, stride)\u001b[0m\n\u001b[0;32m     42\u001b[0m target_slice \u001b[38;5;241m=\u001b[39m input_slice\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 44\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_slice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_slice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     neg_log_likelihood \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m input_slice\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     46\u001b[0m lls\u001b[38;5;241m.\u001b[39mappend(neg_log_likelihood)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 842\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    843\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    844\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    845\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    846\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    847\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    848\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    849\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    850\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    851\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    852\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    854\u001b[0m )\n\u001b[0;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    584\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    591\u001b[0m         position_embeddings,\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    595\u001b[0m         hidden_states,\n\u001b[0;32m    596\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    597\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    598\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    599\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    600\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    601\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    602\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:351\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[0;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[0;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Use your HF token\n",
    "hf_token = \"your personal token\"\n",
    "\n",
    "# Load tokenizer and 8-bit model (for your 5090 GPU)\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token=hf_token\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ------------------ N_eff pruning (per row) ------------------\n",
    "\n",
    "def ls_recover_weights(layer, mask, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Least-squares recovery: after pruning, project original weight matrix\n",
    "    onto the space of preserved rows to minimize reconstruction error.\n",
    "\n",
    "    Args:\n",
    "        layer: nn.Linear module (with pruned weights).\n",
    "        mask: binary mask (same shape as layer.weight).\n",
    "        tol: tolerance for pseudo-inverse stability.\n",
    "    \"\"\"\n",
    "    W = layer.weight.data.float()\n",
    "    W_orig = W.clone()\n",
    "    M = mask.float()\n",
    "\n",
    "    # Skip if layer is too small\n",
    "    if W.shape[0] < 4 or W.shape[1] < 4:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Reconstruct W using only preserved weights\n",
    "        W_preserved = W * M\n",
    "        # Project W_orig onto the row space of W_preserved\n",
    "        # Equivalent to: W_hat = argmin ||W_hat - W_orig||_F s.t. support(W_hat) = M\n",
    "        # Closed-form: W_hat = M ⊙ (W_orig @ V @ V.T) if W_preserved ≈ U S V.T\n",
    "        U, S, Vh = torch.linalg.svd(W_preserved, full_matrices=False)\n",
    "        S_inv = torch.diag(1.0 / (S + tol))\n",
    "        W_hat = (U @ S_inv @ (U.T @ W_orig))\n",
    "        W_hat = W_hat * M\n",
    "        layer.weight.data.copy_(W_hat.to(layer.weight.dtype))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] SVD failed on layer {layer}, skipping LS correction: {e}\")\n",
    "\n",
    "\n",
    "def get_neff_mask_per_row(weight: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute N_eff and create mask per row.\n",
    "    weight: [out_dim, in_dim]\n",
    "    Returns a boolean mask of same shape.\n",
    "    \"\"\"\n",
    "    x = weight.abs().float()\n",
    "    x_norm = x / x.sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "    neff = torch.floor(1.0 / (x_norm ** 2).sum(dim=1, keepdim=True)).int()\n",
    "\n",
    "    sorted_vals, sorted_idx = torch.sort(x_norm, dim=1, descending=True)\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "\n",
    "    for i in range(x.size(0)):  # row-wise\n",
    "        k = neff[i].item()\n",
    "        mask[i, sorted_idx[i, :k]] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "def prune_linear_layer_per_row(layer: nn.Linear):\n",
    "    if not hasattr(layer, \"weight\"):\n",
    "        return 0, 0\n",
    "    with torch.no_grad():\n",
    "        weight = layer.weight.data.clone()\n",
    "        mask = get_neff_mask_per_row(weight)\n",
    "        layer.weight.data *= mask.to(layer.weight.device)\n",
    "    return mask.sum().item(), mask.numel()\n",
    "\n",
    "def prune_llama_per_row(model):\n",
    "    kept, total = 0, 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(k in name for k in [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"\n",
    "        ]):\n",
    "            k, t = prune_linear_layer_per_row(module)\n",
    "            kept += k\n",
    "            total += t\n",
    "    sparsity = 1 - kept / total\n",
    "    print(f\"[PRUNE] Kept {kept} / {total} weights → Sparsity: {sparsity:.4f}\")\n",
    "    return model\n",
    "\n",
    "# ------------------ Evaluate perplexity ------------------\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, max_length=512):\n",
    "    losses = []\n",
    "    for sample in tqdm(dataset, desc=\"Evaluating PPL\"):\n",
    "        encoded = tokenizer(\n",
    "            sample[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        input_ids = encoded.input_ids\n",
    "\n",
    "        # Manually prepend BOS token if model has one\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            bos = torch.tensor([[tokenizer.bos_token_id]], device=input_ids.device)\n",
    "            input_ids = torch.cat([bos, input_ids.to(input_ids.device)], dim=-1)\n",
    "\n",
    "        if input_ids.shape[-1] < 4:\n",
    "            continue  # skip too-short sequences\n",
    "\n",
    "        input_ids = input_ids.to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss.item()\n",
    "            losses.append(loss)\n",
    "    ppl = torch.exp(torch.tensor(losses).mean())\n",
    "    print(f\"[RESULT] Perplexity: {ppl:.2f}\")\n",
    "    return ppl.item()\n",
    "\n",
    "# ------------------ Run test ------------------\n",
    "\n",
    "# Load small test subset of WikiText2\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:2%]\")\n",
    "\n",
    "# Evaluate original\n",
    "print(\"==> Original model:\")\n",
    "ppl_before = compute_perplexity(model, tokenizer, dataset)\n",
    "\n",
    "# Apply pruning\n",
    "print(\"\\n==> Pruning model (per row)...\")\n",
    "model = prune_llama_per_row(model)\n",
    "\n",
    "# Evaluate pruned model\n",
    "print(\"\\n==> Pruned model:\")\n",
    "ppl_after = compute_perplexity(model, tokenizer, dataset)\n",
    "\n",
    "print(\"\\n==> Summary:\")\n",
    "print(f\"Original PPL: {ppl_before:.2f}\")\n",
    "print(f\"Pruned   PPL: {ppl_after:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5ec86538bf4ef7b09c361e2008c3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Original model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PPL: 100%|██████████| 87/87 [00:05<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Perplexity: 24.23\n",
      "\n",
      "==> Row-pruned model:\n",
      "[PRUNE] Kept 4090322544 / 6476005376 weights → Sparsity: 0.3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating PPL: 100%|██████████| 87/87 [00:05<00:00, 16.30it/s]\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_34340\\4009871709.py:101: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # for mixed precision\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_34340\\4009871709.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RESULT] Perplexity: 59.15\n",
      "\n",
      "==> Row-pruned + retrain:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 157\u001b[0m\n\u001b[0;32m    154\u001b[0m ppl_row \u001b[38;5;241m=\u001b[39m compute_perplexity(model_row, tokenizer, dataset)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==> Row-pruned + retrain:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m model_row \u001b[38;5;241m=\u001b[39m \u001b[43mone_shot_retrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m ppl_row_retrain \u001b[38;5;241m=\u001b[39m compute_perplexity(model_row, tokenizer, dataset)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==> Column-pruned model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 128\u001b[0m, in \u001b[0;36mone_shot_retrain\u001b[1;34m(model, tokenizer, dataset, lr, max_grad_norm)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    127\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 128\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n\u001b[0;32m    130\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\amp\\grad_scaler.py:342\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    339\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    340\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 342\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\amp\\grad_scaler.py:264\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[1;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "\u001b[1;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import bitsandbytes as bnb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# HF access token\n",
    "hf_token = \"your personal token\"\n",
    "\n",
    "# Load LLaMA-2 tokenizer and model\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    token=hf_token\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ------------------ N_eff masks ------------------\n",
    "\n",
    "def get_neff_mask_per_row(weight: torch.Tensor) -> torch.Tensor:\n",
    "    x = weight.abs().float()\n",
    "    x_norm = x / x.sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "    neff = torch.floor(1.0 / (x_norm ** 2).sum(dim=1, keepdim=True)).int()\n",
    "    sorted_vals, sorted_idx = torch.sort(x_norm, dim=1, descending=True)\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    for i in range(x.size(0)):\n",
    "        k = neff[i].item()\n",
    "        mask[i, sorted_idx[i, :k]] = True\n",
    "    return mask\n",
    "\n",
    "def get_neff_mask_per_column(weight: torch.Tensor) -> torch.Tensor:\n",
    "    x = weight.abs().float()\n",
    "    x_norm = x / x.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "    neff = torch.floor(1.0 / (x_norm ** 2).sum(dim=0, keepdim=True)).int()\n",
    "    sorted_vals, sorted_idx = torch.sort(x_norm, dim=0, descending=True)\n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    for j in range(x.size(1)):\n",
    "        k = neff[0, j].item()\n",
    "        mask[sorted_idx[:k, j], j] = True\n",
    "    return mask\n",
    "\n",
    "# ------------------ Pruning ------------------\n",
    "\n",
    "def prune_linear_layer(layer: nn.Linear, mask: torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        layer.weight.data *= mask.to(layer.weight.device)\n",
    "\n",
    "def prune_llama(model, mask_fn):\n",
    "    kept, total = 0, 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and any(k in name for k in [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"\n",
    "        ]):\n",
    "            weight = module.weight.data.clone()\n",
    "            mask = mask_fn(weight)\n",
    "            prune_linear_layer(module, mask)\n",
    "            kept += mask.sum().item()\n",
    "            total += mask.numel()\n",
    "    sparsity = 1 - kept / total\n",
    "    print(f\"[PRUNE] Kept {kept} / {total} weights → Sparsity: {sparsity:.4f}\")\n",
    "    return model\n",
    "\n",
    "# ------------------ Evaluation ------------------\n",
    "\n",
    "def compute_perplexity(model, tokenizer, dataset, max_length=512):\n",
    "    losses = []\n",
    "    for sample in tqdm(dataset, desc=\"Evaluating PPL\"):\n",
    "        encoded = tokenizer(\n",
    "            sample[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        input_ids = encoded.input_ids\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            bos = torch.tensor([[tokenizer.bos_token_id]], device=input_ids.device)\n",
    "            input_ids = torch.cat([bos, input_ids.to(input_ids.device)], dim=-1)\n",
    "        if input_ids.shape[-1] < 4:\n",
    "            continue\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss.item()\n",
    "            losses.append(loss)\n",
    "    ppl = torch.exp(torch.tensor(losses).mean())\n",
    "    print(f\"[RESULT] Perplexity: {ppl:.2f}\")\n",
    "    return ppl.item()\n",
    "\n",
    "# ------------------ One-shot retraining ------------------\n",
    "\n",
    "def one_shot_retrain(model, tokenizer, dataset, lr=5e-5):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    for sample in dataset.select(range(2)):  # 2 samples for one-shot\n",
    "        encoded = tokenizer(\n",
    "            sample[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        input_ids = encoded.input_ids.to(model.device)\n",
    "        if tokenizer.bos_token_id is not None:\n",
    "            bos = torch.tensor([[tokenizer.bos_token_id]], device=input_ids.device)\n",
    "            input_ids = torch.cat([bos, input_ids], dim=-1)\n",
    "\n",
    "        if input_ids.shape[-1] < 4:\n",
    "            continue\n",
    "\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        break  # Only one shot\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------ Main ------------------\n",
    "\n",
    "# Load subset of wikitext-2\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:2%]\")\n",
    "\n",
    "# Clone the original model\n",
    "import copy\n",
    "model_row = copy.deepcopy(model)\n",
    "model_col = copy.deepcopy(model)\n",
    "\n",
    "print(\"==> Original model:\")\n",
    "ppl_orig = compute_perplexity(model, tokenizer, dataset)\n",
    "\n",
    "print(\"\\n==> Row-pruned model:\")\n",
    "prune_llama(model_row, get_neff_mask_per_row)\n",
    "ppl_row = compute_perplexity(model_row, tokenizer, dataset)\n",
    "\n",
    "print(\"\\n==> Row-pruned + retrain:\")\n",
    "model_row = one_shot_retrain(model_row, tokenizer, dataset)\n",
    "ppl_row_retrain = compute_perplexity(model_row, tokenizer, dataset)\n",
    "\n",
    "print(\"\\n==> Column-pruned model:\")\n",
    "prune_llama(model_col, get_neff_mask_per_column)\n",
    "ppl_col = compute_perplexity(model_col, tokenizer, dataset)\n",
    "\n",
    "print(\"\\n==> Column-pruned + retrain:\")\n",
    "model_col = one_shot_retrain(model_col, tokenizer, dataset)\n",
    "ppl_col_retrain = compute_perplexity(model_col, tokenizer, dataset)\n",
    "\n",
    "print(\"\\n==> Summary:\")\n",
    "print(f\"Original            : {ppl_orig:.2f}\")\n",
    "print(f\"Row-pruned          : {ppl_row:.2f}\")\n",
    "print(f\"Row-pruned + retrain: {ppl_row_retrain:.2f}\")\n",
    "print(f\"Column-pruned       : {ppl_col:.2f}\")\n",
    "print(f\"Column-pruned + retrain: {ppl_col_retrain:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfd627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
