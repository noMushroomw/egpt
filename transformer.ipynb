{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6465cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 0.54593 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/williams/anaconda3/envs/egpt/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train acc 33.46%  test acc 42.11%\n",
      "Epoch 02: train acc 46.32%  test acc 49.63%\n",
      "Epoch 03: train acc 52.18%  test acc 51.40%\n",
      "Epoch 04: train acc 55.91%  test acc 58.05%\n",
      "Epoch 05: train acc 58.67%  test acc 60.36%\n",
      "Epoch 06: train acc 60.84%  test acc 62.72%\n",
      "Epoch 07: train acc 62.39%  test acc 63.98%\n",
      "Epoch 08: train acc 64.09%  test acc 66.32%\n",
      "Epoch 09: train acc 65.35%  test acc 66.15%\n",
      "Epoch 10: train acc 66.41%  test acc 65.86%\n",
      "Base model sparsity: 0.0000\n",
      "Layer-wise EMP summary:\n",
      " Linear(id=6060751744)          | out= 384 in= 128 | avg N_eff=86.7 | sparsity= 32.2%\n",
      "Linear(id=6060746848)          | out= 128 in= 128 | avg N_eff=81.6 | sparsity= 36.3%\n",
      "Linear(id=6060751216)          | out= 256 in= 128 | avg N_eff=86.8 | sparsity= 32.2%\n",
      "Linear(id=6060747376)          | out= 128 in= 256 | avg N_eff=165.7 | sparsity= 35.3%\n",
      "Linear(id=6060752320)          | out= 384 in= 128 | avg N_eff=88.9 | sparsity= 30.6%\n",
      "Linear(id=6060750976)          | out= 128 in= 128 | avg N_eff=87.1 | sparsity= 32.0%\n",
      "Linear(id=6124831024)          | out= 256 in= 128 | avg N_eff=88.6 | sparsity= 30.8%\n",
      "Linear(id=6124839568)          | out= 128 in= 256 | avg N_eff=168.3 | sparsity= 34.3%\n",
      "Linear(id=6124834864)          | out= 384 in= 128 | avg N_eff=88.3 | sparsity= 31.0%\n",
      "Linear(id=6124833184)          | out= 128 in= 128 | avg N_eff=87.0 | sparsity= 32.0%\n",
      "Linear(id=6124836688)          | out= 256 in= 128 | avg N_eff=88.0 | sparsity= 31.2%\n",
      "Linear(id=6124833616)          | out= 128 in= 256 | avg N_eff=166.9 | sparsity= 34.8%\n",
      "Linear(id=6124837072)          | out= 384 in= 128 | avg N_eff=87.8 | sparsity= 31.4%\n",
      "Linear(id=6124837744)          | out= 128 in= 128 | avg N_eff=86.2 | sparsity= 32.7%\n",
      "Linear(id=6124837120)          | out= 256 in= 128 | avg N_eff=88.8 | sparsity= 30.6%\n",
      "Linear(id=6124830784)          | out= 128 in= 256 | avg N_eff=163.9 | sparsity= 36.0%\n",
      "Linear(id=6124832512)          | out=  10 in= 128 | avg N_eff=87.7 | sparsity= 31.5%\n",
      "EMP sparsity: 0.3209\n",
      "EMP-pruned ViT — test acc 64.93%  (loss 0.9986)\n"
     ]
    }
   ],
   "source": [
    "# ===== Tiny Vision Transformer (CIFAR-10) + Activation-EMP pruning =====\n",
    "import math, copy, time, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --------- Reuse pruning utils (paste from the first block or import them) ----------\n",
    "\n",
    "@torch.no_grad()\n",
    "def model_sparsity(model: nn.Module) -> float:\n",
    "    total, zeros = 0, 0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.dim() >= 2 and 'weight' in n:\n",
    "            total += p.numel()\n",
    "            zeros += (p == 0).sum().item()\n",
    "    return zeros / max(total, 1)\n",
    "\n",
    "# ---- Step 1: collect E[|x|] (per in-feature) for each Linear via forward hooks\n",
    "@torch.no_grad()\n",
    "def collect_input_magnitudes(model: nn.Module,\n",
    "                             data_loader,\n",
    "                             device,\n",
    "                             num_batches: int = 10):\n",
    "    model.eval()\n",
    "    # list Linear modules in traversal order\n",
    "    linear_list = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
    "    sums = []\n",
    "    counts = []\n",
    "    handles = []\n",
    "\n",
    "    for m in linear_list:\n",
    "        sums.append(torch.zeros(m.in_features, device=device))\n",
    "        counts.append(torch.tensor(0, device=device))\n",
    "\n",
    "    index_of = {id(m): i for i, m in enumerate(linear_list)}\n",
    "\n",
    "    def hook_fn(module, inputs, output):\n",
    "        idx = index_of[id(module)]\n",
    "        x = inputs[0].detach()\n",
    "        # flatten all leading dims except last: [..., in_features]\n",
    "        x2d = x.flatten(0, -2)  # (B*..., in_features)\n",
    "        sums[idx] += x2d.abs().sum(dim=0)\n",
    "        counts[idx] += x2d.shape[0]\n",
    "\n",
    "    for m in linear_list:\n",
    "        handles.append(m.register_forward_hook(hook_fn))\n",
    "\n",
    "    seen = 0\n",
    "    for data, target in data_loader:\n",
    "        data = data.to(device)\n",
    "        _ = model(data)\n",
    "        seen += 1\n",
    "        if seen >= num_batches:\n",
    "            break\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    mags = [s / torch.clamp(c.float(), min=1.0) for s, c in zip(sums, counts)]\n",
    "    # Return in the same order as linear_list\n",
    "    return linear_list, mags\n",
    "\n",
    "# ---- Step 2: build the activation-aware mask from N_eff on |W| * E|x|\n",
    "@torch.no_grad()\n",
    "def get_linear_mask_emp(module: nn.Linear,\n",
    "                        in_mag: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        module: nn.Linear with weight shape [out, in]\n",
    "        in_mag: tensor [in] = E[|x|] for this module's input\n",
    "    Returns:\n",
    "        mask (bool) with same shape as weight\n",
    "        neff_row (long) length = out_features\n",
    "    \"\"\"\n",
    "    W = module.weight.data  # [out, in]\n",
    "    # contributions per input to each neuron:\n",
    "    contrib = W.abs() * in_mag.unsqueeze(0)  # [out, in]\n",
    "    row_sum = contrib.sum(dim=1, keepdim=True).clamp(min=1e-12)\n",
    "    norm = contrib / row_sum                 # \\hat c_ji\n",
    "\n",
    "    neff = torch.floor(1.0 / norm.pow(2).sum(dim=1)).clamp(min=1, max=W.shape[1]).long()  # [out]\n",
    "\n",
    "    # sort each row by importance and keep top neff[j]\n",
    "    _, idx = torch.sort(norm, dim=1, descending=True)\n",
    "    out, in_ = W.shape\n",
    "    ranks = torch.arange(in_, device=W.device).unsqueeze(0).expand(out, in_)\n",
    "    keep_sorted = ranks < neff.unsqueeze(1)   # [out, in] (sorted order)\n",
    "\n",
    "    mask = torch.zeros_like(W, dtype=torch.bool)\n",
    "    mask.scatter_(1, idx, keep_sorted)\n",
    "    return mask, neff\n",
    "\n",
    "# ---- Step 3: prune with EMP (optional L1 row re-normalization)\n",
    "@torch.no_grad()\n",
    "def prune_model_emp_activation(model: nn.Module,\n",
    "                               calib_loader,\n",
    "                               device,\n",
    "                               num_calib_batches: int = 10,\n",
    "                               renormalize: bool = False):\n",
    "    pruned = copy.deepcopy(model).to(device)\n",
    "    linear_list, mags = collect_input_magnitudes(pruned, calib_loader, device, num_batches=num_calib_batches)\n",
    "\n",
    "    layer_neff = {}\n",
    "    for lin, mu in zip(linear_list, mags):\n",
    "        W = lin.weight.data\n",
    "        old_row_l1 = W.abs().sum(dim=1, keepdim=True)\n",
    "        mask, neff = get_linear_mask_emp(lin, mu.to(W.device))\n",
    "        # apply mask\n",
    "        W.mul_(mask)\n",
    "        if renormalize:\n",
    "            new_row_l1 = W.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "            scale = old_row_l1 / new_row_l1\n",
    "            W.mul_(scale)\n",
    "        layer_neff[id(lin)] = {\n",
    "            \"name\": getattr(lin, \"_emp_name\", None),\n",
    "            \"neff_row\": neff.detach().cpu(),\n",
    "            \"avg_neff\": neff.float().mean().item(),\n",
    "            \"in_features\": W.shape[1],\n",
    "            \"out_features\": W.shape[0],\n",
    "            \"layer_sparsity\": float((~mask).sum().item() / mask.numel())\n",
    "        }\n",
    "\n",
    "    return pruned, layer_neff\n",
    "\n",
    "# ---- Helper: pretty summary\n",
    "def summarize_emp(layer_neff_dict):\n",
    "    lines = []\n",
    "    for k, v in layer_neff_dict.items():\n",
    "        name = v.get(\"name\") or f\"Linear(id={k})\"\n",
    "        lines.append(\n",
    "            f\"{name:30s} | out={v['out_features']:4d} in={v['in_features']:4d} \"\n",
    "            f\"| avg N_eff={v['avg_neff']:.1f} | sparsity={v['layer_sparsity']*100:5.1f}%\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---- (Optional) attach names to ease reading\n",
    "def tag_linear_names(model: nn.Module):\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m._emp_name = name\n",
    "\n",
    "\n",
    "# --------- Model: Patch embedding, Encoder block, and ViT head ----------\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):                   # x: [B,3,H,W]\n",
    "        x = self.proj(x)                    # [B,embed,H/P,W/P]\n",
    "        x = x.flatten(2).transpose(1, 2)    # [B, N, embed]\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, p=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):         # x: [B, N, C]\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)     # each: [B, N, H, D]\n",
    "        q = q.transpose(1, 2)            # [B, H, N, D]\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale        # [B,H,N,N]\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = attn @ v                                       # [B,H,N,D]\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)           # [B,N,C]\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=2.0, drop=0.0, attn_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = SelfAttention(dim, num_heads, attn_drop, drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp  = MLP(dim, int(dim * mlp_ratio), p=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class TinyViT(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3,\n",
    "                 num_classes=10, embed_dim=128, depth=4, num_heads=4, mlp_ratio=2.0, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, self.patch.num_patches, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            EncoderBlock(embed_dim, num_heads, mlp_ratio, drop, drop) for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch(x) + self.pos\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=1)         # global average over patches (no class token)\n",
    "        x = self.head(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "# --------- Data: CIFAR-10 (32x32) ----------\n",
    "def get_cifar10_loaders(batch_size=128, num_workers=2):\n",
    "    train_tf = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    test_tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_tf)\n",
    "    test  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_tf)\n",
    "    return (DataLoader(train, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True),\n",
    "            DataLoader(test,  batch_size=256,       shuffle=False, num_workers=num_workers, pin_memory=True))\n",
    "\n",
    "# --------- Train / Test ----------\n",
    "def train_one_epoch(model, loader, opt, device):\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.nll_loss(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss/total, 100*correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = F.nll_loss(out, y, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss/total, 100*correct/total\n",
    "\n",
    "# --------- Run everything ----------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0); random.seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_loader, test_loader = get_cifar10_loaders()\n",
    "    model = TinyViT(embed_dim=128, depth=4, num_heads=4, mlp_ratio=2.0, drop=0.1).to(device)\n",
    "\n",
    "    print(\"Params:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "\n",
    "    # Train a few epochs (tune as desired)\n",
    "    epochs = 10\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, opt, device)\n",
    "        te_loss, te_acc = evaluate(model, test_loader, device)\n",
    "        print(f\"Epoch {ep:02d}: train acc {tr_acc:5.2f}%  test acc {te_acc:5.2f}%\")\n",
    "\n",
    "    base_sparsity = model_sparsity(model)\n",
    "    print(f\"Base model sparsity: {base_sparsity:.4f}\")\n",
    "\n",
    "    # Activation-EMP prune using a small calibration slice of the train set\n",
    "    pruned, layer_neff = prune_model_emp_activation(\n",
    "        model, calib_loader=train_loader, device=device, num_calib_batches=8, renormalize=False\n",
    "    )\n",
    "    print(\"Layer-wise EMP summary:\\n\", summarize_emp(layer_neff))\n",
    "    print(f\"EMP sparsity: {model_sparsity(pruned):.4f}\")\n",
    "\n",
    "    # Evaluate pruned model\n",
    "    te_loss, te_acc = evaluate(pruned, test_loader, device)\n",
    "    print(f\"EMP-pruned ViT — test acc {te_acc:5.2f}%  (loss {te_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6166602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline ViT…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: train acc 14.28%  test acc 23.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: train acc 28.55%  test acc 36.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: train acc 38.99%  test acc 47.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: train acc 45.62%  test acc 52.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: train acc 49.84%  test acc 53.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: train acc 52.63%  test acc 58.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: train acc 54.49%  test acc 59.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: train acc 55.90%  test acc 61.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: train acc 57.65%  test acc 62.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: train acc 58.92%  test acc 63.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: train acc 59.85%  test acc 64.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: train acc 61.29%  test acc 64.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: train acc 62.51%  test acc 66.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: train acc 63.31%  test acc 66.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: train acc 64.43%  test acc 66.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: train acc 64.64%  test acc 67.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: train acc 65.40%  test acc 68.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: train acc 65.76%  test acc 68.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: train acc 66.04%  test acc 69.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: train acc 66.18%  test acc 69.18%\n",
      "Base model sparsity: 0.0000\n",
      "        Baseline ViT — test acc 69.18%  (loss 0.8982)\n",
      "Layer-wise Row-NEFF summary:\n",
      " blocks.0.attn.qkv              | avg N_eff=133.5 | sparsity= 36.4%\n",
      " blocks.0.attn.proj             | avg N_eff=134.1 | sparsity= 36.2%\n",
      " blocks.0.mlp.fc1               | avg N_eff=133.5 | sparsity= 36.4%\n",
      " blocks.0.mlp.fc2               | avg N_eff=534.7 | sparsity= 36.4%\n",
      " blocks.1.attn.qkv              | avg N_eff=133.7 | sparsity= 36.3%\n",
      " blocks.1.attn.proj             | avg N_eff=133.1 | sparsity= 36.6%\n",
      " blocks.1.mlp.fc1               | avg N_eff=133.6 | sparsity= 36.4%\n",
      " blocks.1.mlp.fc2               | avg N_eff=534.3 | sparsity= 36.4%\n",
      " blocks.2.attn.qkv              | avg N_eff=133.8 | sparsity= 36.3%\n",
      " blocks.2.attn.proj             | avg N_eff=133.2 | sparsity= 36.5%\n",
      " blocks.2.mlp.fc1               | avg N_eff=133.4 | sparsity= 36.4%\n",
      " blocks.2.mlp.fc2               | avg N_eff=534.7 | sparsity= 36.4%\n",
      " blocks.3.attn.qkv              | avg N_eff=133.5 | sparsity= 36.4%\n",
      " blocks.3.attn.proj             | avg N_eff=133.7 | sparsity= 36.3%\n",
      " blocks.3.mlp.fc1               | avg N_eff=133.8 | sparsity= 36.3%\n",
      " blocks.3.mlp.fc2               | avg N_eff=534.4 | sparsity= 36.4%\n",
      " blocks.4.attn.qkv              | avg N_eff=133.3 | sparsity= 36.5%\n",
      " blocks.4.attn.proj             | avg N_eff=134.1 | sparsity= 36.3%\n",
      " blocks.4.mlp.fc1               | avg N_eff=133.4 | sparsity= 36.5%\n",
      " blocks.4.mlp.fc2               | avg N_eff=534.4 | sparsity= 36.4%\n",
      " blocks.5.attn.qkv              | avg N_eff=134.3 | sparsity= 36.1%\n",
      " blocks.5.attn.proj             | avg N_eff=134.0 | sparsity= 36.2%\n",
      " blocks.5.mlp.fc1               | avg N_eff=133.6 | sparsity= 36.3%\n",
      " blocks.5.mlp.fc2               | avg N_eff=534.4 | sparsity= 36.4%\n",
      " head                           | avg N_eff=140.0 | sparsity= 34.4%\n",
      "\n",
      "Row-NEFF sparsity: 0.3629\n",
      " Row-NEFF pruned ViT — test acc 69.24%  (loss 0.9021)\n",
      "Layer-wise EMP (activation) summary:\n",
      " blocks.0.attn.qkv              | avg N_eff=157.0 | sparsity= 38.7%\n",
      " blocks.0.attn.proj             | avg N_eff=139.7 | sparsity= 45.4%\n",
      " blocks.0.mlp.fc1               | avg N_eff=158.4 | sparsity= 38.1%\n",
      " blocks.0.mlp.fc2               | avg N_eff=599.5 | sparsity= 41.5%\n",
      " blocks.1.attn.qkv              | avg N_eff=159.4 | sparsity= 37.7%\n",
      " blocks.1.attn.proj             | avg N_eff=152.4 | sparsity= 40.5%\n",
      " blocks.1.mlp.fc1               | avg N_eff=159.4 | sparsity= 37.7%\n",
      " blocks.1.mlp.fc2               | avg N_eff=614.8 | sparsity= 40.0%\n",
      " blocks.2.attn.qkv              | avg N_eff=158.3 | sparsity= 38.2%\n",
      " blocks.2.attn.proj             | avg N_eff=152.1 | sparsity= 40.6%\n",
      " blocks.2.mlp.fc1               | avg N_eff=158.4 | sparsity= 38.1%\n",
      " blocks.2.mlp.fc2               | avg N_eff=602.2 | sparsity= 41.2%\n",
      " blocks.3.attn.qkv              | avg N_eff=157.5 | sparsity= 38.5%\n",
      " blocks.3.attn.proj             | avg N_eff=151.8 | sparsity= 40.7%\n",
      " blocks.3.mlp.fc1               | avg N_eff=158.6 | sparsity= 38.0%\n",
      " blocks.3.mlp.fc2               | avg N_eff=604.3 | sparsity= 41.0%\n",
      " blocks.4.attn.qkv              | avg N_eff=157.1 | sparsity= 38.6%\n",
      " blocks.4.attn.proj             | avg N_eff=153.9 | sparsity= 39.9%\n",
      " blocks.4.mlp.fc1               | avg N_eff=158.1 | sparsity= 38.2%\n",
      " blocks.4.mlp.fc2               | avg N_eff=617.0 | sparsity= 39.7%\n",
      " blocks.5.attn.qkv              | avg N_eff=158.3 | sparsity= 38.2%\n",
      " blocks.5.attn.proj             | avg N_eff=154.6 | sparsity= 39.6%\n",
      " blocks.5.mlp.fc1               | avg N_eff=158.8 | sparsity= 38.0%\n",
      " blocks.5.mlp.fc2               | avg N_eff=606.9 | sparsity= 40.7%\n",
      " head                           | avg N_eff=158.4 | sparsity= 38.1%\n",
      "\n",
      "EMP sparsity: 0.3914\n",
      "      EMP-pruned ViT — test acc 69.22%  (loss 0.9083)\n"
     ]
    }
   ],
   "source": [
    "# EMP vs Row-NEFF pruning on a Vision Transformer (CIFAR-10)\n",
    "# -----------------------------------------------------------\n",
    "# Requirements: torch, torchvision, numpy\n",
    "# Optional: tqdm (for pretty progress bars)\n",
    "\n",
    "import math, os, copy, random, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    TQDM = True\n",
    "except Exception:\n",
    "    TQDM = False\n",
    "\n",
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def set_seed(seed: int = 1337):\n",
    "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def accuracy(logits, targets):\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return (pred == targets).float().mean().item()\n",
    "\n",
    "def count_zeros(t: torch.Tensor) -> int:\n",
    "    return (t == 0).sum().item()\n",
    "\n",
    "def model_sparsity(model: nn.Module) -> float:\n",
    "    total = 0; zeros = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.ndim >= 2 and \"weight\" in n:\n",
    "            total += p.numel()\n",
    "            zeros += count_zeros(p)\n",
    "    return zeros / max(total, 1)\n",
    "\n",
    "# -----------------------------\n",
    "# Data\n",
    "# -----------------------------\n",
    "def get_cifar10(batch_size=128, num_workers=2):\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std  = (0.2470, 0.2435, 0.2616)\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "    train = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "    test  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True,\n",
    "                                               num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=256, shuffle=False,\n",
    "                                              num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# -----------------------------\n",
    "# ViT components\n",
    "# -----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic depth (per sample)\"\"\"\n",
    "    def __init__(self, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training: return x\n",
    "        keep = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        rnd = x.new_empty(shape).bernoulli_(keep)\n",
    "        return x * rnd / keep\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "    def forward(self, x):\n",
    "        # B, C, H, W -> B, N, D\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, drop=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x); x = self.act(x); x = self.drop(x)\n",
    "        x = self.fc2(x); x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)                         # B, N, 3C\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]          # each: B, heads, N, head_dim\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path1 = DropPath(drop_path)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
    "        self.drop_path2 = DropPath(drop_path)\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=32, patch=4, in_chans=3, num_classes=10,\n",
    "                 emb_dim=256, depth=6, num_heads=8, mlp_ratio=4.0, drop=0.1, attn_drop=0.1, drop_path=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch, in_chans, emb_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, emb_dim))\n",
    "        self.pos_drop = nn.Dropout(drop)\n",
    "\n",
    "        dpr = torch.linspace(0, drop_path, steps=depth).tolist()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(emb_dim, num_heads, mlp_ratio, drop, attn_drop, dpr[i])\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "        self.head = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        x = self.norm(x)[:, 0]      # CLS\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# Train / Eval\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class TrainCfg:\n",
    "    epochs: int = 20\n",
    "    lr: float = 3e-4\n",
    "    warmup_epochs: int = 2\n",
    "    weight_decay: float = 0.05\n",
    "    label_smoothing: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "def train_one_epoch(model, loader, opt, device, loss_fn, scheduler=None):\n",
    "    model.train()\n",
    "    total_acc, total = 0.0, 0\n",
    "    iterator = tqdm(loader, leave=False) if TQDM else loader\n",
    "    for x, y in iterator:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        if scheduler is not None and hasattr(scheduler, \"optimizer\"):  # for some schedulers\n",
    "            pass\n",
    "        if math.isfinite(cfg.grad_clip): nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "        total_acc += accuracy(logits.detach(), y) * x.size(0)\n",
    "        total += x.size(0)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    return total_acc / max(total,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, loss_fn=None):\n",
    "    model.eval()\n",
    "    total_acc, total, total_loss = 0.0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        total_acc += accuracy(logits, y) * x.size(0)\n",
    "        if loss_fn is not None:\n",
    "            total_loss += F.cross_entropy(logits, y, reduction=\"sum\").item()\n",
    "        total += x.size(0)\n",
    "    return total_acc / max(total,1), (total_loss / max(total,1) if loss_fn is not None else None)\n",
    "\n",
    "# -----------------------------\n",
    "# NEFF helpers (weights-only)\n",
    "# -----------------------------\n",
    "def per_row_neff_from_weight(W: torch.Tensor) -> torch.Tensor:\n",
    "    # W: [out, in]\n",
    "    absW = W.abs()\n",
    "    denom = absW.sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "    p = absW / denom\n",
    "    neff = torch.floor(1.0 / p.pow(2).sum(dim=1)).clamp(min=1, max=W.size(1))\n",
    "    return neff  # [out]\n",
    "\n",
    "def get_linear_mask_per_row(module: nn.Linear) -> torch.Tensor:\n",
    "    W = module.weight.data\n",
    "    out, in_ = W.shape\n",
    "    absW = W.abs()\n",
    "    p = absW / absW.sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "    neff = torch.floor(1.0 / (p.pow(2).sum(dim=1))).clamp_(min=1, max=in_).long()\n",
    "    contrib = absW  # weight-only\n",
    "    _, idx = torch.sort(contrib, dim=1, descending=True)\n",
    "    ranks = torch.arange(in_, device=W.device).unsqueeze(0).expand_as(idx)\n",
    "    mask_sorted = ranks < neff.unsqueeze(1)\n",
    "    mask = torch.zeros_like(W, dtype=torch.bool)\n",
    "    mask.scatter_(1, idx, mask_sorted)\n",
    "    return mask\n",
    "\n",
    "def prune_model_neff_per_row(model: nn.Module, renorm=False) -> Tuple[nn.Module, List[Tuple[str,float,float]]]:\n",
    "    pruned = copy.deepcopy(model)\n",
    "    layer_info = []\n",
    "    with torch.no_grad():\n",
    "        for name, m in pruned.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                mask = get_linear_mask_per_row(m).to(m.weight.device)\n",
    "                m.weight.mul_(mask)\n",
    "                if renorm:\n",
    "                    s = m.weight.abs().sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "                    m.weight.div_(s)  # optional\n",
    "                neff = per_row_neff_from_weight(m.weight).float().mean().item()\n",
    "                layer_spars = count_zeros(m.weight) / m.weight.numel()\n",
    "                layer_info.append((f\"{name}\", neff, layer_spars))\n",
    "    return pruned, layer_info\n",
    "\n",
    "# -----------------------------\n",
    "# Activation-aware EMP (W*x)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def collect_activation_means(model: nn.Module, loader, device, num_batches=8) -> Dict[nn.Linear, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    For each Linear layer, estimate E[|x|] per input channel (last dim).\n",
    "    \"\"\"\n",
    "    stats_sum: Dict[nn.Linear, torch.Tensor] = {}\n",
    "    stats_count: Dict[nn.Linear, int] = {}\n",
    "    handles = []\n",
    "\n",
    "    def register(m: nn.Linear):\n",
    "        stats_sum[m] = torch.zeros(m.in_features, device=device)\n",
    "        stats_count[m] = 0\n",
    "        def hook(mod, inp, out):\n",
    "            x = inp[0].detach()\n",
    "            # reduce over all dims except the last (feature dim)\n",
    "            red_dims = tuple(range(x.dim() - 1))\n",
    "            s = x.abs().sum(dim=red_dims)\n",
    "            stats_sum[mod] += s\n",
    "            stats_count[mod] += (x.numel() // mod.in_features)\n",
    "        return m.register_forward_hook(hook)\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            handles.append(register(m))\n",
    "\n",
    "    model.eval()\n",
    "    it = iter(loader)\n",
    "    for b in range(num_batches):\n",
    "        try:\n",
    "            x, _ = next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        model(x)\n",
    "\n",
    "    for h in handles: h.remove()\n",
    "    means = {m: (stats_sum[m] / max(stats_count[m],1)).clamp_min(1e-12) for m in stats_sum.keys()}\n",
    "    return means\n",
    "\n",
    "def prune_model_emp_activation(model: nn.Module, calib_loader, device, num_calib_batches=8, renorm=False):\n",
    "    \"\"\"\n",
    "    Activation-aware EMP pruning: keep, in each row, the top N_eff elements using\n",
    "    p_ij ∝ |w_ij| * E[|x_j|]\n",
    "    \"\"\"\n",
    "    pruned = copy.deepcopy(model).to(device)\n",
    "    act_means = collect_activation_means(pruned, calib_loader, device, num_batches=num_calib_batches)\n",
    "\n",
    "    layer_info = []\n",
    "    with torch.no_grad():\n",
    "        for name, m in pruned.named_modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                W = m.weight.data\n",
    "                out, in_ = W.shape\n",
    "                mean_abs_x = act_means[m]  # [in]\n",
    "                contrib = W.abs() * mean_abs_x.unsqueeze(0)  # [out, in]\n",
    "                denom = contrib.sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "                p = contrib / denom\n",
    "                neff = torch.floor(1.0 / p.pow(2).sum(dim=1)).clamp_(min=1, max=in_).long()\n",
    "\n",
    "                # build mask row-wise\n",
    "                _, idx = torch.sort(contrib, dim=1, descending=True)\n",
    "                ranks = torch.arange(in_, device=W.device).unsqueeze(0).expand_as(idx)\n",
    "                mask_sorted = ranks < neff.unsqueeze(1)\n",
    "                mask = torch.zeros_like(W, dtype=torch.bool)\n",
    "                mask.scatter_(1, idx, mask_sorted)\n",
    "                m.weight.mul_(mask)\n",
    "                if renorm:\n",
    "                    s = m.weight.abs().sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "                    m.weight.div_(s)\n",
    "\n",
    "                # report\n",
    "                avg_neff = neff.float().mean().item()\n",
    "                layer_spars = count_zeros(m.weight) / m.weight.numel()\n",
    "                layer_info.append((f\"{name}\", avg_neff, layer_spars))\n",
    "\n",
    "    return pruned, layer_info\n",
    "\n",
    "# -----------------------------\n",
    "# Reporting helpers\n",
    "# -----------------------------\n",
    "def print_layerwise_report(tag: str, layer_info: List[Tuple[str,float,float]]):\n",
    "    print(f\"Layer-wise {tag} summary:\")\n",
    "    for (name, neff_avg, spars) in layer_info:\n",
    "        # try to parse layer shape for nicer printing if possible\n",
    "        print(f\" {name:<30s} | avg N_eff={neff_avg:5.1f} | sparsity={100*spars:5.1f}%\")\n",
    "    print()\n",
    "\n",
    "def eval_and_report(model, test_loader, device, tag=\"Model\"):\n",
    "    acc, loss = evaluate(model, test_loader, device, loss_fn=True)\n",
    "    print(f\"{tag:>20s} — test acc {acc*100:5.2f}%  (loss {loss:.4f})\")\n",
    "    return acc, loss\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(123)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader, test_loader = get_cifar10(batch_size=128, num_workers=2)\n",
    "\n",
    "    # Build a stronger baseline than a tiny ViT, but still lightweight\n",
    "    model = ViT(\n",
    "        img_size=32, patch=4, emb_dim=256, depth=6, num_heads=8,\n",
    "        mlp_ratio=4.0, drop=0.1, attn_drop=0.1, drop_path=0.1, num_classes=10\n",
    "    ).to(device)\n",
    "\n",
    "    cfg = TrainCfg(epochs=20, lr=3e-4, warmup_epochs=2, weight_decay=0.05, label_smoothing=0.1, grad_clip=1.0)\n",
    "\n",
    "    # Optimizer + cosine schedule with warmup\n",
    "    opt = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9, 0.999))\n",
    "    sched = CosineAnnealingLR(opt, T_max=cfg.epochs - cfg.warmup_epochs)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)\n",
    "\n",
    "    # Warmup loop\n",
    "    warmup_steps = cfg.warmup_epochs * len(train_loader)\n",
    "    if warmup_steps > 0:\n",
    "        warmup_factor = cfg.lr / warmup_steps\n",
    "        cur_lr = 0.0\n",
    "\n",
    "    print(\"Training baseline ViT…\")\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        if epoch <= cfg.warmup_epochs:\n",
    "            # linear warmup\n",
    "            for pgroup in opt.param_groups:\n",
    "                pgroup[\"lr\"] = min(cfg.lr, (epoch-1)*len(train_loader)*warmup_factor + warmup_factor*1)\n",
    "        else:\n",
    "            # cosine decay after warmup\n",
    "            pass\n",
    "\n",
    "        train_acc = train_one_epoch(model, train_loader, opt, device, loss_fn,\n",
    "                                    scheduler=None if epoch <= cfg.warmup_epochs else sched)\n",
    "        test_acc, _ = evaluate(model, test_loader, device, loss_fn=True)\n",
    "        print(f\"Epoch {epoch:02d}: train acc {train_acc*100:5.2f}%  test acc {test_acc*100:5.2f}%\")\n",
    "\n",
    "    print(f\"Base model sparsity: {model_sparsity(model):.4f}\")\n",
    "    base_acc, base_loss = eval_and_report(model, test_loader, device, tag=\"Baseline ViT\")\n",
    "\n",
    "    # ---------------- Row-NEFF pruning ----------------\n",
    "    row_model, row_info = prune_model_neff_per_row(model, renorm=False)\n",
    "    print_layerwise_report(\"Row-NEFF\", row_info)\n",
    "    print(f\"Row-NEFF sparsity: {model_sparsity(row_model):.4f}\")\n",
    "    eval_and_report(row_model, test_loader, device, tag=\"Row-NEFF pruned ViT\")\n",
    "\n",
    "    # --------------- Activation EMP (W*x) -------------\n",
    "    # Use a small calibration subset (first few batches of training loader)\n",
    "    emp_model, emp_info = prune_model_emp_activation(model, calib_loader=train_loader,\n",
    "                                                     device=device, num_calib_batches=8, renorm=False)\n",
    "    print_layerwise_report(\"EMP (activation)\", emp_info)\n",
    "    print(f\"EMP sparsity: {model_sparsity(emp_model):.4f}\")\n",
    "    eval_and_report(emp_model, test_loader, device, tag=\"EMP-pruned ViT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2668f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NEFF PRUNING: Complete Evaluation Suite\n",
      "For ICLR 2026 Submission\n",
      "============================================================\n",
      "\n",
      "[TEST 1: Language Model Perplexity]\n",
      "\n",
      "==================================================\n",
      "LLM Perplexity Evaluation: facebook/opt-125m\n",
      "==================================================\n",
      "\n",
      "Loading model...\n",
      "\n",
      "1. Original Model\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity:  91%|█████████ | 10/11 [00:00<00:00, 70.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 28.11\n",
      "Parameters: 125,239,296\n",
      "\n",
      "2. NEFF Weight-only Pruning\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing perplexity:  91%|█████████ | 10/11 [00:00<00:00, 68.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 55.25 (Δ: +27.14)\n",
      "Sparsity: 40.18%\n",
      "\n",
      "3. EMP Activation-aware Pruning\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 32/32 [00:00<00:00, 165.00it/s]\n",
      "Computing perplexity:  91%|█████████ | 10/11 [00:00<00:00, 68.89it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 55.25 (Δ: +27.14)\n",
      "Sparsity: 40.18%\n",
      "\n",
      "==================================================\n",
      "SUMMARY - LLM Perplexity Comparison\n",
      "==================================================\n",
      "Method               Perplexity      Sparsity       \n",
      "--------------------------------------------------\n",
      "Original             28.11           0.00%          \n",
      "NEFF (weight)        55.25           40.18%         \n",
      "EMP (activation)     55.25           40.18%         \n",
      "\n",
      "[TEST 2: BERT Classification Tasks]\n",
      "\n",
      "==================================================\n",
      "BERT Experiments on SST2\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb8fbc83a2d4ce09f205144c7077b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a565cbad284301af127a4bf27edf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Evaluating Original BERT Model\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 28/28 [00:00<00:00, 30.05it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Computing MLM perplexity: 100%|██████████| 100/100 [00:00<00:00, 266.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4908\n",
      "Loss: 0.7275\n",
      "MLM Perplexity: 15.50\n",
      "Parameters: 109,483,778\n",
      "\n",
      "2. NEFF Weight-only Pruning\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 28/28 [00:00<00:00, 29.64it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Computing MLM perplexity: 100%|██████████| 100/100 [00:00<00:00, 275.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4908 (Δ: 0.00%)\n",
      "Loss: 0.7263\n",
      "MLM Perplexity: 20.07 (Δ: 4.57)\n",
      "Overall Sparsity: 37.35%\n",
      "\n",
      "3. EMP Activation-aware Pruning\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 16/16 [00:00<00:00, 93.25it/s]\n",
      "Evaluating: 100%|██████████| 28/28 [00:00<00:00, 29.97it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Computing MLM perplexity: 100%|██████████| 100/100 [00:00<00:00, 292.62it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4908 (Δ: 0.00%)\n",
      "Loss: 0.7263\n",
      "MLM Perplexity: 14.23 (Δ: -1.28)\n",
      "Overall Sparsity: 37.35%\n",
      "\n",
      "==================================================\n",
      "SUMMARY COMPARISON\n",
      "==================================================\n",
      "Method               Accuracy     MLM PPL      Sparsity    \n",
      "--------------------------------------------------------\n",
      "Original             0.4908       15.50       0.00%\n",
      "NEFF (weight)        0.4908       20.07      37.35%\n",
      "EMP (activation)     0.4908       14.23      37.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a618cd8a60df47278bfcb1def1548226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mrpc/train-00000-of-00001.parquet:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736d9e06664040a6bc84e91c4e8d0b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mrpc/validation-00000-of-00001.parquet:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621ca76746e2406aa77166e861d06145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mrpc/test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8466c3c179848a4be849a35055b79b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7db1e2f2284c119fdea0b1e9a3d3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad7aa78b2b74b6a8f9ee02047d592cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "BERT Experiments on MRPC\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672a138dd502446499b67a7ccc68e8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb97cd8c3dbc48bd97c83c7c90444956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Evaluating Original BERT Model\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:00<00:00, 28.32it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Computing MLM perplexity: 100%|██████████| 100/100 [00:00<00:00, 288.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6838\n",
      "Loss: 0.6257\n",
      "MLM Perplexity: 18.34\n",
      "Parameters: 109,483,778\n",
      "\n",
      "2. NEFF Weight-only Pruning\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:00<00:00, 29.49it/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Computing MLM perplexity: 100%|██████████| 100/100 [00:00<00:00, 259.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6838 (Δ: 0.00%)\n",
      "Loss: 0.6228\n",
      "MLM Perplexity: 18.44 (Δ: 0.10)\n",
      "Overall Sparsity: 37.35%\n",
      "\n",
      "3. EMP Activation-aware Pruning\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 16/16 [00:00<00:00, 93.49it/s]\n",
      "Evaluating: 100%|██████████| 13/13 [00:00<00:00, 30.21it/s]\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Computing MLM perplexity: 100%|██████████| 100/100 [00:00<00:00, 276.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6838 (Δ: 0.00%)\n",
      "Loss: 0.6228\n",
      "MLM Perplexity: 20.32 (Δ: 1.98)\n",
      "Overall Sparsity: 37.35%\n",
      "\n",
      "==================================================\n",
      "SUMMARY COMPARISON\n",
      "==================================================\n",
      "Method               Accuracy     MLM PPL      Sparsity    \n",
      "--------------------------------------------------------\n",
      "Original             0.6838       18.34       0.00%\n",
      "NEFF (weight)        0.6838       18.44      37.35%\n",
      "EMP (activation)     0.6838       20.32      37.35%\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "LLM Perplexity Results:\n",
      "  NEFF achieves 40.2% sparsity with 96.5% perplexity increase\n",
      "  EMP achieves 40.2% sparsity with 96.5% perplexity increase\n",
      "\n",
      "BERT Results Summary:\n",
      "\n",
      "  SST2:\n",
      "    NEFF: 37.3% sparsity, 0.00% accuracy change\n",
      "    EMP:  37.3% sparsity, 0.00% accuracy change\n",
      "\n",
      "  MRPC:\n",
      "    NEFF: 37.3% sparsity, 0.00% accuracy change\n",
      "    EMP:  37.3% sparsity, 0.00% accuracy change\n",
      "\n",
      "============================================================\n",
      "Ready for ICLR 2026! 🚀\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NEFF Pruning for Language Models: Complete Implementation\n",
    "Includes perplexity evaluation for LLMs and full BERT experiments\n",
    "For ICLR 2026 submission\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    BertForSequenceClassification, BertTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================\n",
    "# Core NEFF Pruning Functions\n",
    "# =====================================\n",
    "\n",
    "def compute_neff(weights: torch.Tensor, dim: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute N_eff = floor(1 / sum(p_i^2)) where p_i = |w_i| / sum(|w_j|)\n",
    "    \n",
    "    Mathematical foundation:\n",
    "    - Effective number of parameters based on weight distribution entropy\n",
    "    - Captures the \"effective dimensionality\" of each neuron\n",
    "    \"\"\"\n",
    "    abs_weights = weights.abs()\n",
    "    weight_sum = abs_weights.sum(dim=dim, keepdim=True).clamp_min(1e-12)\n",
    "    p = abs_weights / weight_sum\n",
    "    neff = torch.floor(1.0 / (p.pow(2).sum(dim=dim))).clamp(min=1, max=weights.shape[dim])\n",
    "    return neff\n",
    "\n",
    "def get_neff_mask_per_row(weight: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Generate pruning mask based on per-row NEFF\"\"\"\n",
    "    out_dim, in_dim = weight.shape\n",
    "    abs_weight = weight.abs()\n",
    "    \n",
    "    # Compute NEFF for each row\n",
    "    neff = compute_neff(weight, dim=1).long()\n",
    "    \n",
    "    # Sort by magnitude and keep top NEFF elements per row\n",
    "    _, indices = torch.sort(abs_weight, dim=1, descending=True)\n",
    "    ranks = torch.arange(in_dim, device=weight.device).unsqueeze(0).expand_as(indices)\n",
    "    mask_sorted = ranks < neff.unsqueeze(1)\n",
    "    \n",
    "    # Scatter back to original positions\n",
    "    mask = torch.zeros_like(weight, dtype=torch.bool)\n",
    "    mask.scatter_(1, indices, mask_sorted)\n",
    "    return mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_activation_statistics(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    num_batches: int = 32,\n",
    "    model_type: str = \"causal_lm\"\n",
    ") -> Dict[nn.Linear, torch.Tensor]:\n",
    "    \"\"\"Collect activation statistics for EMP pruning\"\"\"\n",
    "    stats_sum = {}\n",
    "    stats_count = {}\n",
    "    handles = []\n",
    "    \n",
    "    def register_hook(module: nn.Linear):\n",
    "        stats_sum[module] = torch.zeros(module.in_features, device=next(module.parameters()).device)\n",
    "        stats_count[module] = 0\n",
    "        \n",
    "        def hook(mod, inp, out):\n",
    "            x = inp[0].detach()\n",
    "            if x.dim() == 3:  # [batch, seq, features]\n",
    "                x = x.reshape(-1, x.size(-1))\n",
    "            elif x.dim() == 2:  # [batch, features]\n",
    "                pass\n",
    "            else:\n",
    "                return\n",
    "            \n",
    "            stats_sum[mod] += x.abs().sum(dim=0)\n",
    "            stats_count[mod] += x.size(0)\n",
    "        \n",
    "        return module.register_forward_hook(hook)\n",
    "    \n",
    "    # Register hooks for all Linear layers\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and 'lm_head' not in name:\n",
    "            handles.append(register_hook(module))\n",
    "    \n",
    "    # Run calibration\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Collecting activations\", total=min(num_batches, len(dataloader)))):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        if model_type == \"causal_lm\":\n",
    "            inputs = batch['input_ids'].to(next(model.parameters()).device)\n",
    "            with torch.no_grad():\n",
    "                model(inputs)\n",
    "        elif model_type == \"bert_classification\":\n",
    "            inputs = {k: v.to(next(model.parameters()).device) \n",
    "                     for k, v in batch.items() if k in ['input_ids', 'attention_mask']}\n",
    "            with torch.no_grad():\n",
    "                model(**inputs)\n",
    "    \n",
    "    # Clean up hooks\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    # Compute means\n",
    "    activation_means = {\n",
    "        module: (stats_sum[module] / max(stats_count[module], 1)).clamp_min(1e-12)\n",
    "        for module in stats_sum\n",
    "    }\n",
    "    \n",
    "    return activation_means\n",
    "\n",
    "def prune_model_neff(\n",
    "    model: nn.Module,\n",
    "    method: str = \"weight_only\",\n",
    "    activation_stats: Optional[Dict] = None,\n",
    "    keep_modules: List[str] = ['lm_head', 'classifier', 'embeddings']\n",
    ") -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"\n",
    "    Apply NEFF pruning to model\n",
    "    \n",
    "    Args:\n",
    "        model: Model to prune\n",
    "        method: \"weight_only\" for standard NEFF, \"emp\" for activation-aware\n",
    "        activation_stats: Required for EMP method\n",
    "        keep_modules: Module names to skip pruning\n",
    "    \"\"\"\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    pruning_info = {\n",
    "        'layer_sparsity': {},\n",
    "        'layer_neff': {},\n",
    "        'total_params': 0,\n",
    "        'pruned_params': 0\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for name, module in pruned_model.named_modules():\n",
    "            # Skip non-Linear and protected modules\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "            if any(keep_name in name for keep_name in keep_modules):\n",
    "                continue\n",
    "            \n",
    "            weight = module.weight.data\n",
    "            original_weight = weight.clone()\n",
    "            \n",
    "            if method == \"weight_only\":\n",
    "                mask = get_neff_mask_per_row(weight)\n",
    "            elif method == \"emp\" and activation_stats is not None:\n",
    "                # Activation-aware EMP\n",
    "                if module in activation_stats:\n",
    "                    act_mean = activation_stats[module]\n",
    "                    effective_weight = weight.abs() * act_mean.unsqueeze(0)\n",
    "                    \n",
    "                    # Compute NEFF with activation weighting\n",
    "                    weight_sum = effective_weight.sum(dim=1, keepdim=True).clamp_min(1e-12)\n",
    "                    p = effective_weight / weight_sum\n",
    "                    neff = torch.floor(1.0 / (p.pow(2).sum(dim=1))).clamp(min=1, max=weight.size(1)).long()\n",
    "                    \n",
    "                    # Create mask based on effective contributions\n",
    "                    _, indices = torch.sort(effective_weight, dim=1, descending=True)\n",
    "                    ranks = torch.arange(weight.size(1), device=weight.device).unsqueeze(0).expand_as(indices)\n",
    "                    mask_sorted = ranks < neff.unsqueeze(1)\n",
    "                    mask = torch.zeros_like(weight, dtype=torch.bool)\n",
    "                    mask.scatter_(1, indices, mask_sorted)\n",
    "                else:\n",
    "                    mask = get_neff_mask_per_row(weight)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "            \n",
    "            # Apply mask\n",
    "            module.weight.data = weight * mask.float()\n",
    "            \n",
    "            # Calculate statistics\n",
    "            num_zeros = (module.weight.data == 0).sum().item()\n",
    "            num_params = weight.numel()\n",
    "            sparsity = num_zeros / num_params\n",
    "            avg_neff = compute_neff(module.weight.data, dim=1).float().mean().item()\n",
    "            \n",
    "            pruning_info['layer_sparsity'][name] = sparsity\n",
    "            pruning_info['layer_neff'][name] = avg_neff\n",
    "            pruning_info['total_params'] += num_params\n",
    "            pruning_info['pruned_params'] += num_zeros\n",
    "    \n",
    "    pruning_info['overall_sparsity'] = pruning_info['pruned_params'] / pruning_info['total_params']\n",
    "    \n",
    "    return pruned_model, pruning_info\n",
    "\n",
    "# =====================================\n",
    "# Perplexity Evaluation for LLMs\n",
    "# =====================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_perplexity(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    dataset_name: str = \"wikitext\",\n",
    "    dataset_config: str = \"wikitext-2-raw-v1\",\n",
    "    split: str = \"test\",\n",
    "    max_length: int = 1024,\n",
    "    stride: int = 512,\n",
    "    batch_size: int = 1,\n",
    "    max_samples: Optional[int] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate perplexity on a dataset\n",
    "    \n",
    "    Perplexity = exp(average_negative_log_likelihood)\n",
    "    Lower is better!\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, dataset_config, split=split)\n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    \n",
    "    encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Sliding window approach\n",
    "    for i in tqdm(range(0, encodings.input_ids.size(1), stride), desc=\"Computing perplexity\"):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i\n",
    "        \n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100  # Mask non-target tokens\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            loss = outputs.loss * trg_len\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        total_tokens += trg_len\n",
    "        \n",
    "        if end_loc == encodings.input_ids.size(1):\n",
    "            break\n",
    "    \n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    return perplexity\n",
    "\n",
    "# =====================================\n",
    "# BERT Experiments\n",
    "# =====================================\n",
    "\n",
    "class BertExperiments:\n",
    "    def __init__(self, model_name: str = \"bert-base-uncased\", task: str = \"sst2\"):\n",
    "        self.model_name = model_name\n",
    "        self.task = task\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        if task in [\"sst2\", \"mrpc\", \"cola\", \"qqp\"]:\n",
    "            self.model = BertForSequenceClassification.from_pretrained(\n",
    "                model_name, num_labels=2\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported task: {task}\")\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load dataset\n",
    "        self.dataset = self._load_dataset()\n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        \"\"\"Load GLUE dataset\"\"\"\n",
    "        task_to_dataset = {\n",
    "            \"sst2\": (\"glue\", \"sst2\"),\n",
    "            \"mrpc\": (\"glue\", \"mrpc\"),\n",
    "            \"cola\": (\"glue\", \"cola\"),\n",
    "            \"qqp\": (\"glue\", \"qqp\")\n",
    "        }\n",
    "        \n",
    "        dataset_name, config = task_to_dataset[self.task]\n",
    "        dataset = load_dataset(dataset_name, config)\n",
    "        return dataset\n",
    "    \n",
    "    def preprocess_function(self, examples):\n",
    "        \"\"\"Tokenize dataset\"\"\"\n",
    "        if self.task == \"sst2\":\n",
    "            return self.tokenizer(\n",
    "                examples[\"sentence\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            )\n",
    "        elif self.task in [\"mrpc\", \"qqp\"]:\n",
    "            return self.tokenizer(\n",
    "                examples[\"sentence1\" if self.task == \"mrpc\" else \"question1\"],\n",
    "                examples[\"sentence2\" if self.task == \"mrpc\" else \"question2\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            )\n",
    "        elif self.task == \"cola\":\n",
    "            return self.tokenizer(\n",
    "                examples[\"sentence\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            )\n",
    "    \n",
    "    def create_dataloader(self, split: str = \"validation\", batch_size: int = 32):\n",
    "        \"\"\"Create DataLoader for evaluation\"\"\"\n",
    "        dataset = self.dataset[split]\n",
    "        \n",
    "        # Tokenize\n",
    "        dataset = dataset.map(self.preprocess_function, batched=True)\n",
    "        dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "        \n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate_accuracy(self, model: nn.Module, dataloader: DataLoader) -> Dict:\n",
    "        \"\"\"Evaluate model accuracy\"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(self.device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(self.device),\n",
    "                \"labels\": batch[\"label\"].to(self.device)\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct += (predictions == batch[\"label\"].to(self.device)).sum().item()\n",
    "            total += len(batch[\"label\"])\n",
    "            total_loss += loss.item() * len(batch[\"label\"])\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        avg_loss = total_loss / total\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"loss\": avg_loss,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate_mlm_perplexity(self, model: nn.Module, max_samples: int = 1000) -> float:\n",
    "        \"\"\"Evaluate masked language model perplexity for BERT\"\"\"\n",
    "        # Use BERT for MLM\n",
    "        mlm_model = AutoModelForMaskedLM.from_pretrained(self.model_name).to(self.device)\n",
    "        \n",
    "        # Copy pruned weights to MLM model (excluding pooler which MLM doesn't have)\n",
    "        if hasattr(model, 'bert'):\n",
    "            source_state = model.bert.state_dict()\n",
    "            target_state = mlm_model.bert.state_dict()\n",
    "            \n",
    "            # Only copy weights that exist in both models\n",
    "            filtered_state = {k: v for k, v in source_state.items() \n",
    "                            if k in target_state and 'pooler' not in k}\n",
    "            \n",
    "            mlm_model.bert.load_state_dict(filtered_state, strict=False)\n",
    "        \n",
    "        mlm_model.eval()\n",
    "        \n",
    "        # Load WikiText for perplexity\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        if max_samples:\n",
    "            dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for idx, text in enumerate(tqdm(dataset[\"text\"], desc=\"Computing MLM perplexity\", total=min(max_samples, len(dataset[\"text\"])))):\n",
    "            if idx >= max_samples:\n",
    "                break\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Create masked inputs (15% masking)\n",
    "            input_ids = inputs.input_ids.clone()\n",
    "            labels = inputs.input_ids.clone()\n",
    "            \n",
    "            # Random masking\n",
    "            rand = torch.rand(input_ids.shape).to(self.device)\n",
    "            mask_arr = (rand < 0.15) * (input_ids != self.tokenizer.pad_token_id)\n",
    "            \n",
    "            for i in range(input_ids.shape[0]):\n",
    "                selection = torch.where(mask_arr[i])[0]\n",
    "                if len(selection) > 0:\n",
    "                    input_ids[i, selection] = self.tokenizer.mask_token_id\n",
    "                    # Only compute loss on masked tokens\n",
    "                    labels[i, ~mask_arr[i]] = -100\n",
    "            \n",
    "            outputs = mlm_model(input_ids=input_ids, labels=labels)\n",
    "            \n",
    "            if outputs.loss is not None:\n",
    "                total_loss += outputs.loss.item() * mask_arr.sum().item()\n",
    "                total_predictions += mask_arr.sum().item()\n",
    "        \n",
    "        if total_predictions > 0:\n",
    "            perplexity = math.exp(total_loss / total_predictions)\n",
    "        else:\n",
    "            perplexity = float('inf')\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    def run_full_evaluation(self, calibration_samples: int = 128):\n",
    "        \"\"\"Run complete evaluation pipeline\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"BERT Experiments on {self.task.upper()}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        # Create evaluation dataloader\n",
    "        eval_dataloader = self.create_dataloader(\"validation\", batch_size=32)\n",
    "        calib_dataloader = self.create_dataloader(\"train\", batch_size=8)\n",
    "        \n",
    "        # 1. Evaluate original model\n",
    "        print(\"1. Evaluating Original BERT Model\")\n",
    "        print(\"-\" * 30)\n",
    "        orig_results = self.evaluate_accuracy(self.model, eval_dataloader)\n",
    "        orig_perplexity = self.evaluate_mlm_perplexity(self.model, max_samples=100)\n",
    "        \n",
    "        print(f\"Accuracy: {orig_results['accuracy']:.4f}\")\n",
    "        print(f\"Loss: {orig_results['loss']:.4f}\")\n",
    "        print(f\"MLM Perplexity: {orig_perplexity:.2f}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        \n",
    "        # 2. NEFF Weight-only Pruning\n",
    "        print(\"\\n2. NEFF Weight-only Pruning\")\n",
    "        print(\"-\" * 30)\n",
    "        neff_model, neff_info = prune_model_neff(\n",
    "            self.model,\n",
    "            method=\"weight_only\",\n",
    "            keep_modules=['embeddings', 'classifier', 'pooler']\n",
    "        )\n",
    "        \n",
    "        neff_results = self.evaluate_accuracy(neff_model, eval_dataloader)\n",
    "        neff_perplexity = self.evaluate_mlm_perplexity(neff_model, max_samples=100)\n",
    "        \n",
    "        print(f\"Accuracy: {neff_results['accuracy']:.4f} (Δ: {(neff_results['accuracy'] - orig_results['accuracy'])*100:.2f}%)\")\n",
    "        print(f\"Loss: {neff_results['loss']:.4f}\")\n",
    "        print(f\"MLM Perplexity: {neff_perplexity:.2f} (Δ: {neff_perplexity - orig_perplexity:.2f})\")\n",
    "        print(f\"Overall Sparsity: {neff_info['overall_sparsity']:.2%}\")\n",
    "        \n",
    "        # 3. EMP Activation-aware Pruning\n",
    "        print(\"\\n3. EMP Activation-aware Pruning\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Collect activation statistics\n",
    "        activation_stats = collect_activation_statistics(\n",
    "            self.model,\n",
    "            calib_dataloader,\n",
    "            num_batches=calibration_samples // 8,\n",
    "            model_type=\"bert_classification\"\n",
    "        )\n",
    "        \n",
    "        emp_model, emp_info = prune_model_neff(\n",
    "            self.model,\n",
    "            method=\"emp\",\n",
    "            activation_stats=activation_stats,\n",
    "            keep_modules=['embeddings', 'classifier', 'pooler']\n",
    "        )\n",
    "        \n",
    "        emp_results = self.evaluate_accuracy(emp_model, eval_dataloader)\n",
    "        emp_perplexity = self.evaluate_mlm_perplexity(emp_model, max_samples=100)\n",
    "        \n",
    "        print(f\"Accuracy: {emp_results['accuracy']:.4f} (Δ: {(emp_results['accuracy'] - orig_results['accuracy'])*100:.2f}%)\")\n",
    "        print(f\"Loss: {emp_results['loss']:.4f}\")\n",
    "        print(f\"MLM Perplexity: {emp_perplexity:.2f} (Δ: {emp_perplexity - orig_perplexity:.2f})\")\n",
    "        print(f\"Overall Sparsity: {emp_info['overall_sparsity']:.2%}\")\n",
    "        \n",
    "        # Summary comparison\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"SUMMARY COMPARISON\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"{'Method':<20} {'Accuracy':<12} {'MLM PPL':<12} {'Sparsity':<12}\")\n",
    "        print(\"-\" * 56)\n",
    "        print(f\"{'Original':<20} {orig_results['accuracy']:.4f} {orig_perplexity:>11.2f} {'0.00%':>11}\")\n",
    "        print(f\"{'NEFF (weight)':<20} {neff_results['accuracy']:.4f} {neff_perplexity:>11.2f} {neff_info['overall_sparsity']:>11.2%}\")\n",
    "        print(f\"{'EMP (activation)':<20} {emp_results['accuracy']:.4f} {emp_perplexity:>11.2f} {emp_info['overall_sparsity']:>11.2%}\")\n",
    "        \n",
    "        return {\n",
    "            \"original\": {\"accuracy\": orig_results['accuracy'], \"perplexity\": orig_perplexity},\n",
    "            \"neff\": {\"accuracy\": neff_results['accuracy'], \"perplexity\": neff_perplexity, \"sparsity\": neff_info['overall_sparsity']},\n",
    "            \"emp\": {\"accuracy\": emp_results['accuracy'], \"perplexity\": emp_perplexity, \"sparsity\": emp_info['overall_sparsity']}\n",
    "        }\n",
    "\n",
    "# =====================================\n",
    "# LLM Perplexity Testing\n",
    "# =====================================\n",
    "\n",
    "def test_llm_perplexity(\n",
    "    model_name: str = \"facebook/opt-125m\",\n",
    "    calibration_samples: int = 128,\n",
    "    eval_samples: int = 100\n",
    "):\n",
    "    \"\"\"Test perplexity on a small LLM with different pruning methods\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"LLM Perplexity Evaluation: {model_name}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 1. Original model perplexity\n",
    "    print(\"\\n1. Original Model\")\n",
    "    print(\"-\" * 30)\n",
    "    orig_ppl = evaluate_perplexity(\n",
    "        model, tokenizer,\n",
    "        dataset_name=\"wikitext\",\n",
    "        dataset_config=\"wikitext-2-raw-v1\",\n",
    "        max_samples=eval_samples\n",
    "    )\n",
    "    print(f\"Perplexity: {orig_ppl:.2f}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Prepare calibration data\n",
    "    calib_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    calib_texts = [text for text in calib_dataset[\"text\"][:calibration_samples] if text.strip()]\n",
    "    \n",
    "    calib_data = []\n",
    "    for text in calib_texts[:calibration_samples]:\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "        calib_data.append({\"input_ids\": tokens[\"input_ids\"]})\n",
    "    \n",
    "    calib_loader = DataLoader(calib_data, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # 2. NEFF weight-only pruning\n",
    "    print(\"\\n2. NEFF Weight-only Pruning\")\n",
    "    print(\"-\" * 30)\n",
    "    neff_model, neff_info = prune_model_neff(\n",
    "        model,\n",
    "        method=\"weight_only\",\n",
    "        keep_modules=['lm_head', 'embed_tokens', 'embed_positions']\n",
    "    )\n",
    "    \n",
    "    neff_ppl = evaluate_perplexity(\n",
    "        neff_model, tokenizer,\n",
    "        dataset_name=\"wikitext\",\n",
    "        dataset_config=\"wikitext-2-raw-v1\",\n",
    "        max_samples=eval_samples\n",
    "    )\n",
    "    print(f\"Perplexity: {neff_ppl:.2f} (Δ: +{neff_ppl - orig_ppl:.2f})\")\n",
    "    print(f\"Sparsity: {neff_info['overall_sparsity']:.2%}\")\n",
    "    \n",
    "    # 3. EMP activation-aware pruning\n",
    "    print(\"\\n3. EMP Activation-aware Pruning\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    activation_stats = collect_activation_statistics(\n",
    "        model,\n",
    "        calib_loader,\n",
    "        num_batches=min(32, len(calib_loader)),\n",
    "        model_type=\"causal_lm\"\n",
    "    )\n",
    "    \n",
    "    emp_model, emp_info = prune_model_neff(\n",
    "        model,\n",
    "        method=\"emp\",\n",
    "        activation_stats=activation_stats,\n",
    "        keep_modules=['lm_head', 'embed_tokens', 'embed_positions']\n",
    "    )\n",
    "    \n",
    "    emp_ppl = evaluate_perplexity(\n",
    "        emp_model, tokenizer,\n",
    "        dataset_name=\"wikitext\",\n",
    "        dataset_config=\"wikitext-2-raw-v1\",\n",
    "        max_samples=eval_samples\n",
    "    )\n",
    "    print(f\"Perplexity: {emp_ppl:.2f} (Δ: +{emp_ppl - orig_ppl:.2f})\")\n",
    "    print(f\"Sparsity: {emp_info['overall_sparsity']:.2%}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"SUMMARY - LLM Perplexity Comparison\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"{'Method':<20} {'Perplexity':<15} {'Sparsity':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Original':<20} {orig_ppl:<15.2f} {'0.00%':<15}\")\n",
    "    print(f\"{'NEFF (weight)':<20} {neff_ppl:<15.2f} {neff_info['overall_sparsity']:<15.2%}\")\n",
    "    print(f\"{'EMP (activation)':<20} {emp_ppl:<15.2f} {emp_info['overall_sparsity']:<15.2%}\")\n",
    "    \n",
    "    return {\n",
    "        \"original\": orig_ppl,\n",
    "        \"neff\": {\"perplexity\": neff_ppl, \"sparsity\": neff_info['overall_sparsity']},\n",
    "        \"emp\": {\"perplexity\": emp_ppl, \"sparsity\": emp_info['overall_sparsity']}\n",
    "    }\n",
    "\n",
    "# =====================================\n",
    "# Main Execution\n",
    "# =====================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Test 1: LLM Perplexity\n",
    "    print(\"\\n[TEST 1: Language Model Perplexity]\")\n",
    "    llm_results = test_llm_perplexity(\n",
    "        model_name=\"facebook/opt-125m\",  # Small model for demo\n",
    "        calibration_samples=128,\n",
    "        eval_samples=100  # Increase for paper\n",
    "    )\n",
    "    \n",
    "    # Test 2: BERT on GLUE tasks\n",
    "    print(\"\\n[TEST 2: BERT Classification Tasks]\")\n",
    "    \n",
    "    # Run on multiple tasks for comprehensive evaluation\n",
    "    tasks = [\"sst2\", \"mrpc\"]  # Add more: \"cola\", \"qqp\" for paper\n",
    "    all_bert_results = {}\n",
    "    \n",
    "    for task in tasks:\n",
    "        bert_exp = BertExperiments(\n",
    "            model_name=\"bert-base-uncased\",\n",
    "            task=task\n",
    "        )\n",
    "        results = bert_exp.run_full_evaluation(calibration_samples=128)\n",
    "        all_bert_results[task] = results\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nLLM Perplexity Results:\")\n",
    "    print(f\"  NEFF achieves {llm_results['neff']['sparsity']:.1%} sparsity with {(llm_results['neff']['perplexity']/llm_results['original'] - 1)*100:.1f}% perplexity increase\")\n",
    "    print(f\"  EMP achieves {llm_results['emp']['sparsity']:.1%} sparsity with {(llm_results['emp']['perplexity']/llm_results['original'] - 1)*100:.1f}% perplexity increase\")\n",
    "    \n",
    "    print(\"\\nBERT Results Summary:\")\n",
    "    for task, results in all_bert_results.items():\n",
    "        print(f\"\\n  {task.upper()}:\")\n",
    "        orig_acc = results['original']['accuracy']\n",
    "        neff_acc = results['neff']['accuracy']\n",
    "        emp_acc = results['emp']['accuracy']\n",
    "        print(f\"    NEFF: {results['neff']['sparsity']:.1%} sparsity, {(neff_acc/orig_acc - 1)*100:.2f}% accuracy change\")\n",
    "        print(f\"    EMP:  {results['emp']['sparsity']:.1%} sparsity, {(emp_acc/orig_acc - 1)*100:.2f}% accuracy change\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2eb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
