{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f378eab0",
   "metadata": {},
   "source": [
    "# test for effective activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7c3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "\n",
    "folder = \"test_results\"\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7b334",
   "metadata": {},
   "source": [
    "## well trained linear mlp model in MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f10f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size= [512, 512, 512]):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))        \n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ffc6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "\n",
    "# MINIST-10 dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearModel(input_size=28*28, output_size=10, hidden_size=[1024, 512, 512]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "result = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a19630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.2991, Accuracy: 1147/10000 (11.47%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.288179\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.293578\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.196733\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.122115\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.125013\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.128586\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.137612\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.169054\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.162738\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.057133\n",
      "\n",
      "Test set: Average loss: 0.1367, Accuracy: 9550/10000 (95.50%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.057720\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.086064\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.079180\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.195058\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.111307\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.060655\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.215371\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.027528\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.041934\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.194882\n",
      "\n",
      "Test set: Average loss: 0.0881, Accuracy: 9712/10000 (97.12%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.076563\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.050461\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.075961\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.009364\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.090921\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.016513\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.074465\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.014362\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.013701\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.106188\n",
      "\n",
      "Test set: Average loss: 0.0829, Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.015127\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.011197\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.055825\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.012943\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.087981\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.042015\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.000621\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.129626\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001671\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.040056\n",
      "\n",
      "Test set: Average loss: 0.0910, Accuracy: 9733/10000 (97.33%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.050884\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.016944\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.019355\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.016359\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.004287\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.001159\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.035557\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.045900\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.070755\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.063111\n",
      "\n",
      "Test set: Average loss: 0.0818, Accuracy: 9761/10000 (97.61%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.010027\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.012383\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.011572\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.005299\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.002665\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.008041\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.114753\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.031377\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.006698\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.075682\n",
      "\n",
      "Test set: Average loss: 0.0728, Accuracy: 9782/10000 (97.82%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.057377\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.002861\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.032210\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.007987\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.005848\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.000890\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.011339\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.008820\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.009347\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.078348\n",
      "\n",
      "Test set: Average loss: 0.0726, Accuracy: 9818/10000 (98.18%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.002980\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.013694\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.010508\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001901\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.003187\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.025278\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.012303\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.103193\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.007364\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.000865\n",
      "\n",
      "Test set: Average loss: 0.0709, Accuracy: 9808/10000 (98.08%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.050219\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.003014\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.003962\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.030461\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.065504\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.002303\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.049418\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.000536\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.001087\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.203398\n",
      "\n",
      "Test set: Average loss: 0.0719, Accuracy: 9810/10000 (98.10%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.027363\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.030418\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.042697\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001976\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000765\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.056545\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.005432\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000053\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.146697\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.002336\n",
      "\n",
      "Test set: Average loss: 0.0906, Accuracy: 9771/10000 (97.71%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial test\n",
    "test_loss, test_accuracy = test(model, device, test_loader)\n",
    "result['test_loss'].append(test_loss)\n",
    "result['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    result['train_loss'].append(train_loss)\n",
    "    result['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    # Test after each epoch\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    result['test_loss'].append(test_loss)\n",
    "    result['test_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# Save the model\n",
    "\n",
    "torch.save(model.state_dict(), folder + '/' +'linear_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b86fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(folder + '/' +'linear_model.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4821bc3",
   "metadata": {},
   "source": [
    "## per colum pruning which means Prune weights going into a neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37825d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask_per_column(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfdb1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_per_column(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask_per_column(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n",
    "\n",
    "def model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a3432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0879, Accuracy: 9757/10000 (97.57%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0879, Test Accuracy: 97.57%\n"
     ]
    }
   ],
   "source": [
    "prune_model = prune_model_neff_per_column(model, renormalize=False)\n",
    "prune_model.to(device)\n",
    "# Test the pruned model without renormalization\n",
    "test_loss, test_accuracy = test(prune_model, device, test_loader)\n",
    "print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model without renormalization\n",
    "torch.save(prune_model.state_dict(), folder + '/' + 'pruned_linear_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1afde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the pruned model without renormalization: 0.3416\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sparsity = model_sparsity(prune_model)\n",
    "print(f'Sparsity of the pruned model without renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5272d8",
   "metadata": {},
   "source": [
    "## per row pruning which means Prune weights going into a neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2f1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask_per_row(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=1, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=1, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=1, descending=True)\n",
    "    range_tensor = torch.arange(input_size, device=x.device).unsqueeze(0).expand(output_size, -1)\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(1, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03e185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_per_row(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask_per_row(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efa968e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0876, Accuracy: 9758/10000 (97.58%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0876, Test Accuracy: 97.58%\n"
     ]
    }
   ],
   "source": [
    "prune_model_row = prune_model_neff_per_row(model, renormalize=False)\n",
    "prune_model_row.to(device)\n",
    "# Test the pruned model without renormalization\n",
    "test_loss, test_accuracy = test(prune_model_row, device, test_loader)\n",
    "print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model without renormalization\n",
    "torch.save(prune_model_row.state_dict(), folder + '/' + 'pruned_linear_model_row.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1e2acf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the pruned model without renormalization: 0.3645\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sparsity = model_sparsity(prune_model_row)\n",
    "print(f'Sparsity of the pruned model without renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794930b",
   "metadata": {},
   "source": [
    "## test bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a275e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
