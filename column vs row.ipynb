{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f378eab0",
   "metadata": {},
   "source": [
    "# test for effective activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7c3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import copy\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "\n",
    "folder = \"test_results\"\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7b334",
   "metadata": {},
   "source": [
    "## well trained linear mlp model in MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f10f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size= [512, 512, 512]):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for size in hidden_size:\n",
    "            self.layers.append(nn.Linear(prev_size, size))\n",
    "            prev_size = size\n",
    "            \n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))        \n",
    "        x = self.output(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ffc6c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 9.60MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 923kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 6.95MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 3e-4\n",
    "\n",
    "# MINIST-10 dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   \n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearModel(input_size=28*28, output_size=10, hidden_size=[1024, 512, 512]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "result = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'test_loss': [],\n",
    "    'test_accuracy': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a19630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3008, Accuracy: 1003/10000 (10.03%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307224\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.280008\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.227946\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.304226\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.162672\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.179618\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.150404\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.126181\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.220011\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.120385\n",
      "\n",
      "Test set: Average loss: 0.1090, Accuracy: 9644/10000 (96.44%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.135619\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.024176\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.108532\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.142846\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.157513\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.081913\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.080279\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.070293\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.021064\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.029085\n",
      "\n",
      "Test set: Average loss: 0.0869, Accuracy: 9721/10000 (97.21%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.022356\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.014631\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.013269\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.069180\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.067533\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.046236\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.082794\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.120247\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.013096\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.159467\n",
      "\n",
      "Test set: Average loss: 0.0704, Accuracy: 9771/10000 (97.71%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.065439\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.002368\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.062667\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.031449\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.106618\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.151638\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.013899\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.055587\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.033254\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.057394\n",
      "\n",
      "Test set: Average loss: 0.0797, Accuracy: 9756/10000 (97.56%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.023161\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.002971\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.007462\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.095388\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.015369\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.016567\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.078819\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.114463\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.039622\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.079584\n",
      "\n",
      "Test set: Average loss: 0.0630, Accuracy: 9811/10000 (98.11%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.025989\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.010404\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.011096\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.000158\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001122\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.005482\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.081662\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.016579\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.010042\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.004905\n",
      "\n",
      "Test set: Average loss: 0.0664, Accuracy: 9815/10000 (98.15%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.004561\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.029575\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.009859\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.000779\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.027460\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.019641\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.000919\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.022112\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.002938\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.000856\n",
      "\n",
      "Test set: Average loss: 0.0768, Accuracy: 9794/10000 (97.94%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.002146\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.005091\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.002195\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.002620\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.003101\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001504\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.003834\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.051610\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.017480\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.002232\n",
      "\n",
      "Test set: Average loss: 0.0720, Accuracy: 9814/10000 (98.14%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000976\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.005299\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.003346\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.013858\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.060596\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.005108\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001489\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.007943\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.005077\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.004871\n",
      "\n",
      "Test set: Average loss: 0.0733, Accuracy: 9793/10000 (97.93%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.002520\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001551\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.140939\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.007918\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000100\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000377\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.013600\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.001782\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.065137\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.073775\n",
      "\n",
      "Test set: Average loss: 0.0751, Accuracy: 9815/10000 (98.15%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial test\n",
    "test_loss, test_accuracy = test(model, device, test_loader)\n",
    "result['test_loss'].append(test_loss)\n",
    "result['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, device, train_loader, optimizer, epoch)\n",
    "    result['train_loss'].append(train_loss)\n",
    "    result['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    # Test after each epoch\n",
    "    test_loss, test_accuracy = test(model, device, test_loader)\n",
    "    result['test_loss'].append(test_loss)\n",
    "    result['test_accuracy'].append(test_accuracy)\n",
    "    \n",
    "# Save the model\n",
    "\n",
    "torch.save(model.state_dict(), folder + '/' +'linear_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b86fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(folder + '/' +'linear_model.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4821bc3",
   "metadata": {},
   "source": [
    "## per colum pruning which means Prune weights going into a neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37825d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask_per_column(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=0, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=0, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=0, descending=True)\n",
    "    range_tensor = torch.arange(output_size, device=x.device).unsqueeze(0).expand(input_size, -1).T\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(0, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfdb1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_per_column(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask_per_column(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n",
    "\n",
    "def model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of the model\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += torch.sum(param == 0).item()\n",
    "    \n",
    "    sparsity = zero_params / total_params\n",
    "    return sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a3432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0685, Accuracy: 9803/10000 (98.03%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0685, Test Accuracy: 98.03%\n"
     ]
    }
   ],
   "source": [
    "prune_model = prune_model_neff_per_column(model, renormalize=False)\n",
    "prune_model.to(device)\n",
    "# Test the pruned model without renormalization\n",
    "test_loss, test_accuracy = test(prune_model, device, test_loader)\n",
    "print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model without renormalization\n",
    "torch.save(prune_model.state_dict(), folder + '/' + 'pruned_linear_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1afde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the pruned model without renormalization: 0.3420\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sparsity = model_sparsity(prune_model)\n",
    "print(f'Sparsity of the pruned model without renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5272d8",
   "metadata": {},
   "source": [
    "## per row pruning which means Prune weights going into a neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2f1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_mask_per_row(module:nn.Module) -> torch.Tensor:\n",
    "    x = module.weight.data\n",
    "    output_size, input_size = x.shape\n",
    "    x_norm = torch.abs(x) / torch.sum(torch.abs(x), dim=1, keepdim=True)\n",
    "    neff = torch.floor(1/torch.sum((x_norm ** 2), dim=1, keepdim=True).squeeze(0))\n",
    "    \n",
    "    _, indices = torch.sort(x_norm, dim=1, descending=True)\n",
    "    range_tensor = torch.arange(input_size, device=x.device).unsqueeze(0).expand(output_size, -1)\n",
    "    sorted_mask = range_tensor < neff\n",
    "    \n",
    "    mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "    mask.scatter_(1, indices, sorted_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03e185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model_neff_per_row(model, renormalize=False):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            mask = get_linear_mask_per_row(module).to(module.weight.device)\n",
    "            with torch.no_grad():\n",
    "                module.weight *= mask\n",
    "                if renormalize:\n",
    "                    row_sum = module.weight.sum(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "                    module.weight.div_(row_sum)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa968e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9803/10000 (98.03%)\n",
      "\n",
      "Pruned Model without Renormalization - Test Loss: 0.0707, Test Accuracy: 98.03%\n"
     ]
    }
   ],
   "source": [
    "prune_model_row = prune_model_neff_per_row(model, renormalize=False)\n",
    "prune_model_row.to(device)\n",
    "# Test the pruned model without renormalization\n",
    "test_loss, test_accuracy = test(prune_model_row, device, test_loader)\n",
    "print(f'Pruned Model without Renormalization - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "# Save the pruned model without renormalization\n",
    "torch.save(prune_model_row.state_dict(), folder + '/' + 'pruned_linear_model_row.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e2acf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the pruned model without renormalization: 0.3646\n",
      "Sparsity of the original model: 0.0000\n"
     ]
    }
   ],
   "source": [
    "sparsity = model_sparsity(prune_model_row)\n",
    "print(f'Sparsity of the pruned model without renormalization: {sparsity:.4f}')\n",
    "sparsity = model_sparsity(model)\n",
    "print(f'Sparsity of the original model: {sparsity:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794930b",
   "metadata": {},
   "source": [
    "## EMP ACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "710a275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Activation-EMP (W·x) pruning utilities for nn.Linear =====\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def model_sparsity(model: nn.Module) -> float:\n",
    "    total, zeros = 0, 0\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.dim() >= 2 and 'weight' in n:\n",
    "            total += p.numel()\n",
    "            zeros += (p == 0).sum().item()\n",
    "    return zeros / max(total, 1)\n",
    "\n",
    "# ---- Step 1: collect E[|x|] (per in-feature) for each Linear via forward hooks\n",
    "@torch.no_grad()\n",
    "def collect_input_magnitudes(model: nn.Module,\n",
    "                             data_loader,\n",
    "                             device,\n",
    "                             num_batches: int = 10):\n",
    "    model.eval()\n",
    "    # list Linear modules in traversal order\n",
    "    linear_list = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
    "    sums = []\n",
    "    counts = []\n",
    "    handles = []\n",
    "\n",
    "    for m in linear_list:\n",
    "        sums.append(torch.zeros(m.in_features, device=device))\n",
    "        counts.append(torch.tensor(0, device=device))\n",
    "\n",
    "    index_of = {id(m): i for i, m in enumerate(linear_list)}\n",
    "\n",
    "    def hook_fn(module, inputs, output):\n",
    "        idx = index_of[id(module)]\n",
    "        x = inputs[0].detach()\n",
    "        # flatten all leading dims except last: [..., in_features]\n",
    "        x2d = x.flatten(0, -2)  # (B*..., in_features)\n",
    "        sums[idx] += x2d.abs().sum(dim=0)\n",
    "        counts[idx] += x2d.shape[0]\n",
    "\n",
    "    for m in linear_list:\n",
    "        handles.append(m.register_forward_hook(hook_fn))\n",
    "\n",
    "    seen = 0\n",
    "    for data, target in data_loader:\n",
    "        data = data.to(device)\n",
    "        _ = model(data)\n",
    "        seen += 1\n",
    "        if seen >= num_batches:\n",
    "            break\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    mags = [s / torch.clamp(c.float(), min=1.0) for s, c in zip(sums, counts)]\n",
    "    # Return in the same order as linear_list\n",
    "    return linear_list, mags\n",
    "\n",
    "# ---- Step 2: build the activation-aware mask from N_eff on |W| * E|x|\n",
    "@torch.no_grad()\n",
    "def get_linear_mask_emp(module: nn.Linear,\n",
    "                        in_mag: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        module: nn.Linear with weight shape [out, in]\n",
    "        in_mag: tensor [in] = E[|x|] for this module's input\n",
    "    Returns:\n",
    "        mask (bool) with same shape as weight\n",
    "        neff_row (long) length = out_features\n",
    "    \"\"\"\n",
    "    W = module.weight.data  # [out, in]\n",
    "    # contributions per input to each neuron:\n",
    "    contrib = W.abs() * in_mag.unsqueeze(0)  # [out, in]\n",
    "    row_sum = contrib.sum(dim=1, keepdim=True).clamp(min=1e-12)\n",
    "    norm = contrib / row_sum                 # \\hat c_ji\n",
    "\n",
    "    neff = torch.floor(1.0 / norm.pow(2).sum(dim=1)).clamp(min=1, max=W.shape[1]).long()  # [out]\n",
    "\n",
    "    # sort each row by importance and keep top neff[j]\n",
    "    _, idx = torch.sort(norm, dim=1, descending=True)\n",
    "    out, in_ = W.shape\n",
    "    ranks = torch.arange(in_, device=W.device).unsqueeze(0).expand(out, in_)\n",
    "    keep_sorted = ranks < neff.unsqueeze(1)   # [out, in] (sorted order)\n",
    "\n",
    "    mask = torch.zeros_like(W, dtype=torch.bool)\n",
    "    mask.scatter_(1, idx, keep_sorted)\n",
    "    return mask, neff\n",
    "\n",
    "# ---- Step 3: prune with EMP (optional L1 row re-normalization)\n",
    "@torch.no_grad()\n",
    "def prune_model_emp_activation(model: nn.Module,\n",
    "                               calib_loader,\n",
    "                               device,\n",
    "                               num_calib_batches: int = 10,\n",
    "                               renormalize: bool = False):\n",
    "    pruned = copy.deepcopy(model).to(device)\n",
    "    linear_list, mags = collect_input_magnitudes(pruned, calib_loader, device, num_batches=num_calib_batches)\n",
    "\n",
    "    layer_neff = {}\n",
    "    for lin, mu in zip(linear_list, mags):\n",
    "        W = lin.weight.data\n",
    "        old_row_l1 = W.abs().sum(dim=1, keepdim=True)\n",
    "        mask, neff = get_linear_mask_emp(lin, mu.to(W.device))\n",
    "        # apply mask\n",
    "        W.mul_(mask)\n",
    "        if renormalize:\n",
    "            new_row_l1 = W.abs().sum(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "            scale = old_row_l1 / new_row_l1\n",
    "            W.mul_(scale)\n",
    "        layer_neff[id(lin)] = {\n",
    "            \"name\": getattr(lin, \"_emp_name\", None),\n",
    "            \"neff_row\": neff.detach().cpu(),\n",
    "            \"avg_neff\": neff.float().mean().item(),\n",
    "            \"in_features\": W.shape[1],\n",
    "            \"out_features\": W.shape[0],\n",
    "            \"layer_sparsity\": float((~mask).sum().item() / mask.numel())\n",
    "        }\n",
    "\n",
    "    return pruned, layer_neff\n",
    "\n",
    "# ---- Helper: pretty summary\n",
    "def summarize_emp(layer_neff_dict):\n",
    "    lines = []\n",
    "    for k, v in layer_neff_dict.items():\n",
    "        name = v.get(\"name\") or f\"Linear(id={k})\"\n",
    "        lines.append(\n",
    "            f\"{name:30s} | out={v['out_features']:4d} in={v['in_features']:4d} \"\n",
    "            f\"| avg N_eff={v['avg_neff']:.1f} | sparsity={v['layer_sparsity']*100:5.1f}%\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---- (Optional) attach names to ease reading\n",
    "def tag_linear_names(model: nn.Module):\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m._emp_name = name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3883051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-wise EMP summary:\n",
      " Linear(id=13268523312)         | out=1024 in= 784 | avg N_eff=355.7 | sparsity= 54.6%\n",
      "Linear(id=13268522976)         | out= 512 in=1024 | avg N_eff=270.8 | sparsity= 73.6%\n",
      "Linear(id=4891302336)          | out= 512 in= 512 | avg N_eff=250.4 | sparsity= 51.1%\n",
      "Linear(id=4891300320)          | out=  10 in= 512 | avg N_eff=253.3 | sparsity= 50.5%\n",
      "\n",
      "Test set: Average loss: 0.0851, Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "EMP-pruned (W·x) - Test Loss: 0.0851, Test Acc: 97.49%\n",
      "EMP-pruned model sparsity: 0.6026\n"
     ]
    }
   ],
   "source": [
    "# After you finish training your model `model` and have loaders:\n",
    "#   train_loader, test_loader, device, and your `test(...)` function defined\n",
    "\n",
    "# 1) Build a pruned copy using a few calibration batches\n",
    "pruned_emp, layer_neff = prune_model_emp_activation(\n",
    "    model, calib_loader=train_loader, device=device, num_calib_batches=10, renormalize=False\n",
    ")\n",
    "print(\"Layer-wise EMP summary:\\n\", summarize_emp(layer_neff))\n",
    "\n",
    "# 2) Evaluate\n",
    "test_loss, test_acc = test(pruned_emp, device, test_loader)\n",
    "print(f\"EMP-pruned (W·x) - Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "print(f\"EMP-pruned model sparsity: {model_sparsity(pruned_emp):.4f}\")\n",
    "\n",
    "# (Optional) compare to your row/column-only weight pruning results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbee97a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
